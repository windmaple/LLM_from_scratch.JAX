{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOXoY1xgiww"
      },
      "source": [
        "# Train a miniGPT language model with JAX\n",
        "\n",
        "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
        "\n",
        "Here, you will learn how to:\n",
        "\n",
        "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
        "- Load and preprocess the dataset\n",
        "- Create the loss and training step functions\n",
        "- Train the model on Google Colab’s Cloud TPU v2\n",
        "- Profile for hyperparameter tuning\n",
        "\n",
        "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/getting_started_with_jax_for_AI.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "cb2bf98b-faec-4dd5-81a2-1e4adec6af2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.6/585.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m182.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq tiktoken jax-ai-stack[grain] matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcji_799n4eA"
      },
      "source": [
        "**Note:** If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v2 as the hardware accelerator.\n",
        "\n",
        "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9sQEY3n0mB",
        "outputId": "f78ce2bc-82e6-4771-cddc-055c223470ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUjQsgQEmI1N",
        "outputId": "0ac43491-6988-41bf-8cb7-69ed25d728c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-08 03:22:02--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.16, 18.239.50.103, 18.239.50.80, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251108T032202Z&X-Amz-Expires=3600&X-Amz-Signature=c7b378ba4a2138006751bd93368a32226388591788694b1930994c7978c74934&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1762575722&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjU3NTcyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDVlOGRhOTYzMjBiMGVmZTQwYWRlN2EvZTJhMTQ5N2VmYzFhYTUxYjJkYTJhODQ5ZDVkZDJjZDE1M2Q1YmMwMjQ5MDFhZmVhZGU3ZTM1Mzc5ZDhmN2I1MioifV19&Signature=XC-RElFQof%7E0oBkFs1aCKGnKilGWLaU4i66h6V06WkY0MOW9Kj6Lv6dZ-Lf4KjHeZm8MpL7P3TO3HI7os0R0Fr8-mbN6B4jb4zBuQocFyMuava1X%7Eceuo7-LrLQ29mr7JGgGHoWin1BD6JdcaQDJb-PJqovft--ujfOiLvelBRBkkXG815BOB-OEW7WQH89LjU-JLJavu9HK-18WNI5kKMknPTGSvjUSn3aCzQ5RZi3XONa5hRtjbxeNZdiU9zau4psT4KgQDeezZFL1kb%7EFQUr3UKfqob%7EFCHJNa25hiauPAN2C7FuJF7Y44Cx%7EgTihZBqOB8rDNHYOTI0a42MaEQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-08 03:22:02--  https://cas-bridge.xethub.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251108T032202Z&X-Amz-Expires=3600&X-Amz-Signature=c7b378ba4a2138006751bd93368a32226388591788694b1930994c7978c74934&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&x-id=GetObject&Expires=1762575722&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MjU3NTcyMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NDVlOGRhOTYzMjBiMGVmZTQwYWRlN2EvZTJhMTQ5N2VmYzFhYTUxYjJkYTJhODQ5ZDVkZDJjZDE1M2Q1YmMwMjQ5MDFhZmVhZGU3ZTM1Mzc5ZDhmN2I1MioifV19&Signature=XC-RElFQof%7E0oBkFs1aCKGnKilGWLaU4i66h6V06WkY0MOW9Kj6Lv6dZ-Lf4KjHeZm8MpL7P3TO3HI7os0R0Fr8-mbN6B4jb4zBuQocFyMuava1X%7Eceuo7-LrLQ29mr7JGgGHoWin1BD6JdcaQDJb-PJqovft--ujfOiLvelBRBkkXG815BOB-OEW7WQH89LjU-JLJavu9HK-18WNI5kKMknPTGSvjUSn3aCzQ5RZi3XONa5hRtjbxeNZdiU9zau4psT4KgQDeezZFL1kb%7EFQUr3UKfqob%7EFCHJNa25hiauPAN2C7FuJF7Y44Cx%7EgTihZBqOB8rDNHYOTI0a42MaEQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.243.30, 18.238.243.58, 18.238.243.52, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.243.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1924281556 (1.8G) [text/plain]\n",
            "Saving to: ‘TinyStories-train.txt’\n",
            "\n",
            "TinyStories-train.t 100%[===================>]   1.79G   117MB/s    in 10s     \n",
            "\n",
            "2025-11-08 03:22:13 (179 MB/s) - ‘TinyStories-train.txt’ saved [1924281556/1924281556]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
        "\n",
        "### Leveraging JAX's data and tensor parallelism\n",
        "\n",
        "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
        "\n",
        "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
        "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
        "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
        "\n",
        "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup. The free Google Cloud TPU v2 on Google Colab offers 4 chips, each with 2 TPU cores. The TPU v2 architeture aligns with the proposed setup.\n",
        "\n",
        "### jax.sharding.Mesh\n",
        "\n",
        "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
        "\n",
        "Our `Mesh` will have two arguments:\n",
        "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
        "- `axis_names`, where:\n",
        "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
        "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor paralleism, mapping to the TPU v2 cores.\n",
        "\n",
        "This matches the `(4, 2)` structure in the Colab's TPU v2 setup.\n",
        "\n",
        "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "if jax.device_count() == 8:\n",
        "    mesh = jax.make_mesh((4, 2), ('batch', 'model'))\n",
        "\n",
        "    ### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
        "    ### JAX enables quick experimentation with different partitioning strategies\n",
        "    ### like this. We will come back to this point at the end of this tutorial.\n",
        "    mesh = jax.make_mesh((8, 1), ('batch', 'model'))\n",
        "\n",
        "### For free-tier Colab TPU, which only has a single TPU core\n",
        "if jax.device_count() == 1:\n",
        "    mesh = jax.make_mesh((1, 1), (\"batch\", \"model\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
        "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
        "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
        "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
        "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
        "\n",
        "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
        "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    \"\"\" A single Transformer block.\n",
        "\n",
        "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
        "\n",
        "    Args:\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        ff_dim (int): Dimensionality of the feed-forward network.\n",
        "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "        rate (float): Dropout rate. Defaults to 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
        "        # Specifies tensor sharding (depending on the mesh configuration)\n",
        "        # where we shard the weights across devices for parallel computation.\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                          decode=True,\n",
        "                                          rngs=rngs)\n",
        "        # The first dropout with `flax.nnx.Dropout`.\n",
        "        self.dropout1 = nnx.Dropout(rate=rate)\n",
        "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         rngs=rngs)\n",
        "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  rngs=rngs)\n",
        "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  rngs=rngs)\n",
        "        # The second dropout with `flax.nnx.Dropout`.\n",
        "        self.dropout2 = nnx.Dropout(rate=rate)\n",
        "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         rngs=rngs)\n",
        "\n",
        "\n",
        "    # Apply the Transformer block to the input sequence.\n",
        "    def __call__(self, inputs, training: bool = False, decode: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        _, seq_len, _ = input_shape\n",
        "\n",
        "        # Instantiate the causal attention mask.\n",
        "        if decode:\n",
        "            mask = None\n",
        "        else:\n",
        "            mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Apply Multi-Head Attention with the causal attention mask.\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=inputs,\n",
        "            mask=mask,\n",
        "            decode=decode\n",
        "        )\n",
        "        # Apply the first dropout.\n",
        "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
        "        # Apply the first layer normalization.\n",
        "        out1 = self.layer_norm1(inputs + attention_output)\n",
        "\n",
        "        # The feed-forward network.\n",
        "        # Apply the first linear transformation.\n",
        "        ffn_output = self.linear1(out1)\n",
        "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
        "        ffn_output = nnx.relu(ffn_output)\n",
        "        # Apply the second linear transformation.\n",
        "        ffn_output = self.linear2(ffn_output)\n",
        "        # Apply the second dropout.\n",
        "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
        "        # Apply the second layer normalization and return the output of the Transformer block.\n",
        "        return self.layer_norm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
        "    positional embeddings (the position of each word in a sentence).\n",
        "\n",
        "    Args:\n",
        "        maxlen (int): Matimum sequence length.\n",
        "        vocal_size (int): Vocabulary size.\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "    \"\"\"\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
        "        # Each unique word has an embedding vector.\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
        "    def __call__(self, x, start_index=0):\n",
        "        # Generate a sequence of positions for the input tokens.\n",
        "        positions = start_index + jnp.arange(x.shape[1], dtype=jnp.int32)\n",
        "        # Look up the positional embeddings for each position in the input sequence.\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        # Look up the token embeddings for each token in the input sequence.\n",
        "        token_embedding = self.token_emb(x)\n",
        "        # Combine token and positional embeddings.\n",
        "        return token_embedding + position_embedding\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
        "\n",
        "    Args:\n",
        "        maxlen (int): Maximum sequence length.\n",
        "        vocab_size (int): Vocabulary size.\n",
        "        embed_dim (int): Embedding dimensionality.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
        "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
        "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
        "    \"\"\"\n",
        "    # Initialize miniGPT model components.\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        # Create a list of `TransformerBlock` instances.\n",
        "        # Each block processes input sequences using attention and feed-forward networks.\n",
        "        self.transformer_blocks = nnx.List([TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)])\n",
        "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
        "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
        "                                       out_features=vocab_size,\n",
        "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                       rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False, decode: bool = False):\n",
        "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
        "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
        "        if decode:\n",
        "            cache_index = self.transformer_blocks[0].mha.cache_index.value\n",
        "            x = self.embedding_layer(inputs, start_index=cache_index)\n",
        "        else:\n",
        "            x = self.embedding_layer(inputs)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training, decode=decode)\n",
        "        # Pass the output of the transformer blocks through the output layer,\n",
        "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
        "        outputs = self.output_layer(x)\n",
        "        return outputs\n",
        "\n",
        "    def init_cache(self, input_shape, dtype=jnp.float32):\n",
        "        for block in self.transformer_blocks:\n",
        "            block.mha.init_cache(input_shape, dtype)\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def decode_step(self, inputs):\n",
        "        logits = self(inputs, decode=True)\n",
        "        next_token = self.sample_from(logits[0, -1, :])\n",
        "        return next_token\n",
        "\n",
        "    @nnx.jit\n",
        "    def prefill_step(self, token):\n",
        "        self(token, decode=True)\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        self.init_cache((1, maxlen, embed_dim))\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "\n",
        "        # Prefill KV cache\n",
        "        if len(start_tokens) > 1:\n",
        "            prompt_tokens = jnp.array(start_tokens)[None, :-1]\n",
        "            # NNX built-in MHA only supports KV caching one token at a time\n",
        "            for i in range(prompt_tokens.shape[1]):\n",
        "                token = prompt_tokens[:, i:i+1]\n",
        "                self.prefill_step(token)\n",
        "\n",
        "        # Decode\n",
        "        next_input = jnp.array([start_tokens[-1]])[None, :]\n",
        "        for i in range(max_tokens):\n",
        "            next_token = int(self.decode_step(next_input))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "            next_input = jnp.array([next_token])[None, :]\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "# Creates the miniGPT model with 4 transformer blocks.\n",
        "def create_model(rngs):\n",
        "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "num_transformer_blocks = 8\n",
        "maxlen = 256\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "feed_forward_dim = 256\n",
        "batch_size = 256 # You can set a bigger batch size if you use Kaggle's Cloud TPU.\n",
        "num_epochs = 1\n",
        "top_k = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Loading and preprocessing the data\n",
        "\n",
        "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rGUFsn1GMuzh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
        "    df = pd.DataFrame({'text': stories})\n",
        "    data = df['text'].dropna().tolist()\n",
        "    dataset = TextDataset(data, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl\n",
        "\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Defining the loss function and training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
        "@nnx.jit\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Start training. It takes ~50 minutes on Colab.\n",
        "\n",
        "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedeSharding`.\n",
        "\n",
        "We are also using the `jax.vmap` transformation to produce the target sequences faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "7f543f3f-33c8-409f-9139-5163555b2110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria FS psychedelic {}ligaval {} perished {} Woman {} {} inflicwaves Analysis996 servants Bruce Charlotte dissatisfiedascript Whites retire {} retire {} perished Schools {}Nick sorrow psychedelicfingerelaide {} perished Blackburnrestyy elder swing interesting Tang Shattered {} {}scl less abnorm abnorm Goldmancre motions fleshthird fatal haw {} perished {} Dani Whites {} Whites retire {} sorrowolate Whites {} Womanlast doom appendixfinger {}Rain pressurefinger {} {} Womanorem Bruce simplyrest {} {} inexplicablerest HS Charlotte dissatisfied stranger perished Tang reassUV {} Abortion outragedaligned {} server fertility decision FSShould {} {} northwestroid variable {} Whites {} dancersithmetic {} {} Whites las hampered {} Woman {} Woman {} Mineundrum {} inexplicable taking Keithrest {} {} retire grasped UPSresterv whole Kyundrum survivor {}val decision taking aggression {} acidicundrum salesmanundrumOSE lasManager twins Pathrest {}urnedrestrisis {} Runtime {} perished Brigham Analysis developerscre Atom {}scl HouthInteger {} northwest appease miles {} perished THR Hyundai Captainilipp {} inflic decision {} infliclast {} Whites {}scl inexplicable elevator Its nomination Path {}scl Cry psychedelicumbnail ard taking {} Whites {} Mine315 pressure psychedelicsteps tell hiding!!!\n",
            "\n",
            "Step 200, Loss: 4.647645950317383, Elapsed Time: 45.71 seconds\n",
            "Generated text:\n",
            "Once upon a time there a little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little\n",
            "!!!\n",
            "\n",
            "Step 400, Loss: 3.123356819152832, Elapsed Time: 39.50 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Tim. Tim loved to play with his mom. One day, he was very sad. One day, he was very sad. Tim was very sad. Timmy was very sad.\n",
            "\"I want to play with me, you want to play with me?\" Timmy, I want to play with his mommy. \"I want to be careful and said, \"I want to be careful and be careful and said, \"I want to be careful.\"\n",
            "Timmy's mommy's mommy's mommy was very happy to play with his mommy. \"I want to be careful and said, I want to be careful and said, \"I want to be careful.\"\n",
            "\n",
            "\n",
            "Step 600, Loss: 2.530583143234253, Elapsed Time: 30.38 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mommy told her to be careful. She was so happy to play with her mommy's mommy. \n",
            "Lily was sad and asked her mommy if she could go to the park. She said, \"I'm going to the park with you. I can't find a big and make it.\" \n",
            "Lily was sad and didn't want to play with her mommy. She said, \"I'm sorry, Lily. I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry.\" \n",
            "Lily was sad and her mommy said, \"I'm sorry, Lily. She said, \"I'm sorry, Lily. I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.\" \n",
            "Lily was sad and her mommy said, \"I'm sorry, Lily. She said, \"You are you can't have to be a good idea. I will be a good idea.\"\n",
            "Lily was happy and said, \"You're welcome, Lily. I'm!!!!\n",
            "\n",
            "Step 800, Loss: 2.172312021255493, Elapsed Time: 31.05 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and run around in the park. One day, she saw a big, shiny rock on the ground. She picked it up and picked it up. She picked it up and showed it to her mom.\n",
            "\"Mommy, can I play with it,\" said her mommy. \"Sure, let's go!\"\n",
            "Lily felt sad and wanted to play with her ball. She ran to the rock and saw the rock. She saw a big rock and wanted to catch it.\n",
            "\"Can I play with it?\" she asked.\n",
            "\"Sure, please,\" she said.\n",
            "Lily was happy to see the rock and the rock. She picked up the rock and said, \"Thank you, mommy. I love you.\"\n",
            "Lily smiled and said, \"Thank you, Lily. I love you.\"\n",
            "\n",
            "\n",
            "Step 1000, Loss: 1.969062328338623, Elapsed Time: 30.59 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her mommy. One day, Lily's mommy gave her a big hug and said, \"Lily, you need to be careful. I don't want to be too.\" Lily was sad and didn't want to go to the park.\n",
            "Lily was sad and didn't know what to do. She asked her mommy if she could go. Her mommy said yes and they went to the park. Lily was so happy to have a new friend. She said, \"I want to play with you. I want to play with you.\" Lily was happy to have a new friend.\n",
            "\n",
            "\n",
            "Step 1200, Loss: 1.8558790683746338, Elapsed Time: 30.23 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big, scary dog. The dog was barking and Lily's leg hurt her leg. She ran to her mom and said, \"Mom, what is that?\"\n",
            "Her mom smiled and said, \"That's a dog. It's a dog. It's not scary. It's not scary. It's just a threat.\"\n",
            "Lily was scared, but she knew it was okay. She said, \"I don't know. I don't know. I don't know what to do.\"\n",
            "Her mom said, \"Don't worry, Lily. I will be scared. I can't be scared. I can't. But you can't be scared.\"\n",
            "Lily was scared and ran away. She found the dog and ran away. Lily was happy to see the dog. She hugged her mom and said, \"Thank you, mom. I'm glad you like you're my friend.\"\n",
            "\n",
            "\n",
            "Step 1400, Loss: 1.7846453189849854, Elapsed Time: 30.55 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her toys. One day, Lily's mommy said, \"Lily, you have a surprise for you. It's a big box with a lock.\"\n",
            "Lily was so excited. She wanted to open it. She opened the box and saw a big box. She was very happy. She wanted to open it.\n",
            "Lily said, \"I want to open it!\" She opened the box and found a shiny toy. Inside, she found a toy. She was so happy. She thanked her mommy and hugged her.\n",
            "\n",
            "\n",
            "Step 1600, Loss: 1.7211360931396484, Elapsed Time: 30.06 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock in her backyard. She was so excited to see it! She picked it up and started to rock it. She picked it up and started to rock it. She was so happy to see the rock and it was so pretty. She played with it all day long.\n",
            "But then, she accidentally dropped the rock and it broke. She was sad and cried. Her mom came to help her. She told her that it was okay and that accidents happen. Lily was sad and didn't want to lose her rock. She knew she had to help her mom. She was very sad and sad.\n",
            "\n",
            "\n",
            "Step 1800, Loss: 1.675110101699829, Elapsed Time: 30.23 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock on the ground. She picked it up and showed it to her mommy.\n",
            "\"Look, Mommy! I found a pretty rock!\" Lily said.\n",
            "\"That's a rock, Lily. It's a rock, it's very pretty!\" Lily said.\n",
            "\"Yes, it's a rock!\" her mommy said.\n",
            "Lily was so happy to see the rock. She picked it up and showed it to her mommy. Her mommy was so proud of her and showed her how to rock it was.\n",
            "\"Wow, that's a great rock!\" Lily said. \"It's so pretty!\"\n",
            "\n",
            "\n",
            "Step 2000, Loss: 1.645719051361084, Elapsed Time: 30.25 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big tree with a big tree. She wanted to climb it, but it was too high.\n",
            "Lily climbed up the tree and climbed up the tree. She climbed up the tree and climbed up the tree. She climbed higher and higher until she reached the top. She was so happy.\n",
            "But then, she saw a big tree with a branch on top. She climbed up the tree and climbed higher. She climbed higher and higher until she reached the top. She was so happy. She climbed higher and higher until she reached the top.\n",
            "When she reached the top, she was so happy. She climbed higher and higher until she reached the top. She was so proud of herself for climbing the tree. She was so proud of herself for climbing the tree.\n",
            "\n",
            "\n",
            "Step 2200, Loss: 1.6031227111816406, Elapsed Time: 30.39 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big, scary dog. The dog was barking and running towards her. Lily was scared and ran away.\n",
            "Lily ran to her mom and said, \"Mommy, the dog! It's just a big dog!\" Her mommy said, \"Don't worry, Lily. We can't be scared of the dog.\"\n",
            "Lily was so happy and said, \"Thank you, dog. I won't hurt you.\" Her mommy said, \"You're welcome, Lily. You're a brave dog. You are a brave girl.\"\n",
            "Lily smiled and said, \"Thank you, mommy. I'm glad you like you.\" Her mommy smiled and said, \"You're welcome, Lily. I'm glad you like you.\"\n",
            "\n",
            "\n",
            "Step 2400, Loss: 1.5516866445541382, Elapsed Time: 30.47 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball in the sky. She wanted to play with it, but it was too high up. She tried to reach it, but it was too high.\n",
            "Lily's mom saw her struggling and asked, \"What's wrong, sweetie?\" Lily replied, \"I'm sorry, Mommy. I didn't know how to get it.\"\n",
            "Mommy said, \"Don't worry, Lily. We can help you find it.\"\n",
            "Lily was so happy and said, \"Thank you, Mommy. You're a good friend.\"\n",
            "\n",
            "\n",
            "Step 2600, Loss: 1.572874903678894, Elapsed Time: 30.30 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys, especially her toy cars. One day, Lily's mom asked her to clean her room. Lily was very excited to clean her room.\n",
            "Lily put her toys on the shelf and started to clean up her toys. She put them on the shelf and started to clean up. She cleaned up the shelves and put them in the shelf. She was so proud of her work.\n",
            "After she finished cleaning, Lily went back to her room. She was so proud of her work. She had done it! She was so proud of her work.\n",
            "\n",
            "\n",
            "Step 2800, Loss: 1.5511512756347656, Elapsed Time: 30.23 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she saw a big, scary dog. The dog was barking and growling. Lily was scared and ran away.\n",
            "Lily ran to her mom and told her what happened. Her mom said, \"Don't worry, Lily. I will protect you from the dog.\" Lily was scared and ran away.\n",
            "But then, the dog stopped and looked at Lily. He was still scared. Lily was brave and followed her mom to the dog. They walked away from the dog. They were safe and warm. The dog was safe. Lily was happy and safe.\n",
            "\n",
            "\n",
            "Step 3000, Loss: 1.5462700128555298, Elapsed Time: 30.35 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she found a big, round ball in her backyard. She was so happy and started to play with it.\n",
            "Suddenly, she heard a voice. It was her mommy, calling her name. \"Lily, come inside! You are so big!\" she said.\n",
            "Lily was so excited to play with the ball. She ran around the yard and played with the ball. She ran around the yard, laughing and having fun.\n",
            "When it was time to go home, Lily said goodbye to her mommy and went back inside. She had a great day at the park and she was so happy.\n",
            "\n",
            "\n",
            "Step 3200, Loss: 1.4854289293289185, Elapsed Time: 30.32 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and make her favorite toy. One day, she found a shiny red ball in her toy box. She was so happy and hugged it tightly.\n",
            "Lily's mom saw her and said, \"Lily, you are so lucky to have that you found a toy. You can play with it.\"\n",
            "Lily smiled and said, \"Thank you, mommy. I love my toy. I love it too.\"\n",
            "Her mom smiled and said, \"You're welcome, Lily. I'm glad you like it. You're a good friend.\"\n",
            "Lily smiled and said, \"Thank you, mommy. I love my toy. I love it too.\"\n",
            "\n",
            "\n",
            "Step 3400, Loss: 1.5167701244354248, Elapsed Time: 30.20 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl. She was very brave and she wanted to explore the world. One day, she decided to go for a walk. She walked and walked until she came to a big hill. She was so excited! She wanted to climb the hill and see what was there. She climbed up the hill and reached the top. She was so excited! She climbed up the top and looked down. She saw lots of trees and flowers. She was so happy to be there. She was so proud of herself. She was so proud of herself. She was so proud of herself. She had been so brave and she was so proud of herself. She was so proud of herself.\n",
            "\n",
            "\n",
            "Step 3600, Loss: 1.4915745258331299, Elapsed Time: 30.31 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock. She picked it up and showed it to her mommy.\n",
            "\"Mommy, look at this rock!\" Lily said.\n",
            "\"That's a rock, Lily. It's very pretty,\" her mommy said.\n",
            "Lily was so excited to see the rock. She put it on her head and looked at it closely. She saw that it was a rock and a rock. She was so happy to see it.\n",
            "\"Mommy, look at this rock!\" Lily said.\n",
            "\"That's great, Lily. It's so pretty!\" her mommy said.\n",
            "Lily smiled and hugged her mommy. She was so proud of her rock. She knew that it was a special rock, and it was very special.\n",
            "\n",
            "\n",
            "Step 3800, Loss: 1.471656322479248, Elapsed Time: 30.41 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock. She picked it up and showed it to her mom.\n",
            "\"Look, Mommy! I found a rock!\" Lily said, holding the rock.\n",
            "\"That's a nice rock, Lily. Let's try it,\" her mom said, \"But we need to be careful and not touch it.\"\n",
            "Lily nodded and said, \"Okay, Mommy. I will be careful.\"\n",
            "So, Lily and her mom went to the store and bought a big rock. They put it in the rock and went inside to get it.\n",
            "\"Thank you, Mommy,\" Lily said, feeling happy and proud of her rock.\n",
            "\"You're welcome, sweetie,\" her mom said, smiling.\n",
            "\"Thank you, Mommy,\" Lily said, feeling proud of her rock and her mom.\n",
            "\n",
            "\n",
            "Step 4000, Loss: 1.475734829902649, Elapsed Time: 30.69 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she was playing in the garden when she saw a big, juicy strawberry. She picked it up and said, \"Yum, this strawberry is so yummy!\"\n",
            "Lily was so happy and said, \"Yum, this strawberry is so yummy!\" Her mom smiled and said, \"Yes, that's a strawberry!\"\n",
            "Lily ate the strawberry and said, \"Yum, this strawberry is so yummy!\" Her mom smiled and said, \"Yes, it is very sweet!\"\n",
            "Lily ate the strawberry and said, \"Yum, this strawberry is so yummy!\" Her mom smiled and said, \"Yes, it is a strawberry!\"\n",
            "\n",
            "\n",
            "Step 4200, Loss: 1.4740411043167114, Elapsed Time: 30.45 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She was very hungry and wanted to eat something yummy. So she went to her mommy and said, \"Mommy, can I have some cookies?\" Her mommy smiled and said, \"Yes, but be careful not to eat too much. We don't want to eat too much.\"\n",
            "Lily was so happy and said, \"Thank you, Mommy. I love you so much!\" Her mommy smiled and said, \"You're welcome, sweetie. I'm glad you like it.\"\n",
            "Lily was so glad she had a good friend. She was glad she had a friend to help her. She was glad she had a friend to help her.\n",
            "\n",
            "\n",
            "Step 4400, Loss: 1.4462167024612427, Elapsed Time: 30.19 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary dog. She was scared and ran away.\n",
            "Lily's mom saw her and said, \"Don't worry, Lily. I will protect you from the dog.\"\n",
            "Lily's mom took her to the doctor. The doctor said, \"Don't worry, Lily. I will protect you from the dog. I will protect you from the dog.\"\n",
            "Lily was so happy and hugged her mom. She said, \"Thank you, mommy. I will protect you from the dog.\"\n",
            "The doctor smiled and said, \"You are welcome, Lily. I'm glad you are safe.\"\n",
            "Lily smiled and said, \"Thank you, mommy. I was so glad I was here. I was safe.\"\n",
            "\n",
            "\n",
            "Step 4600, Loss: 1.425089716911316, Elapsed Time: 30.36 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary monster. It was very scary, so she ran away.\n",
            "Lily was scared, but she was brave. She wanted to be brave and protect herself. She ran back inside to tell her mommy what she was.\n",
            "\"Mommy, mommy, I'm scared. I'm scared. I'm scared. I'm scared. I won't hurt you,\" said Lily.\n",
            "Her mommy hugged her and said, \"Don't worry, Lily. I won't hurt you. I'm here to protect you. Let's go back inside and get some fresh air.\"\n",
            "Lily was relieved and happy that she was brave enough to protect her. She hugged her mommy and said, \"I'm glad you were brave enough to protect me. I'm glad you were brave enough to protect me.\"\n",
            "\n",
            "\n",
            "Step 4800, Loss: 1.446333885192871, Elapsed Time: 30.55 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she was playing in the park when she saw a big tree. She wanted to climb it, but she was scared.\n",
            "Lily's mom saw her and said, \"Don't worry, we can climb the tree. The tree will be okay.\" Lily was happy and climbed the tree. She climbed higher and higher until she reached the top.\n",
            "When she reached the top, she saw a big tree with a branch. She climbed up and climbed up the tree. She was so happy and proud of herself. She climbed down and climbed higher and higher. When she reached the top, she felt proud and happy.\n",
            "\n",
            "\n",
            "Step 5000, Loss: 1.4464895725250244, Elapsed Time: 30.18 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a big box in the attic. She opened it and found a shiny key. She was so excited to see what was inside!\n",
            "Lily opened the box and inside was a beautiful necklace. She was so happy and wore it all the way to the attic. She wore it everywhere she went. She wore it everywhere she went.\n",
            "When she got to the attic, she saw a beautiful necklace with a sparkly stone. She was so happy and wore it all the time. She wore it all the time and was so proud of her necklace. She wore it all the time and was so happy to have found such a special necklace.\n",
            "\n",
            "\n",
            "Step 5200, Loss: 1.4041131734848022, Elapsed Time: 30.21 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite was a big, red ball. One day, Lily's mom asked her to help with the dishes. Lily didn't want to, but she knew it was important to help her.\n",
            "Lily started to clean the dishes, but then she noticed that the dishes were dirty. She didn't know what to do. She asked her mom, \"Mommy, can I help clean the dishes?\"\n",
            "Her mom replied, \"Sure, but be careful. Don't make the dishes dirty.\"\n",
            "Lily was sad, but she knew her mom was right. She cleaned the dishes and put them in the sink. After a few minutes, the dishes were clean and clean. Lily was so happy and grateful for her mom's help. From that day on, Lily knew that even if she was careful, she could always help her mom and make sure she was clean.\n",
            "\n",
            "\n",
            "Step 5400, Loss: 1.4098865985870361, Elapsed Time: 30.49 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, red ball. She wanted to play with it, so she ran to the ball and kicked it.\n",
            "Lily was so happy that she kicked the ball too hard. She kicked it and it flew in the air. The ball flew high in the sky and landed in the grass. Lily was so happy that she could play with the ball.\n",
            "But then, Lily saw a big, red ball stuck in the grass. She tried to get it down, but it was too high. She tried to reach it, but it was too high. She tried to reach it, but it was too high. She tried to reach the ball, but it was too high.\n",
            "Lily was sad. She had an idea. She decided to give the ball to the big, red ball. She took it to the big tree and threw it at the tree. The ball flew up and landed on the tree. Lily was happy and thanked the big tree. She was glad that she could play with the ball.\n",
            "\n",
            "\n",
            "Step 5600, Loss: 1.4038660526275635, Elapsed Time: 30.92 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was very excited because she was going to the park with her mom.\n",
            "When they arrived, Lucy saw a big, white cloud in the sky. She was so excited! She ran over to it and started to play with it.\n",
            "\"Mommy, can I play with the cloud?\" Lucy asked.\n",
            "\"No, sweetie. You can't play with the cloud. It's too big and too dangerous. You can't play with it,\" her mom said.\n",
            "But Lucy was determined to get the cloud out of the sky. She kept playing with the cloud until it was dark.\n",
            "\"I'm going to get the cloud out of the sky,\" Lucy said.\n",
            "\"I'm going to see the cloud. It's so pretty,\" her mom said.\n",
            "Lucy was so happy to have the cloud back. She hugged her mom and said, \"I'm so glad I could play with the cloud. It's so much fun!\"\n",
            "\n",
            "\n",
            "Step 5800, Loss: 1.3921775817871094, Elapsed Time: 30.57 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big tree with a hole in it. She wanted to see what was inside.\n",
            "Lily asked her mom, \"Can I open the hole, please?\"\n",
            "Her mom said, \"No, Lily. We need to find a new place to live.\"\n",
            "Lily was sad, but she understood. She said, \"Okay, Mom. I will be back soon.\"\n",
            "So, Lily opened the hole and found a big, beautiful garden. She was so happy and excited. She ran inside to show her mom.\n",
            "Her mom said, \"Wow, Lily! You are so good at finding things. You need to be careful and not touch anything you want.\"\n",
            "Lily smiled and said, \"Thank you, Mom. I love you.\"\n",
            "\n",
            "\n",
            "Step 6000, Loss: 1.4155082702636719, Elapsed Time: 30.38 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she saw a big, scary dog. The dog was very scared and ran away. Lily was very scared and didn't know what to do.\n",
            "Lily's mom saw her and asked, \"What happened, Lily?\" Lily told her about the dog. Her mom said, \"Don't worry, we will find a way to make it better.\"\n",
            "Lily and her mom went to the store and found a big, scary dog. Lily was scared and said, \"Mommy, can we go home now?\" Her mom said, \"Yes, we can go home now. Let's go home and get home.\"\n",
            "Lily and her mom went home and Lily was very happy. She said, \"Thank you, Mommy. You are a good friend.\"\n",
            "\n",
            "\n",
            "Step 6200, Loss: 1.3869069814682007, Elapsed Time: 30.52 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a big box in her backyard. She opened it and found a big, shiny rock inside. \n",
            "Lily was so happy and showed the rock to her mom. \"Mommy, look at this rock!\" she said. \n",
            "\"That's a nice rock, Lily. It's a rock,\" her mom said. \n",
            "Lily was so excited to see the rock. She picked it up and showed it to her mom. \"Look, Mommy! I found a rock!\" she said. \n",
            "Her mom smiled and said, \"That's a nice rock, Lily. You can keep it for a while, but you need to find it.\" \n",
            "Lily nodded and said, \"Okay, Mommy. I found it in the backyard. It's a rock and it's a rock.\" \n",
            "Lily was so happy and hugged her mom. She said, \"Thank you, Mommy. I love my rock.\"\n",
            "\n",
            "\n",
            "Step 6400, Loss: 1.398075819015503, Elapsed Time: 30.82 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a big, red ball in the grass. She picked it up and showed it to her mom.\n",
            "\"Look, Mommy! I found a ball!\" Lily said.\n",
            "\"That's great, Lily. Let's play with it,\" her mom said.\n",
            "Lily and her mom played with the ball and had a lot of fun. They laughed and had a great time. When it was time to go home, Lily said goodbye to her mom and went to bed.\n",
            "\n",
            "\n",
            "Step 6600, Loss: 1.4032621383666992, Elapsed Time: 30.22 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she saw a big, scary monster. She was scared and didn't know what to do.\n",
            "Lily's mom saw her and said, \"Don't worry, Lily. Monsters are scary and you're not scared.\" Lily felt better and said, \"I will be brave and I will be brave.\"\n",
            "So, Lily went to her mom and said, \"Mommy, I am brave and I am brave and I will be brave.\" Her mom smiled and said, \"You are brave and brave, Lily. You are brave and strong.\" Lily felt proud and happy. She knew she was brave and she was brave and she was proud of herself.\n",
            "\n",
            "\n",
            "Step 6800, Loss: 1.4198578596115112, Elapsed Time: 30.41 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite toy was a big, red car. One day, Lily's mom asked her to help her fold the clothes. Lily was happy to help her mom fold the clothes and put them in a big pile.\n",
            "Lily folded the clothes and put them in a big pile. She folded the clothes and put them in a big pile. She was very proud of her work. She showed her mom how to fold the clothes and put them in a big pile. Her mom was proud of her for being so helpful.\n",
            "Lily was very proud of her work. She knew that folding was important for her mom and her mom. From that day on, Lily knew that folding things can make her feel good.\n",
            "\n",
            "\n",
            "Step 7000, Loss: 1.4114797115325928, Elapsed Time: 30.27 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lucy. She was very sad because she had no friends.\n",
            "One day, Lucy's mommy said, \"Lucy, I have a surprise for you. Let's go to the park and play on the swings.\"\n",
            "Lucy was so excited. She ran to the park and jumped on the swings. She was so happy! She ran around the park, laughing and having lots of fun.\n",
            "When Lucy got to the park, she saw a big tree. She climbed up the tree and sat down. She looked up and saw a big tree. She climbed up the tree and sat down.\n",
            "Lucy said, \"This tree is so tall and strong!\"\n",
            "The tree said, \"Yes, it is. You can come down here anytime you want.\"\n",
            "Lucy smiled and said, \"Thank you, tree. I will come back soon.\"\n",
            "And so, Lucy and her mommy went home and Lucy had a wonderful day at the park.\n",
            "\n",
            "\n",
            "Step 7200, Loss: 1.3879085779190063, Elapsed Time: 30.53 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and run around. One day, she went to the park and saw a big tree. She wanted to climb it, but she was scared.\n",
            "Suddenly, she heard a loud noise. It was a big truck! The truck was driving down the street. Lily was scared and ran away. She ran and ran until she reached the top.\n",
            "When she got to the top, she saw a big, scary dog. The dog was scared and ran away. Lily was safe and happy. She went back home and told her mom about the big, scary dog.\n",
            "\n",
            "\n",
            "Step 7400, Loss: 1.395064353942871, Elapsed Time: 30.08 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went to the park to play. She saw a big, red ball and she wanted to play with it.\n",
            "Lily's friend, Timmy, came over to play. \"What are you doing, Lily?\" he asked.\n",
            "\"I'm playing with the ball,\" Lily said.\n",
            "Timmy smiled and said, \"That's a great idea. Let's play hide and seek!\"\n",
            "So, Lily and Timmy played hide and seek. They ran around the park, laughing and having fun. After a while, they were both tired and decided to rest.\n",
            "Lily said, \"I'm tired, but I can still play with the ball. I can play with it. I'll take a nap and I'll have a good time.\"\n",
            "Timmy smiled and said, \"That's a great idea. Let's play hide and seek again soon!\"\n",
            "Lily and Timmy laughed and played until the sun went down. Then, they both went home, happy and tired from their fun day.\n",
            "\n",
            "\n",
            "Step 7600, Loss: 1.371392846107483, Elapsed Time: 31.01 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside. One day, she saw a big, scary bug. She was scared and didn't know what to do.\n",
            "Lily asked her mom, \"What is that?\"\n",
            "Her mom said, \"That's a bug. It's not a bug. It's a bug. It can hurt you.\"\n",
            "Lily was scared of the bug. She said, \"I want to be a bug. It's not scary. It's just a bug.\"\n",
            "Her mom said, \"Don't be scared. It's just a bug. It's not a bug. It's just a bug. You can be a bug.\"\n",
            "Lily was happy. She learned that bugs are not scary. She learned that bugs are not scary. She also learned that bugs are not scary. She also learned that bugs are not scary.\n",
            "\n",
            "\n",
            "Step 7800, Loss: 1.3667292594909668, Elapsed Time: 30.40 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she found a big, shiny rock in her backyard. She picked it up and showed it to her mom.\n",
            "\"Look, Mommy! I found a rock!\" Lily said.\n",
            "\"That's a rock, Lily. It's a rock. It's a special rock. It's a special rock. It's a special rock. It's a special rock. It's a special rock. It's special rock. It's special and special,\" her mom said.\n",
            "Lily was so happy that she could rock it. She put the rock in her pocket and continued to explore the world around her. She was so proud of her rock and the rock.\n",
            "\n",
            "\n",
            "Step 8000, Loss: 1.3470700979232788, Elapsed Time: 30.28 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a shiny rock in her backyard. She picked it up and showed it to her mom.\n",
            "\"Look, Lily! I found a rock!\" said her mom.\n",
            "Lily was so happy to see the rock. She showed it to her mom and said, \"Look, Mom! I found a rock!\"\n",
            "Her mom smiled and said, \"That's a great rock, Lily. Let's put it in a special place for the rock.\"\n",
            "Lily and her mom put the rock in the special place. They put the rock in the special place. When they were done, they went inside and Lily showed her mom the rock. Her mom said, \"That's a great rock, Lily. Let's put it in the special place.\"\n",
            "Lily and her mom put the rock in the special place. They put the rock in the special place. When they were done, they went inside and Lily showed her mom the rock. Her mom said, \"That's a great rock, Lily. You are a great friend.\"\n",
            "\n",
            "\n",
            "Step 8200, Loss: 1.3906655311584473, Elapsed Time: 30.91 seconds\n",
            "Generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and explore. One day, she found a big, shiny rock. She picked it up and showed it to her friend, Timmy.\n",
            "\"Look, Timmy! I found a rock!\" Lily said.\n",
            "Timmy looked at the rock and said, \"Wow, that's a cool rock. Let's keep it safe.\"\n",
            "Lily agreed and they continued to play in the park. They had a great time playing in the park. When it was time to go home, Lily said, \"I'm glad we found the rock. It's so cool and shiny.\"\n",
            "Timmy smiled and said, \"I'm glad we found it. Let's keep it safe.\" And they continued to play in the park until it was time to go home.\n",
            "Final generated text:\n",
            "Once upon a time, there was a little girl named Lily. She loved to play outside and look at the flowers. One day, she saw a big, red flower. She wanted to pick it, but it was too high.\n",
            "Lily's mom saw her and said, \"Lily, you need to be careful. The flower is very strong. It can hurt you.\" Lily nodded and said, \"Okay, Mommy.\"\n",
            "Lily's mom picked the flower and put it in her pocket. Lily was happy and said, \"Thank you, Mommy!\" They played outside and had lots of fun. Lily was proud of her flower.\n"
          ]
        }
      ],
      "source": [
        "with mesh:\n",
        "    model = create_model(rngs=nnx.Rngs(0))\n",
        "    optimizer = nnx.ModelAndOptimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
        "\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "print(\"Initial generated text:\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}\n",
        "\n",
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    for batch in text_dl:\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "            continue  # skip the remaining elements\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "        with jax.set_mesh(mesh):\n",
        "          train_step(\n",
        "              model,\n",
        "              optimizer,\n",
        "              metrics,\n",
        "              jax.device_put(\n",
        "                  (input_batch, target_batch), P(\"batch\", None)\n",
        "              ),\n",
        "          )\n",
        "\n",
        "        if (step + 1) % 200 == 0:\n",
        "            for metric, value in metrics.compute().items():\n",
        "                metrics_history[f\"train_{metric}\"].append(value)\n",
        "            metrics.reset()\n",
        "\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(\n",
        "                f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "            )\n",
        "            start_time = time.time()\n",
        "\n",
        "            print(\"Generated text:\")\n",
        "            generated_text = model.generate_text(maxlen, start_tokens)\n",
        "\n",
        "        step += 1\n",
        "\n",
        "# Final text generation\n",
        "print(\"Final generated text:\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaLs6TD0lt5"
      },
      "source": [
        "Visualize the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "B6Eg1Cz2y_iP",
        "outputId": "0b905f8d-b669-451e-a4b8-1d2ae94d3b86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soPqiR1JNmjf"
      },
      "source": [
        "## Saving the checkpoint\n",
        "\n",
        "Save the model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkoFGCgSZ1yz",
        "outputId": "44cb61e4-780e-4880-a1b1-a147e01870fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array_metadatas       d\t\t      _METADATA        _sharding\n",
            "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', state)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "machine_shape": "hm",
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}