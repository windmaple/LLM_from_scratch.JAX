{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvP1eNN_pExM"
   },
   "source": [
    "# GPT2 DPO finetuning\n",
    "\n",
    "This notebook demonstrates how to finetune a instruction-tuned GPT2(124M) model with [Direct Preference Optimization](https://arxiv.org/pdf/2305.18290). Note that this notebook only works on Kaggle TPU v3 or Cloud TPU v3+ (Colab TPU v2 simply does not have enough HBM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD3bo9FxhrTE"
   },
   "source": [
    "## Determine platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:36.783949Z",
     "iopub.status.busy": "2025-07-16T08:10:36.783670Z",
     "iopub.status.idle": "2025-07-16T08:10:36.800925Z",
     "shell.execute_reply": "2025-07-16T08:10:36.796280Z",
     "shell.execute_reply.started": "2025-07-16T08:10:36.783924Z"
    },
    "id": "7XcEXnSbhhKV",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists('/kaggle/'):\n",
    "  platform = \"Kaggle\"\n",
    "else:\n",
    "  # Assume using Cloud TPU otherwise\n",
    "  platform = \"GCP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install JAX and Flax first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:36.806070Z",
     "iopub.status.busy": "2025-07-16T08:10:36.805064Z",
     "iopub.status.idle": "2025-07-16T08:10:45.730314Z",
     "shell.execute_reply": "2025-07-16T08:10:45.726736Z",
     "shell.execute_reply.started": "2025-07-16T08:10:36.806027Z"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "e99591ab-c664-4e18-8d87-fb092165876e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q jax-ai-stack[grain]\n",
    "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
    "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
    "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cWxBvz6bZDd"
   },
   "source": [
    "Confirm we have TPUs set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:45.733355Z",
     "iopub.status.busy": "2025-07-16T08:10:45.733110Z",
     "iopub.status.idle": "2025-07-16T08:10:49.220466Z",
     "shell.execute_reply": "2025-07-16T08:10:49.215515Z",
     "shell.execute_reply.started": "2025-07-16T08:10:45.733330Z"
    },
    "id": "uZUaKdi5bSEN",
    "outputId": "4d72a412-2f71-486b-e41f-6b35e20eb0a5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "E0000 00:00:1752653446.244167    3695 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n",
      "=== Source Location Trace: === \n",
      "learning/45eac/tfrc/runtime/common_lib.cc:230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Take care of the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:49.221869Z",
     "iopub.status.busy": "2025-07-16T08:10:49.221567Z",
     "iopub.status.idle": "2025-07-16T08:10:50.712682Z",
     "shell.execute_reply": "2025-07-16T08:10:50.708194Z",
     "shell.execute_reply.started": "2025-07-16T08:10:49.221844Z"
    },
    "id": "MKYFNOhdLq98",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import optax, orbax\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from jax.experimental import mesh_utils\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
    "import numpy as np\n",
    "import tiktoken, time, wandb\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors import safe_open\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Build the model\n",
    "\n",
    "Define the device mesh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:50.716157Z",
     "iopub.status.busy": "2025-07-16T08:10:50.714986Z",
     "iopub.status.idle": "2025-07-16T08:10:50.725714Z",
     "shell.execute_reply": "2025-07-16T08:10:50.720762Z",
     "shell.execute_reply.started": "2025-07-16T08:10:50.716131Z"
    },
    "id": "xuMlCK3Q8WJD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:50.728835Z",
     "iopub.status.busy": "2025-07-16T08:10:50.728021Z",
     "iopub.status.idle": "2025-07-16T08:10:51.016609Z",
     "shell.execute_reply": "2025-07-16T08:10:51.012515Z",
     "shell.execute_reply.started": "2025-07-16T08:10:50.728810Z"
    },
    "id": "iWbkk1V7-Isg",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:51.017821Z",
     "iopub.status.busy": "2025-07-16T08:10:51.017607Z",
     "iopub.status.idle": "2025-07-16T08:10:51.028164Z",
     "shell.execute_reply": "2025-07-16T08:10:51.023657Z",
     "shell.execute_reply.started": "2025-07-16T08:10:51.017801Z"
    },
    "id": "GRhiDsCrMZRp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "GPT2_variant = \"GPT2\"\n",
    "\n",
    "num_transformer_blocks = 12\n",
    "seqlen = 1024\n",
    "embed_dim = 768\n",
    "num_heads = 12\n",
    "feed_forward_dim = 4 * embed_dim\n",
    "batch_size = 64\n",
    "dropout_rate = 0.1\n",
    "\n",
    "init_learning_rate = 1e-5 #5e-4\n",
    "weight_decay = 1e-1\n",
    "top_k = 10\n",
    "sampling_temp = 2\n",
    "dtype = jnp.bfloat16\n",
    "param_dtype = jnp.float32\n",
    "beta = 0.1\n",
    "max_steps = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "Now define the model architecture, which is the same as in our previous instruction tuning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:51.030351Z",
     "iopub.status.busy": "2025-07-16T08:10:51.030093Z",
     "iopub.status.idle": "2025-07-16T08:10:51.070930Z",
     "shell.execute_reply": "2025-07-16T08:10:51.065052Z",
     "shell.execute_reply.started": "2025-07-16T08:10:51.030328Z"
    },
    "id": "z0p-IHurrB9i",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len), dtype=jnp.bool_))\n",
    "\n",
    "class CustomMHA(nnx.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate, layer_idx, rngs):\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // self.num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        kernel_init = nnx.with_partitioning(\n",
    "            nnx.initializers.xavier_uniform(), (P(None, \"model\"),)\n",
    "        )\n",
    "\n",
    "        self.query = nnx.Linear(\n",
    "            embed_dim, embed_dim, rngs=rngs, use_bias=False, kernel_init=kernel_init\n",
    "        )\n",
    "        self.key = nnx.Linear(\n",
    "            embed_dim, embed_dim, rngs=rngs, use_bias=False, kernel_init=kernel_init\n",
    "        )\n",
    "        self.value = nnx.Linear(\n",
    "            embed_dim, embed_dim, rngs=rngs, use_bias=False, kernel_init=kernel_init\n",
    "        )\n",
    "        self.out = nnx.Linear(\n",
    "            embed_dim, embed_dim, rngs=rngs, use_bias=False, kernel_init=kernel_init\n",
    "        )\n",
    "\n",
    "        self.q_bias = nnx.Param(\n",
    "            jnp.zeros((embed_dim,), dtype=param_dtype), sharding=P(\"model\")\n",
    "        )\n",
    "        self.k_bias = nnx.Param(\n",
    "            jnp.zeros((embed_dim,), dtype=param_dtype), sharding=P(\"model\")\n",
    "        )\n",
    "        self.v_bias = nnx.Param(\n",
    "            jnp.zeros((embed_dim,), dtype=param_dtype), sharding=P(\"model\")\n",
    "        )\n",
    "\n",
    "        self.out_bias = nnx.Param(\n",
    "            jnp.zeros((embed_dim,), dtype=param_dtype), sharding=P(\"model\")\n",
    "        )\n",
    "\n",
    "        self.dropout = nnx.Dropout(dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self, x, mask, padding_mask=None, training: bool = False, rngs: nnx.Rngs = None\n",
    "    ):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        q = self.query(x) + self.q_bias\n",
    "        k = self.key(x) + self.k_bias\n",
    "        v = self.value(x) + self.v_bias\n",
    "\n",
    "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
    "            (0, 2, 1, 3)\n",
    "        )\n",
    "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
    "            (0, 2, 1, 3)\n",
    "        )\n",
    "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
    "            (0, 2, 1, 3)\n",
    "        )\n",
    "\n",
    "        attn_weights = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(\n",
    "            self.head_dim\n",
    "        )\n",
    "\n",
    "        combined_mask = mask\n",
    "        if padding_mask is not None:\n",
    "            combined_mask = jnp.logical_and(mask, padding_mask)\n",
    "\n",
    "        if combined_mask is not None:\n",
    "            attn_weights = jnp.where(combined_mask, attn_weights, -jnp.inf)\n",
    "\n",
    "        attn_weights = nnx.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = self.dropout(attn_weights, deterministic=not training, rngs=rngs)\n",
    "\n",
    "        attn_output = jnp.matmul(attn_weights, v)\n",
    "        attn_output = attn_output.transpose((0, 2, 1, 3)).reshape(\n",
    "            (batch_size, seq_len, self.embed_dim)\n",
    "        )\n",
    "\n",
    "        output = self.out(attn_output) + self.out_bias\n",
    "        return output\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        ff_dim: int,\n",
    "        dropout_rate: float,\n",
    "        rngs: nnx.Rngs,\n",
    "        layer_idx: int,\n",
    "    ):\n",
    "        self.layer_norm1 = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(\n",
    "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.mha = CustomMHA(embed_dim, num_heads, dropout_rate, layer_idx, rngs)\n",
    "        self.dropout1 = nnx.Dropout(rate=dropout_rate)\n",
    "        self.layer_norm2 = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(\n",
    "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=embed_dim,\n",
    "            out_features=ff_dim,\n",
    "            kernel_init=nnx.with_partitioning(\n",
    "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=ff_dim,\n",
    "            out_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(\n",
    "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self, inputs, padding_mask=None, training: bool = False, rngs: nnx.Rngs = None\n",
    "    ):\n",
    "        input_shape = inputs.shape\n",
    "        bs, seq_len, emb_sz = input_shape\n",
    "\n",
    "        attention_output = self.mha(\n",
    "            self.layer_norm1(inputs),\n",
    "            mask=causal_attention_mask(seq_len),\n",
    "            padding_mask=padding_mask,\n",
    "            training=training,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        x = inputs + self.dropout1(\n",
    "            attention_output, deterministic=not training, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # MLP\n",
    "        mlp_output = self.linear1(self.layer_norm2(x))\n",
    "        mlp_output = nnx.gelu(mlp_output)\n",
    "        mlp_output = self.linear2(mlp_output)\n",
    "        mlp_output = self.dropout2(mlp_output, deterministic=not training, rngs=rngs)\n",
    "\n",
    "        return x + mlp_output\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.token_emb = nnx.Embed(\n",
    "            num_embeddings=vocab_size,\n",
    "            features=embed_dim,\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "        self.pos_emb = nnx.Embed(\n",
    "            num_embeddings=seqlen,\n",
    "            features=embed_dim,\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        token_embedding = self.token_emb(x)\n",
    "        return self.token_emb, token_embedding + position_embedding\n",
    "\n",
    "\n",
    "class GPT2(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seqlen: int,\n",
    "        vocab_size: int,\n",
    "        embed_dim: int,\n",
    "        num_heads: int,\n",
    "        rate: float,\n",
    "        feed_forward_dim: int,\n",
    "        num_transformer_blocks: int,\n",
    "        rngs: nnx.Rngs,\n",
    "    ):\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "            seqlen, vocab_size, embed_dim, rngs=rngs\n",
    "        )\n",
    "        self.dropout = nnx.Dropout(rate=rate)\n",
    "\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                feed_forward_dim,\n",
    "                dropout_rate,\n",
    "                rngs=rngs,\n",
    "                layer_idx=i,\n",
    "            )\n",
    "            for i in range(num_transformer_blocks)\n",
    "        ]\n",
    "\n",
    "        self.layer_norm = nnx.LayerNorm(\n",
    "            epsilon=1e-6,\n",
    "            num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(\n",
    "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            bias_init=nnx.with_partitioning(\n",
    "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            param_dtype=param_dtype,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self, inputs, padding_mask=None, training: bool = False, rngs: nnx.Rngs = None\n",
    "    ):\n",
    "        token_embedding, x = self.embedding_layer(inputs)\n",
    "        x = self.dropout(x, deterministic=not training, rngs=rngs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(\n",
    "                x, padding_mask=padding_mask, training=training, rngs=rngs\n",
    "            )\n",
    "        x = self.layer_norm(x)\n",
    "        outputs = token_embedding.attend(x)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    @nnx.jit\n",
    "    def sample_from(logits, key):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits / sampling_temp)\n",
    "        return jax.random.choice(key, indices, p=logits)\n",
    "\n",
    "    @staticmethod\n",
    "    @nnx.jit\n",
    "    def generate_step_static(params, static_def, padded_tokens, length, key):\n",
    "        padding_mask = jnp.arange(seqlen) < length\n",
    "        padding_mask = padding_mask.reshape(1, 1, 1, seqlen)\n",
    "\n",
    "        model = nnx.merge(params, static_def)\n",
    "        logits = model(padded_tokens, padding_mask=padding_mask, training=False)\n",
    "        last_token_logits = logits[:, length - 1, :]\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        next_token = GPT2.sample_from(jnp.squeeze(last_token_logits), subkey)\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        key = jax.random.PRNGKey(int(time.time()))\n",
    "\n",
    "        params, static_def = nnx.split(self)\n",
    "\n",
    "        tokens = jnp.array(start_tokens, dtype=jnp.int32)[None, :]\n",
    "        end_token = tokenizer.encode(\n",
    "            \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        current_len = tokens.shape[1]\n",
    "        padded_tokens = jnp.pad(tokens, ((0, 0), (0, seqlen - current_len)), \"constant\")\n",
    "\n",
    "        print(tokenizer.decode(tokens[0]), end=\"\", flush=True)\n",
    "\n",
    "        for i in range(max_tokens):\n",
    "            key, subkey = jax.random.split(key)\n",
    "\n",
    "            next_token = self.generate_step_static(\n",
    "                params, static_def, padded_tokens, current_len, subkey\n",
    "            )\n",
    "\n",
    "            if next_token.item() == end_token:\n",
    "                break\n",
    "\n",
    "            print(tokenizer.decode([next_token.item()]), end=\"\", flush=True)\n",
    "\n",
    "            padded_tokens = padded_tokens.at[:, current_len].set(next_token.item())\n",
    "            current_len += 1\n",
    "\n",
    "        final_tokens = padded_tokens[0, :current_len]\n",
    "        return tokenizer.decode(final_tokens.tolist())\n",
    "\n",
    "\n",
    "def create_model(rngs):\n",
    "    return GPT2(\n",
    "        seqlen,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        num_heads,\n",
    "        dropout_rate,\n",
    "        feed_forward_dim,\n",
    "        num_transformer_blocks,\n",
    "        rngs=rngs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBfT1dp5hMUm"
   },
   "source": [
    "Use Weights and Biases to track training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:51.072961Z",
     "iopub.status.busy": "2025-07-16T08:10:51.072738Z",
     "iopub.status.idle": "2025-07-16T08:10:53.678568Z",
     "shell.execute_reply": "2025-07-16T08:10:53.674014Z",
     "shell.execute_reply.started": "2025-07-16T08:10:51.072941Z"
    },
    "id": "IbhEtsganEWg",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250716_081052-6t1l9cnj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-DPO/runs/6t1l9cnj' target=\"_blank\">stoic-feather-15</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-DPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/windmaple/GPT2-DPO' target=\"_blank\">https://wandb.ai/windmaple/GPT2-DPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/windmaple/GPT2-DPO/runs/6t1l9cnj' target=\"_blank\">https://wandb.ai/windmaple/GPT2-DPO/runs/6t1l9cnj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-DPO/runs/6t1l9cnj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb3746f8430>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if platform == \"Colab\":\n",
    "  from google.colab import userdata\n",
    "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
    "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
    "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
    "elif platform == \"Kaggle\":\n",
    "  from kaggle_secrets import UserSecretsClient\n",
    "  user_secrets = UserSecretsClient()\n",
    "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
    "else:\n",
    "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='GPT2-DPO',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "      'architecture': GPT2_variant,\n",
    "      'dataset': 'OpenWebText',\n",
    "      'platform': platform,\n",
    "      'dtype': dtype,\n",
    "      'param_dtype': param_dtype,\n",
    "      'init_learning_rate': init_learning_rate,\n",
    "      'num_transformer_blocks': num_transformer_blocks,\n",
    "      'seqlen': seqlen,\n",
    "      'embed_dim': embed_dim,\n",
    "      'num_heads': num_heads,\n",
    "      'feed_forward_dim': feed_forward_dim,\n",
    "      'max_steps': 'unknown',\n",
    "      'batch_size': batch_size,\n",
    "      'weight_decay': weight_decay,\n",
    "      'beta': beta\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## DPO\n",
    "\n",
    "DPO training requires a model to be trained and a separate reference model to compute loss. We are going to initialize them from our previous instruct tuned 124M GPT2 model. On Kaggle, you need to manually add the [model](https://www.kaggle.com/models/windmaple/gpt2/jax/124m-it) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:10:53.681595Z",
     "iopub.status.busy": "2025-07-16T08:10:53.681342Z",
     "iopub.status.idle": "2025-07-16T08:11:00.982557Z",
     "shell.execute_reply": "2025-07-16T08:11:00.976518Z",
     "shell.execute_reply.started": "2025-07-16T08:10:53.681569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1251: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "from orbax.checkpoint import PyTreeCheckpointer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Create the model and load the pretrained weights\n",
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "state = nnx.state(model)\n",
    "checkpointer = PyTreeCheckpointer()\n",
    "\n",
    "checkpoint_path = '/kaggle/input/gpt2/jax/124m-it/1'\n",
    "state = checkpointer.restore(checkpoint_path, item=state)\n",
    "nnx.update(model, state)\n",
    "\n",
    "# Create a reference model with the same pretrained weights\n",
    "ref_model = create_model(rngs=nnx.Rngs(1))\n",
    "ref_state = nnx.state(ref_model)\n",
    "ref_state = checkpointer.restore(checkpoint_path, item=ref_state)\n",
    "nnx.update(ref_model, ref_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a [pairwise preference dataset from Argilla](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences-cleaned). And create a preprocessing helper function and a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:11:01.078309Z",
     "iopub.status.busy": "2025-07-16T08:11:01.077665Z",
     "iopub.status.idle": "2025-07-16T08:11:02.353937Z",
     "shell.execute_reply": "2025-07-16T08:11:02.346512Z",
     "shell.execute_reply.started": "2025-07-16T08:11:01.078271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\n",
    "    \"argilla/ultrafeedback-binarized-preferences-cleaned\", split=\"train\"\n",
    ")\n",
    "\n",
    "max_steps = ds.num_rows // batch_size\n",
    "\n",
    "# Define the template for the chat messages\n",
    "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\"\n",
    "\n",
    "# Define a helper function to preprocess the dataset\n",
    "def preprocess_function(examples):\n",
    "    def format_and_tokenize(messages):\n",
    "        prompts = []\n",
    "        for msg_pair in messages:\n",
    "            instruction = msg_pair[0][\"content\"]\n",
    "            output = msg_pair[1][\"content\"] if len(msg_pair) > 1 else \"\"\n",
    "            prompts.append(\n",
    "                template.format(instruction=instruction, input=\"\", output=output)\n",
    "            )\n",
    "\n",
    "        tokenized_prompts = [tokenizer.encode(p) for p in prompts]\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for tokens in tokenized_prompts:\n",
    "            if len(tokens) > seqlen:\n",
    "                tokens = tokens[:seqlen]\n",
    "\n",
    "            padding_len = seqlen - len(tokens)\n",
    "            input_ids.append(tokens + [50256] * padding_len)\n",
    "            attention_masks.append([1] * len(tokens) + [0] * padding_len)\n",
    "\n",
    "        return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "    chosen_input_ids, chosen_attention_mask = format_and_tokenize(examples[\"chosen\"])\n",
    "    rejected_input_ids, rejected_attention_mask = format_and_tokenize(\n",
    "        examples[\"rejected\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"chosen_input_ids\": chosen_input_ids,\n",
    "        \"chosen_attention_mask\": chosen_attention_mask,\n",
    "        \"rejected_input_ids\": rejected_input_ids,\n",
    "        \"rejected_attention_mask\": rejected_attention_mask,\n",
    "    }\n",
    "\n",
    "# Create a data loader.\n",
    "def data_loader(dataset, batch_size):\n",
    "    while True:\n",
    "        for i in range(0, len(dataset[\"chosen\"]), batch_size):\n",
    "            batch = {\n",
    "                \"chosen\": dataset[\"chosen\"][i : i + batch_size],\n",
    "                \"rejected\": dataset[\"rejected\"][i : i + batch_size],\n",
    "            }\n",
    "            processed_batch = preprocess_function(batch)\n",
    "            yield processed_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define functions to calculate DPO loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T08:11:02.359180Z",
     "iopub.status.busy": "2025-07-16T08:11:02.358721Z",
     "iopub.status.idle": "2025-07-16T08:11:02.376609Z",
     "shell.execute_reply": "2025-07-16T08:11:02.372076Z",
     "shell.execute_reply.started": "2025-07-16T08:11:02.359154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the DPO loss function\n",
    "def dpo_loss(\n",
    "    policy_chosen_logps,\n",
    "    policy_rejected_logps,\n",
    "    ref_chosen_logps,\n",
    "    ref_rejected_logps,\n",
    "    beta,\n",
    "):\n",
    "    pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "    ref_logratios = ref_chosen_logps - ref_rejected_logps\n",
    "    return -jax.nn.log_sigmoid(beta * (pi_logratios - ref_logratios))\n",
    "\n",
    "\n",
    "# Define a function to get the log probabilities of the sequences\n",
    "def get_log_probs(logits, labels, attention_mask):\n",
    "    batch_size, seq_len = labels.shape\n",
    "    assert logits.shape[:2] == (batch_size, seq_len), f\"Shape mismatch: {logits.shape} vs {labels.shape}\"\n",
    "\n",
    "    # Get the log probabilities from the logits.\n",
    "    log_probs = jax.nn.log_softmax(logits, axis=-1)\n",
    "    # Get the log probabilities of the labels.\n",
    "    log_probs_labels = jnp.squeeze(\n",
    "        jnp.take_along_axis(log_probs, labels[:, :, None], axis=-1), -1\n",
    "    )\n",
    "    # Set the log probabilities of the padding tokens to 0.\n",
    "    return (log_probs_labels * attention_mask).sum(axis=-1)\n",
    "\n",
    "\n",
    "def calculate_loss(model, ref_model, batch, rngs):\n",
    "    # Get the logits from the policy model.\n",
    "    policy_chosen_logits = model(batch[\"chosen_input_ids\"], training=True, rngs=rngs)\n",
    "    policy_rejected_logits = model(\n",
    "        batch[\"rejected_input_ids\"], training=True, rngs=rngs\n",
    "    )\n",
    "\n",
    "    # Get the log probabilities from the policy model.\n",
    "    policy_chosen_logps = get_log_probs(\n",
    "        policy_chosen_logits,\n",
    "        batch[\"chosen_input_ids\"],\n",
    "        batch[\"chosen_attention_mask\"],\n",
    "    )\n",
    "    policy_rejected_logps = get_log_probs(\n",
    "        policy_rejected_logits,\n",
    "        batch[\"rejected_input_ids\"],\n",
    "        batch[\"rejected_attention_mask\"],\n",
    "    )\n",
    "\n",
    "    # Get the logits from the reference model.\n",
    "    ref_chosen_logits = jax.lax.stop_gradient(ref_model(batch[\"chosen_input_ids\"], training=False))\n",
    "    ref_rejected_logits = jax.lax.stop_gradient(ref_model(batch[\"rejected_input_ids\"], training=False))\n",
    "\n",
    "    # Get the log probabilities from the reference model.\n",
    "    ref_chosen_logps = get_log_probs(\n",
    "        ref_chosen_logits,\n",
    "        batch[\"chosen_input_ids\"],\n",
    "        batch[\"chosen_attention_mask\"],\n",
    "    )\n",
    "    ref_rejected_logps = get_log_probs(\n",
    "        ref_rejected_logits,\n",
    "        batch[\"rejected_input_ids\"],\n",
    "        batch[\"rejected_attention_mask\"],\n",
    "    )\n",
    "\n",
    "    # Calculate the DPO loss.\n",
    "    loss = dpo_loss(\n",
    "        policy_chosen_logps,\n",
    "        policy_rejected_logps,\n",
    "        ref_chosen_logps,\n",
    "        ref_rejected_logps,\n",
    "        beta,\n",
    "    )\n",
    "    return jnp.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2025-07-16T08:11:02.381802Z",
     "iopub.status.busy": "2025-07-16T08:11:02.381562Z",
     "iopub.status.idle": "2025-07-16T08:38:29.574778Z",
     "shell.execute_reply": "2025-07-16T08:38:29.569394Z",
     "shell.execute_reply.started": "2025-07-16T08:11:02.381779Z"
    },
    "id": "6uHcKfOvbcsx",
    "outputId": "9f2885b7-3cfb-4562-d54a-6bca1c651215",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Generated text before DPO:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the future for human?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "As human beings progress to their next evolutionary stages on a much larger and more advanced level and become even faster-Evolutron. In the next evolutionary stage they will be able to survive for billions of years and will become a species with a special form. Their genetic material is made up of cells that have unique features such as wings, hair, and a single organelle within the wings of a human. The cells will eventually have evolved over time, and their genetic structure and function has been modified drastically as human evolution takes a far more complex journey.Step 0, Loss: 5.59375\n",
      "Step 50, Loss: 2.59375\n",
      "Step 100, Loss: 2.09375\n",
      "Step 150, Loss: 1.10156\n",
      "Step 200, Loss: 2.0625\n",
      "Step 250, Loss: 1.96094\n",
      "Step 300, Loss: 2.07812\n",
      "Step 350, Loss: 2.07812\n",
      "Step 400, Loss: 1.9375\n",
      "Step 450, Loss: 1.125\n",
      "Step 500, Loss: 1.75781\n",
      "Step 550, Loss: 1.36719\n",
      "Step 600, Loss: 1.15625\n",
      "Step 650, Loss: 1.5625\n",
      "Step 700, Loss: 1.41406\n",
      "Step 750, Loss: 1.71094\n",
      "Step 800, Loss: 1.35938\n",
      "Step 850, Loss: 0.84375\n",
      "Step 900, Loss: 0.882812\n",
      "Step 950, Loss: 1.02344\n",
      "***Generated text after DPO:\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is the future for human?\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Response:\n",
      "The future for humans may include an increase in AI-led development initiatives, improved medical technology, improved communication and collaboration technologies, or improved transportation technologies. In the short term, advances in automation may lead to advances in artificial technologies such as AI and Machine Learning technology. However, human labor is becoming more important in many fields as machines can be used by machines for tasks with humans to perform repetitive tasks. Additionally, advances in technology could help in increasing the efficiency of tasks and making tasks simpler."
     ]
    }
   ],
   "source": [
    "start_prompt = template.format(\n",
    "    instruction=\"What is the future for human?\",\n",
    "    input=\"\",\n",
    "    output=\"\",\n",
    ")\n",
    "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
    "print(f\"***Generated text before DPO:\")\n",
    "generated_text = model.generate_text(seqlen // 5, start_tokens)\n",
    "\n",
    "# Define the training step\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, ref_model, batch, rngs):\n",
    "    # Calculate the loss and gradients with respect to the model's parameters.\n",
    "    loss, grads = nnx.value_and_grad(calculate_loss, argnums=0)(\n",
    "        model, ref_model, batch, rngs\n",
    "    )\n",
    "    # Update the model's parameters.\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model, \n",
    "    optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),  # Add gradient clipping\n",
    "        optax.adamw(learning_rate=init_learning_rate, weight_decay=weight_decay)\n",
    "    )\n",
    ")\n",
    "\n",
    "data_gen = data_loader(ds, batch_size)\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "# Train the model\n",
    "for step in range(max_steps):\n",
    "    batch = next(data_gen)\n",
    "    batch = jax.device_put(batch, NamedSharding(mesh, P(\"batch\")))\n",
    "    loss = train_step(model, optimizer, ref_model, batch, rngs=rngs)\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss}\")        \n",
    "        wandb.log(data={'Loss': loss}, step=step)\n",
    "\n",
    "# Generate text after DPO\n",
    "print(f\"***Generated text after DPO:\")\n",
    "generated_text = model.generate_text(seqlen // 5, start_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANTHzx4TrzTE",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "isSourceIdPinned": false,
     "modelId": 239405,
     "modelInstanceId": 379972,
     "sourceId": 471295,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 239405,
     "modelInstanceId": 217697,
     "sourceId": 470091,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31091,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
