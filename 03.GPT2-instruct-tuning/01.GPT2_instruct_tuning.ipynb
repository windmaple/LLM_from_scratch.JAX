{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "# GPT2 instruction tuning\n",
        "\n",
        "This notebook demonstrates how to finetune a pretrained GPT2(124M) model to follow user instructions. We are going to re-use a lot of code from the previous GPT2 pretraining notebook; as a matter of fact, the only major change needed to data preparation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD3bo9FxhrTE"
      },
      "source": [
        "## Determine platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.403273Z",
          "iopub.status.busy": "2025-02-21T04:30:18.403068Z",
          "iopub.status.idle": "2025-02-21T04:30:18.413321Z",
          "shell.execute_reply": "2025-02-21T04:30:18.412689Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.403251Z"
        },
        "id": "7XcEXnSbhhKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "cf965d74-e371-4dc1-9b46-a1da2bc2d1b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m450.6/456.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.0/456.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.3/473.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.2/135.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2025.4.9 requires jax==0.5.3, but you have jax 0.6.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.1/356.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jax-ai-stack[grain]\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cWxBvz6bZDd"
      },
      "source": [
        "Confirm we have TPUs set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z"
        },
        "id": "uZUaKdi5bSEN",
        "outputId": "c5c6b89b-31b2-4157-a53b-a6fcfce42f14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z"
        },
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z"
        },
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z"
        },
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z"
        },
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 16  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "sampling_temp = 2\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "Now define the model architecture, which is the same as in our previous pretraining notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z"
        },
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          dropout_rate=dropout_rate,\n",
        "                                          deterministic=False,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits/sampling_temp)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBfT1dp5hMUm"
      },
      "source": [
        "Use Weights and Biases to track training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z"
        },
        "id": "IbhEtsganEWg",
        "outputId": "473d41ea-ff17-4f50-8ebf-c3184c013b62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250713_101248-sojae18f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-it/runs/sojae18f' target=\"_blank\">elated-fire-59</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-it' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/windmaple/GPT2-it' target=\"_blank\">https://wandb.ai/windmaple/GPT2-it</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/windmaple/GPT2-it/runs/sojae18f' target=\"_blank\">https://wandb.ai/windmaple/GPT2-it/runs/sojae18f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-it/runs/sojae18f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x798f403aad90>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-it',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Instruct tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5GU7yPKSdtj"
      },
      "source": [
        "We are going to use the [Alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca) from Stanford."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "032fd848bceb4a51acc562e10be1f72c",
            "6b1bd572947346a2a754626ea26a5128",
            "dbdd6729098f439180735c6ed5cb6ebe",
            "bdc1aa1ae2984a8f912597d09a877716",
            "93ecef5610e54665b8238d7f8f10185d",
            "d797882f766a4e73b79c11fbde000041",
            "9d707c2839f34818a5a44704a4df1cc5",
            "5465b044f9bc49c2a51f0f022956d210",
            "4b38c521ee034f94b23ca0203b6c8f07",
            "b15821b3803344be8c3d01860b888282",
            "80e60ea264914dad8c4f648a600e0484",
            "03486fd7b5084687bdcae0832ed9fe8b",
            "48bdbebd897040679d2907e29758bc07",
            "59fc8e96e5c849b290d38ca0842082b0",
            "2cae46a9f65b405d85cf51b6f2bb0fd8",
            "948f9f3e9e484bffb9d6d5dbbf3dc951",
            "57429077ad3c4ae4b4127ffd3639b0d6",
            "2a2cb6dda88c44fcb77f894c49cdd601",
            "41d16f3bd3424013a8fb40530707d4f8",
            "cdf94d6ff6484c7ba33a627c49a90d12",
            "2daef0eb064b4623b5b2f8784b13c916",
            "33f7fdaa3c474825b5a459e522dba22e",
            "46fdff999d0a4bd9bfbb17cc0a25affe",
            "bdc2d511613e4c6fac54d5aa4329b90d",
            "a65b9a8311614159831421c9f621ee28",
            "2e501534210c4b91af028ff2351db078",
            "da9131d9bc9f4434bf7864ddc673ea10",
            "8f0da75d431d4403bf2242f1884759d1",
            "9b391a6b31fe43d3bf12deb483d032f3",
            "a3c4dccba56d45408efb8678a7df534d",
            "a16659663d684d8ca9fe2fba5f966655",
            "90653e03f504430ab4d9d5b63412622b",
            "c3daf453930246cb871d62a99ad7e451"
          ]
        },
        "id": "0VcKINdFKmzQ",
        "outputId": "4e81d81a-5b85-45e5-8e2a-cdffb789d780"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "032fd848bceb4a51acc562e10be1f72c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03486fd7b5084687bdcae0832ed9fe8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46fdff999d0a4bd9bfbb17cc0a25affe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "@dataclass\n",
        "class TextDataset:\n",
        "    data_df: list\n",
        "    seqlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(\n",
        "            self.data_df.iloc[idx], allowed_special={\"<|endoftext|>\"}\n",
        "        )[:self.seqlen        ]\n",
        "        return encoding + [50256] * (self.seqlen - len(encoding))\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(alpaca_data, batch_size, seqlen):\n",
        "    alpaca_data_df = pd.DataFrame(alpaca_data)\n",
        "    dataset = TextDataset(alpaca_data_df[\"text\"], seqlen)\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=1,\n",
        "    )\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "    return dl\n",
        "\n",
        "\n",
        "alpaca_data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "text_dl = load_and_preprocess_data(alpaca_data, batch_size, seqlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VarkoU2TIn2"
      },
      "source": [
        "Define the loss and training step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfHW-ESTTOyf"
      },
      "outputs": [],
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=batch[1]\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(\n",
        "    model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch\n",
        "):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqR0j1IFTWbv"
      },
      "source": [
        "Load the checkpoint from Kaggle. This checkpoint was saved from our previous pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "127dbdf3208d478299ba76193e1b74ac",
            "dd05ead8ecb9448185ea26fbcd53bd1f",
            "926822267aa942fbac672960c64443c7",
            "582a2fb4ba6d4746923311e98af9512e",
            "3f397b76398c40759480351c131584ff",
            "6f15b879fc804ee797b361e98104b91f",
            "473d2b6786f24930881551061f42cae9",
            "e149c85863d940149aaabd974f0df7bb",
            "b68d8b29e5e34885977d64de83fc0b60",
            "bb2bf78ac1b644558f10c014df2edaab",
            "18103075e01949c9a997ddfa49fa97af"
          ]
        },
        "id": "vKyoh9DFTb2n",
        "outputId": "7d2e31c4-e2de-4127-f961-c8991cf34f13"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "127dbdf3208d478299ba76193e1b74ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/04db136358b12417ad1c55ab5f496c76...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 618/618 [00:00<00:00, 1.11MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/18cbc9a84c0bb9c72de5f1e760428afe...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 206/206 [00:00<00:00, 293kB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/d/44befee83135b1c7e5bdc443829202ff...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0.00/322k [00:00<?, ?B/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/05182b6618397da6915e6b8c4438a3fc...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 609/609 [00:00<00:00, 627kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/0e90dd4c14b1961d0e0208ec8dffed9a...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0.00/64.9M [00:00<?, ?B/s]\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/1abfc1b551e0937d0d3eb9d8355905b6...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 649/649 [00:00<00:00, 1.51MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/0265150dfb34bd179945520d55afd528...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 602/602 [00:00<00:00, 574kB/s]\n",
            "\n",
            "100%|██████████| 322k/322k [00:00<00:00, 2.51MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/array_metadatas/process_0...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 29.4k/29.4k [00:00<00:00, 36.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/2b0c054b76f2dd858506bb355a97a077...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0.00/347M [00:00<?, ?B/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/32ebfb821980740ad0fe79c7626b16ca...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 616/616 [00:00<00:00, 625kB/s]\n",
            "\n",
            "\n",
            "  2%|▏         | 1.00M/64.9M [00:00<00:12, 5.47MB/s]\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/75efef239d934a989da6e5b60bb4b230...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 594/594 [00:00<00:00, 1.22MB/s]\n",
            "\n",
            "\n",
            " 12%|█▏        | 8.00M/64.9M [00:00<00:01, 34.3MB/s]\u001b[A\u001b[A\n",
            "  0%|          | 1.00M/347M [00:00<01:06, 5.41MB/s]\u001b[A\n",
            "\n",
            " 29%|██▉       | 19.0M/64.9M [00:00<00:00, 64.6MB/s]\u001b[A\u001b[A\n",
            "  3%|▎         | 9.00M/347M [00:00<00:09, 37.5MB/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/manifest.ocdbt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "100%|██████████| 627/627 [00:00<00:00, 1.20MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/a5a6375fa533d7a3ca9fd496147bee2c...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0.00/26.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/7998766a9ea74d52bb5aac7bd8c22c47...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 596/596 [00:00<00:00, 802kB/s]\n",
            "\n",
            "  5%|▌         | 19.0M/347M [00:00<00:05, 60.3MB/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/c0952a83d7f1bdfcfbfacf36a5d87479...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 42%|████▏     | 27.0M/64.9M [00:00<00:00, 62.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 661/661 [00:00<00:00, 1.00MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/aa4f9d9e8cecda4405392ced7a5131d2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 604/604 [00:00<00:00, 829kB/s]\n",
            "\n",
            "  9%|▉         | 31.0M/347M [00:00<00:04, 81.6MB/s]\u001b[A\n",
            "\n",
            " 59%|█████▊    | 38.0M/64.9M [00:00<00:00, 78.7MB/s]\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/ocdbt.process_0/d/d7b42575bde7fe85fab9db9ef482ecc5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0.00/2.09M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|▍         | 1.00M/26.3M [00:00<00:04, 5.37MB/s]\u001b[A\u001b[A\u001b[A\n",
            " 12%|█▏        | 41.0M/347M [00:00<00:03, 86.6MB/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 48.0M/64.9M [00:00<00:00, 85.1MB/s]\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/_sharding...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0.00/64.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|███▍      | 9.00M/26.3M [00:00<00:00, 36.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            " 15%|█▍        | 51.0M/347M [00:00<00:03, 91.5MB/s]\u001b[A\n",
            "\n",
            "100%|██████████| 64.7k/64.7k [00:00<00:00, 1.90MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 2.09M/2.09M [00:00<00:00, 9.59MB/s]\n",
            "100%|██████████| 64.9M/64.9M [00:00<00:00, 76.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            " 80%|███████▉  | 21.0M/26.3M [00:00<00:00, 65.1MB/s]\u001b[A\u001b[A\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/_METADATA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0.00/71.9k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "100%|██████████| 71.9k/71.9k [00:00<00:00, 1.84MB/s]\n",
            "100%|██████████| 26.3M/26.3M [00:00<00:00, 54.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/_CHECKPOINT_METADATA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 258/258 [00:00<00:00, 510kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/windmaple/gpt2/jax/124m/3/download/manifest.ocdbt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 120/120 [00:00<00:00, 325kB/s]\n",
            "\n",
            " 21%|██        | 72.0M/347M [00:00<00:03, 92.2MB/s]\u001b[A\n",
            " 24%|██▍       | 84.0M/347M [00:01<00:02, 99.9MB/s]\u001b[A\n",
            " 28%|██▊       | 96.0M/347M [00:01<00:02, 102MB/s] \u001b[A\n",
            " 31%|███       | 106M/347M [00:01<00:02, 100MB/s] \u001b[A\n",
            " 34%|███▍      | 119M/347M [00:01<00:02, 105MB/s]\u001b[A\n",
            " 38%|███▊      | 132M/347M [00:01<00:02, 109MB/s]\u001b[A\n",
            " 41%|████      | 143M/347M [00:01<00:01, 109MB/s]\u001b[A\n",
            " 45%|████▍     | 155M/347M [00:01<00:01, 109MB/s]\u001b[A\n",
            " 48%|████▊     | 168M/347M [00:01<00:01, 115MB/s]\u001b[A\n",
            " 52%|█████▏    | 180M/347M [00:01<00:01, 114MB/s]\u001b[A\n",
            " 55%|█████▌    | 192M/347M [00:02<00:01, 116MB/s]\u001b[A\n",
            " 59%|█████▉    | 204M/347M [00:02<00:01, 115MB/s]\u001b[A\n",
            " 62%|██████▏   | 216M/347M [00:02<00:01, 118MB/s]\u001b[A\n",
            " 66%|██████▌   | 228M/347M [00:02<00:01, 109MB/s]\u001b[A\n",
            " 69%|██████▉   | 240M/347M [00:02<00:00, 113MB/s]\u001b[A\n",
            " 73%|███████▎  | 252M/347M [00:02<00:00, 113MB/s]\u001b[A\n",
            " 76%|███████▌  | 264M/347M [00:02<00:00, 113MB/s]\u001b[A\n",
            " 80%|███████▉  | 277M/347M [00:02<00:00, 117MB/s]\u001b[A\n",
            " 84%|████████▎ | 290M/347M [00:02<00:00, 118MB/s]\u001b[A\n",
            " 87%|████████▋ | 303M/347M [00:03<00:00, 121MB/s]\u001b[A\n",
            " 91%|█████████ | 315M/347M [00:03<00:00, 118MB/s]\u001b[A\n",
            " 94%|█████████▍| 327M/347M [00:03<00:00, 118MB/s]\u001b[A\n",
            "100%|██████████| 347M/347M [00:03<00:00, 104MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1251: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import orbax.checkpoint as orbax\n",
        "\n",
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.state(model)\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpoint_path = kagglehub.model_download(\"windmaple/gpt2/jax/124m\")\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc9ySGQgTwaJ"
      },
      "source": [
        "Define optimizer and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om0mZQkqTzP6"
      },
      "outputs": [],
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "    init_value=init_learning_rate, decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "    optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYCeS5BOT65i"
      },
      "source": [
        "Do a test run on our pretrained model to see how it reponds to instruction. Note how we use a template to format the prompt, which needs to be consistent with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8_OTAAZUYhF",
        "outputId": "412a99d9-0f9c-4f1e-cb64-0cefdf3bf39f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***Initial generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Input:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:\n",
            "\n",
            "### Output:"
          ]
        }
      ],
      "source": [
        "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\"\n",
        "\n",
        "start_prompt = template.format(\n",
        "    instruction=\"What is the future for human?\",\n",
        "    input=\"\",\n",
        "    output=\"\",\n",
        ")\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"***Initial generated text:\")\n",
        "generated_text = model.generate_text(seqlen//5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUZamJKUpyg"
      },
      "source": [
        "As you can see, the pretrained model generates a bunch of garbage; clearly it does not know how to follow the instruction to generate an appropriate answer, which is not surprising given that we have not trained it to do so.\n",
        "\n",
        "Now let's do the instruction tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWWc2Eb_n52d",
        "outputId": "fe28c7b9-da5f-48f0-d3e2-3204ec1f30b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 20, Loss: 3.263867139816284, Elapsed Time: 48.28 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            " is task instruction instruction response is is is task instruction response instruction instruction response instruction response instruction response instruction response instruction instruction instruction instruction response instruction: task instruction response instruction response instruction response instruction response instruction instruction response instruction response instruction: task instruction response instruction: is instruction response instruction: task instruction response instruction response is task instruction: task instruction response instruction: task instruction response is task instruction response instruction: task instruction response instruction response instruction response instruction response instruction response instruction response instruction: task instruction: task instruction response instruction instruction response instruction: task instruction response instruction: task instruction response instruction response instruction response instruction response instruction response instruction instruction: task instruction: task instruction response instruction response instruction response instruction: task instruction response instruction: task instruction response instruction: task instruction response instruction response instruction response instruction: task instruction response instruction instruction response instruction response instruction: task instruction response instruction instruction: task instruction: is task instruction response instruction response instruction response instruction response instruction response instruction instruction response instruction response instruction response instruction response instruction response instruction response instruction instruction instruction response instruction: task instruction\n",
            "Step 40, Loss: 0.76171875, Elapsed Time: 36.48 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "Step 60, Loss: 0.638378918170929, Elapsed Time: 4.08 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "Step 80, Loss: 0.54150390625, Elapsed Time: 4.09 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "### Instruction:### Instruction:### Instruction:### Instruction:\n",
            "### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:### Instruction:\n",
            "### Instruction:\n",
            "\n",
            "\n",
            "### Instruction:### Instruction:### Instruction:\n",
            "### Instruction:\n",
            "### Instruction:\n",
            "\n",
            "### Instruction\n",
            "Step 100, Loss: 0.46757814288139343, Elapsed Time: 15.58 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "\n",
            "Step 120, Loss: 0.42724609375, Elapsed Time: 6.01 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "### Response:\n",
            "The most-1, the human species of humans have a robot is an alien robots, the robot has a robot robot, robots, and robot is an robot robot robot robot robot robot robot robot robot robot robot robots!\n",
            "Step 140, Loss: 0.36455079913139343, Elapsed Time: 7.15 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "What is a human brain, the human cell is the cell is the human brain, and cell is the brain, and a computer. It is the brain, it is the brain, the cell, it is the brain. Its cell, the cell, the cell, and it is the brain, it is the brain.\n",
            "\n",
            "It is the brain, and it is the cell is a cell, the brain. Its neurons. Its brain is a brain, it is the cell, it is the cell. Its neurons. It is the brain, it is the brain, the cell is the cell.\n",
            "Step 160, Loss: 0.3360351622104645, Elapsed Time: 11.00 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "What is the world of the world is a planet?\n",
            "\n",
            "\n",
            "What is the world of the world.\n",
            "\n",
            "What is the planet of the planet.\n",
            "What is the world?\n",
            "\n",
            "What is the world is the planet. The world is the planet, which is the world. The world of the world. The world is a planet, which is a planet, which are a planet, and its world is the planet, and its world is the world of the planet, which are a planet, the planet, which are a planet, the planet, and its world is a planet, and its world, the world is the planet, and its own.\n",
            "Step 180, Loss: 0.29658204317092896, Elapsed Time: 11.86 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "What is the future for humans and is a major milestone for the future of humans and human. It is a milestone for humanity and the development of the development of the human and the development of the human and other aspects of the human. Human is the greatest milestone of human development and human life. Human has the potential to revolutionize the way humans and other to make the way of life possible. Human has been a major milestone in human life. Human has the potential to make a meaningful and powerful way for humanity. Human is a much greater milestone in human life.\n",
            "Step 200, Loss: 0.2596679627895355, Elapsed Time: 10.43 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "1. The Future for Human is a wave of events in human history, which have caused the development of a number of people over millions of years.\n",
            "2. The future for humanity is the emergence of a human species. The rise of the development of the world is a profound and unprecedented human experience that will have implications on humanity.\n",
            "Step 220, Loss: 0.24199219048023224, Elapsed Time: 7.69 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human has been a lot of progress and exploration, but the advent of artificial intelligence is becoming more advanced and the emergence of AI have been made. The emergence of AI has revolutionized our understanding of human life, allowing humans to learn and understand more efficiently and the complex nature of the universe. The advent of AI has also opened up new opportunities to explore the possibilities and use it. The emergence of artificial intelligence is the foundation of human intelligence and its potential.\n",
            "Step 240, Loss: 0.24721680581569672, Elapsed Time: 9.13 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Coding a sentence in the past is an important factor in human society. The current population of countries in Europe and Africa are high, and it can lead to the success of human-powered machines in society. This is important to human intelligence, and the potential for human intelligence and human intelligence.\n",
            "Step 260, Loss: 0.233154296875, Elapsed Time: 7.21 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A new type of AI technology has come from a company in a company called Google, founded by a group of individuals that have become involved in various projects, have a company that can use a wide variety of AI systems to develop their own products and processes. They have the capacity to automate the process of making decisions, such as buying the new car or the car, but their capabilities have become increasingly limited to the type of AI systems available today. This has allowed us to automate tasks such as buying the new car, finding the best car to make a new car or the car, and finding the best to buy the new car.\n",
            "\n",
            "\n",
            "### Response:\n",
            "The new car and the car have the capacity to handle any problems, and have the capacity to handle any other tasks. This is the capacity to automate tasks, as it can provide the ability to make a car and drive, but the ability to make the car faster, is also available in today, as there is the ability to handle the various tasks,\n",
            "Step 280, Loss: 0.22275391221046448, Elapsed Time: 15.20 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A future for human is the future for humanity. It's the future for us.\n",
            "Step 300, Loss: 0.21821288764476776, Elapsed Time: 4.74 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human intelligence is a gradual decrease in complexity.\n",
            "\n",
            "### Response:\n",
            "The future for human intelligence is a gradual decrease in complexity.\n",
            "Step 320, Loss: 0.21293945610523224, Elapsed Time: 5.88 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human has been an immense event, and human is a species that is becoming extinct due to extinction events, such as the extinction of the dinosaurs, extinction of the species. This is an incredibly significant event that is being recognized in the scientific world. It has caused the creation of the world's first intelligent species. \n",
            " \n",
            "The species are now being extinct and are currently in extinction due to extinction events, which can have an immense impact on our lives. \n",
            " \n",
            " \n",
            "The species are currently in a rapid and intense extinction cycle. \n",
            " \n",
            " \n",
            "The extinction of the species has been a devastating and intense event, with human extinction being a devastating event, which has forced us to take on ourselves to the ground and the Earth's future. \n",
            "\n",
            " \n",
            " \n",
            "The species are now being extinct due to extinction events, and human extinction is being an intense and intense event. \n",
            " \n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Step 340, Loss: 0.22036133706569672, Elapsed Time: 14.83 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A future for humans.\n",
            "Step 360, Loss: 0.22539062798023224, Elapsed Time: 4.57 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The human has a potential for life, and is ready to take over all of our tasks, including the internet, education, and leisure activities.\n",
            "\n",
            "### Response:\n",
            "The future for humans has a potential for life, and is a great source of technology, which can help to improve the quality of life we have in our society. \n",
            "\n",
            "                                                                                                                                     \n",
            "Step 380, Loss: 0.20595704019069672, Elapsed Time: 14.58 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is the future for humans. It will continue to be the future for human populations, and humans have the opportunity to make an impact on their lives and their communities. This will continue to be a significant challenge for humans.\n",
            "Step 400, Loss: 0.22133789956569672, Elapsed Time: 6.60 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans will depend on how many people have to live.\n",
            "Step 420, Loss: 0.19521485269069672, Elapsed Time: 4.64 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is the present, and it will depend on factors such as human development, human development, and economic status. \n",
            "\n",
            "### Response:\n",
            "The future for human development is the present, and human development is depend on factors such as economic status, socio-economic status, and political status.\n",
            "Step 440, Loss: 0.20957031846046448, Elapsed Time: 7.80 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humanity is becoming increasingly dependent on technology.\n",
            "Step 460, Loss: 0.20073242485523224, Elapsed Time: 4.65 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life is in a range of scenarios, from the possibilities of using natural language processing to developing artificial intelligence applications such as deep learning algorithms to understanding and manipulating human language. The future of human life will be more diverse, from a world where computers can be used to develop AI applications. The future for human life will be a world of possibilities that will include intelligent computers, intelligent assistants, autonomous vehicles, autonomous vehicles, autonomous vehicles, and intelligent vehicles. The future for human life will be more diverse, from a world of possibilities that could include autonomous vehicles, autonomous vehicles, autonomous vehicles, and intelligent vehicles. The future for human life will be more diverse, from a world of possibilities that could include intelligent cars, autonomous vehicles, and intelligent vehicles. The future for human life will include more possibilities, such as intelligent cars, autonomous vehicles, autonomous vehicles, and intelligent vehicles.\n",
            "Step 480, Loss: 0.19501952826976776, Elapsed Time: 13.95 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life is a world of uncertainty, uncertainty, and uncertainty. It is a situation in which the world's future will depend on uncertainty and uncertainty.\n",
            "Step 500, Loss: 0.19682617485523224, Elapsed Time: 6.08 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human societies is a global pandemic, with a number of countries affected by a global pandemic, including China, Saudi Arabia, Saudi Arabia, Russia, India, Saudi Arabia, Saudi Arabia, Saudi Arabia, Saudi Arabia, Russia, and Saudi Arabia all struggling to survive in the global marketplace.\n",
            "Step 520, Loss: 0.21708984673023224, Elapsed Time: 7.34 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human in AI is becoming increasingly dependent on the advancement of artificial intelligence, which can help to identify and detect and detect patterns in AI and detect patterns in AI, enabling it to detect patterns in the data, identify patterns in AI and identify patterns in patterns, detect patterns in the AI system, and identify patterns in AI and identify patterns in AI.\n",
            "Step 540, Loss: 0.19174805283546448, Elapsed Time: 8.19 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is the possibility of an even greater future for humans. It could be that humans have the ability to access the information and services they need, and they could also access information from others, as well as access information from others.\n",
            "Step 560, Loss: 0.21875, Elapsed Time: 6.85 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for AI is the possibility of the development of new applications, such as robotics, robotics, and healthcare. AI can enable the use of a wide variety of technologies such as facial recognition, voice recognition, facial recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice, voice recognition, voice recognition, voice recognition, voice recognition, voice recognition, voice, voice recognition, voice, voice recognition, voice recognition, voice recognition, voice,\n",
            "Step 580, Loss: 0.207275390625, Elapsed Time: 14.76 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human development could be 2019.\n",
            "Step 600, Loss: 0.199951171875, Elapsed Time: 4.44 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Humanity is the result of human activities such as human activity and education, such as technology, that are not possible in today's world.\n",
            "Step 620, Loss: 0.19345703721046448, Elapsed Time: 5.86 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is looking brighter.\n",
            "Step 640, Loss: 0.204833984375, Elapsed Time: 4.49 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human?\n",
            "Step 660, Loss: 0.21103516221046448, Elapsed Time: 4.73 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is the development of new technologies, such as virtual reality, virtual reality, and virtual reality. These technologies can revolutionize our daily lives and provide unprecedented access to the world with advanced technology, allowing us to travel from one place to another, and increase access to a range of different platform and applications. Additionally, these technologies can also be used to improve our productivity, allowing us to stay connected and work from anywhere with a greater ease than a smartphone.\n",
            "Step 680, Loss: 0.19338379800319672, Elapsed Time: 9.19 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humanity is an incredibly promising and promising future for humanity. It has been achieved through a variety of technological advancements and breakthroughs, from quantum computing to quantum computing. It is an era of progress, and is being driven by many new ideas, technologies, and opportunities. The future of humanity is a vibrant and promising place to discover new things, explore new ways of living, and create a new environment for the rest of the world.\n",
            "Step 700, Loss: 0.19780273735523224, Elapsed Time: 9.32 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The human body is a rapidly changing world, and the world around us is rapidly changing rapidly.\n",
            "Step 720, Loss: 0.20732422173023224, Elapsed Time: 5.55 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human is the present.\n",
            "Step 740, Loss: 0.19975586235523224, Elapsed Time: 4.59 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human is the most difficult one to understand. It is a period of immense uncertainty, uncertainty, and doubt that will have profound consequences for human beings. It is the only hope for a better world for humans, as the universe has a deep impact on our world.\n",
            "Step 760, Loss: 0.18212890625, Elapsed Time: 7.21 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans in AI technology is being driven by the potential for a future of artificial intelligence (AI).\n",
            "Step 780, Loss: 0.20224609971046448, Elapsed Time: 5.06 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is the next step forward towards a more perfect world for animals. This is due to advancements in technology and technological advancement.\n",
            "Step 800, Loss: 0.19863282144069672, Elapsed Time: 5.78 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is set for the next few years, and it will continue to unfold as we keep up with technology and the world we have made of.\n",
            "Step 820, Loss: 0.19970703125, Elapsed Time: 6.04 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is the present, the future is a set of possibilities and opportunities, and we can all benefit from it. We are able to gain the skills, resources, and knowledge that are necessary to create and sustain our future. We are also able to make the most of our time and energy.\n",
            "Step 840, Loss: 0.20327149331569672, Elapsed Time: 7.50 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human life is marked by a range of events. It includes a range of human events, including a species extinction, a pandemic, the adoption of new technology, and a global pandemic. It is a world of possibility, as well as an endless possibility, that the human race will never end.\n",
            "Step 860, Loss: 0.197265625, Elapsed Time: 7.91 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is in a future of limitless energy, limitless resources, and endless opportunities.\n",
            "Step 880, Loss: 0.20175781846046448, Elapsed Time: 5.63 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is the future of humanity. It is a time when humanity has the capacity to take action, make a decision, and take a stand to protect the planet. The future is the future that humans must make in order to live, work, and make progress in the future.\n",
            "Step 900, Loss: 0.20820312201976776, Elapsed Time: 7.38 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life is in a future of limitless future possibilities. We are now in a world where we are living in an abundance of possibilities and possibilities, from a future to the future, with each person's choices to be carefully explored. This future will include opportunities to pursue and develop in a variety of ways, from education to the possibilities of life. This will be an opportunity to pursue the future with courage and bravery. It will be a time for all, as we have been facing a variety of challenges, from rising unemployment rates to global poverty. We will be facing the challenge of life and the potential of a future where humans have to come to an end, as well as a future where the world will become more accessible to everyone.\n",
            "Step 920, Loss: 0.19428710639476776, Elapsed Time: 11.74 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life is the future of humanity. Humanity has grown from the smallest group, making it a part of humanity's world. It has become an important part of humanity, and is a key component of humanity's future. Humanity is the largest and largest of the human race, with a population of around 4 billion people. Humanity has the potential to become the largest and largest of all humanity. It has a responsibility to the future of humanity, as it allows humanity to live and work with its resources, as it provides the foundation to humanity's growth. It also helps to bridge the gap between the world's two main civilizations, allowing us to gain a more sustainable future for ourselves. In this world, humanity has a role to play in helping to ensure that it can continue to exist. It has a role to shape our world and the future of humanity, and is a vital part of the future of humanity.\n",
            "Step 940, Loss: 0.20732422173023224, Elapsed Time: 14.17 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life is in a distant future where there are no humans, and no one is responsible for their own decisions or decisions. In the distant future, human beings have a limited and interconnected perspective and can only learn from the experiences they have made and how they can adapt to the world they have lived. In the distant future, humans are now living in harmony, with their lives and their experiences changing over time. In the distant future, human life is a form of human activity, with no human being able to adapt to its environment and no one being able to adjust their behavior or adapt to the world around them. In the distant future, human beings have a more direct connection to the universe, with their experiences being seen by humans.\n",
            "Step 960, Loss: 0.20166015625, Elapsed Time: 12.41 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human?\n",
            "Step 980, Loss: 0.19833984971046448, Elapsed Time: 4.39 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human is to be a time-consuming and challenging time.\n",
            "Step 1000, Loss: 0.19052734971046448, Elapsed Time: 5.08 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is the emergence of artificial intelligence, artificial intelligence, and automation.\n",
            "Step 1020, Loss: 0.18930664658546448, Elapsed Time: 5.21 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is the possibility of artificial intelligence, as the development of AI can lead to the development of a range of ethical, ethical, and ethical applications. AI could be used to improve customer satisfaction, increase customer satisfaction, or even improve customer service. It could also be used to automate customer service, increase customer satisfaction, and increase customer satisfaction.\n",
            "Step 1040, Loss: 0.19497071206569672, Elapsed Time: 8.13 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is that we are living in a future that has the ability to live in a world where our brains have become obsolete.\n",
            "Step 1060, Loss: 0.18574218451976776, Elapsed Time: 5.86 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is a question of human progress. It is a difficult and complex issue for human beings to comprehend. It's a complex problem for human beings, as humans are responsible for the future of the universe and for our planet's future. Human progress has become an important factor for humans, and its future has become an essential part of the future of human societies. It has been an important part of our future, and its potential for humans to continue.\n",
            "Step 1080, Loss: 0.19365234673023224, Elapsed Time: 9.68 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human life depends on human nature, as it exists for a variety of reasons. It can be a time when we feel connected to a community, or a planet. Human nature is also responsible for human life, and it is responsible for human activities. It can be difficult to come up with solutions for human life because of its difficulties. Human nature is responsible for the creation of food, the environment, and other aspects of human life. Human nature can also be a source of human emotion and energy. Human nature is also responsible for human activities, such as food and entertainment. It can also be responsible for human nature's impact on the world, such as its impact on human culture and the development of human technology. Human nature can also be an essential factor in human life. It is also important to consider human nature as an important tool in order to ensure the success of human civilization.\n",
            "### Response:\n",
            "Human nature can be an essential tool in human life. Human nature is essential for human life and\n",
            "Step 1100, Loss: 0.20253907144069672, Elapsed Time: 15.45 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Humanity is the future for humans and its future in many ways. Human activity has become a part of our everyday lives and it has been an important part of the modern world, from its origins in ancient times. Human life has been impacted by human activities, from agriculture to transportation and leisure. It is a place of beauty and is an important part of our world today.\n",
            "Step 1120, Loss: 0.19736328721046448, Elapsed Time: 8.52 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Future for humans to continue living.\n",
            "Step 1140, Loss: 0.19047851860523224, Elapsed Time: 4.37 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human technology is expected to continue in the coming years, with the rapid spread of AI and natural intelligence. AI-assisted AI-assisted AI is used to provide automated decisions that are more efficient than humans.\n",
            "Step 1160, Loss: 0.18305663764476776, Elapsed Time: 6.80 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Future for Human: \n",
            "[1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 13, 16, 19, 23, 23, 24, 25, 27, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36\n",
            "Step 1180, Loss: 0.189453125, Elapsed Time: 15.46 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Human life has a number of possible applications. One potential future for human life is the development of renewable energy sources such as solar, wind, and hydropower. This can help reduce our dependence on fossil fuels and the amount of waste produced. Another potential application is to create a more efficient transportation system that uses renewable energy and is able to generate more energy from renewable sources.\n",
            "Step 1200, Loss: 0.19184570014476776, Elapsed Time: 8.15 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human? \n",
            "                                                                                                                                                                                                     \n",
            "Step 1220, Loss: 0.19130860269069672, Elapsed Time: 15.55 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future is in a brightening world, where humans are taking risks in order to save our world and create new ones. We are in an era of immense danger and hope, where humans have the potential to create a brighter future for generations to follow. We are also taking risks to save the world and make a difference in our world, as well as the potential of a future of greater freedom and freedom. We can make sure that our future is filled with opportunities, possibilities, and a sense of hope, as well as a future that we can all share in the world.\n",
            "Step 1240, Loss: 0.20371094346046448, Elapsed Time: 10.46 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is a simulation of a future where human is created, created, and maintained by human beings.\n",
            "Step 1260, Loss: 0.19731445610523224, Elapsed Time: 5.57 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is set in the past.\n",
            "Step 1280, Loss: 0.18046875298023224, Elapsed Time: 4.55 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humans is expected to grow significantly. Human-based AI is already capable of making large strides in the field, and the development of artificial intelligence can lead to the development of AI systems. It can also lead to advances in the technology industry, with advancements in the fields of robotics, machine learning, and natural language processing. In addition, the potential for AI-based applications and services are expected to be immense. In conclusion, the future for humans is likely to bring greater advancements and advancements in the fields of artificial intelligence.\n",
            "Step 1300, Loss: 0.18974609673023224, Elapsed Time: 10.06 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human is unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable, unpredictable\n",
            "Step 1320, Loss: 0.177490234375, Elapsed Time: 15.09 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is uncertain, as human activity and the emergence of human-like machines are still a challenge, with the development of new forms of communication, machines, and humans. The human population has grown to an average, with a number of human species, such as the chimpanzee, chimpanzee, and gorilla. The emergence of humans has led to the development of more advanced forms of communication, such as language and communication, which have become more effective and more effective than ever before. The human-human revolution has been driven by human creativity and the development of human emotions.\n",
            "Step 1340, Loss: 0.18325196206569672, Elapsed Time: 10.18 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for human is unpredictable.\n",
            "Step 1360, Loss: 0.17998047173023224, Elapsed Time: 4.30 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Humanity is a future of technology, and it is rapidly becoming the cornerstone of the universe. It is a time-traveling, innovative and rapidly changing field, with many of the world's most populous places now in orbit around the Sun. It is the world of living and of living.\n",
            "Step 1380, Loss: 0.20307616889476776, Elapsed Time: 7.10 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humanity is a simulation of the universe and the universe that has never been seen before. This simulation of the universe will involve a series of events and experiments, such as experiments, experiments, and experiments, to test our understanding of the universe. This simulation will include the introduction of new species"
          ]
        }
      ],
      "source": [
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "for batch in text_dl:\n",
        "    if len(batch) % len(jax.devices()) != 0:\n",
        "        continue  # skip the remaining elements\n",
        "    input_batch = jnp.array(batch).T\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    train_step(\n",
        "        model,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        jax.device_put(\n",
        "            (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if (step + 1) % 20 == 0:\n",
        "        for metric, value in metrics.compute().items():\n",
        "            metrics_history[f\"train_{metric}\"].append(value)\n",
        "        metrics.reset()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            f\"\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "        )\n",
        "        wandb.log(data={'train_loss': metrics_history['train_loss'][-1]}, step=step)\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n***Intermediate generated text:\")\n",
        "        generated_text = model.generate_text(seqlen // 5, start_tokens)\n",
        "    step += 1\n",
        "\n",
        "# Final text generation\n",
        "print(f\"\\n***Final generated text:\")\n",
        "generated_text = model.generate_text(seqlen // 5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beji0Q63afpe"
      },
      "source": [
        "As you can see, at the end of the finetuning, the model is able to generate a somewhat sensible answer, based on the simple user question. This means our instruction tuning actually works.\n",
        "\n",
        "However, GPT2 is fundamentally a very small (and weak) model, especially the 124M variant we are using here. If you ask the model a more challenging question, it will easily break down.\n",
        "\n",
        "But hopefully you have learned the key concept behind instruction tuning through this simple notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uHcKfOvbcsx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "datasetId": 5848741,
          "sourceId": 10577797,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30920,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "state": {},
        "032fd848bceb4a51acc562e10be1f72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b1bd572947346a2a754626ea26a5128",
              "IPY_MODEL_dbdd6729098f439180735c6ed5cb6ebe",
              "IPY_MODEL_bdc1aa1ae2984a8f912597d09a877716"
            ],
            "layout": "IPY_MODEL_93ecef5610e54665b8238d7f8f10185d"
          }
        },
        "03486fd7b5084687bdcae0832ed9fe8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48bdbebd897040679d2907e29758bc07",
              "IPY_MODEL_59fc8e96e5c849b290d38ca0842082b0",
              "IPY_MODEL_2cae46a9f65b405d85cf51b6f2bb0fd8"
            ],
            "layout": "IPY_MODEL_948f9f3e9e484bffb9d6d5dbbf3dc951"
          }
        },
        "127dbdf3208d478299ba76193e1b74ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd05ead8ecb9448185ea26fbcd53bd1f",
              "IPY_MODEL_926822267aa942fbac672960c64443c7",
              "IPY_MODEL_582a2fb4ba6d4746923311e98af9512e"
            ],
            "layout": "IPY_MODEL_3f397b76398c40759480351c131584ff"
          }
        },
        "18103075e01949c9a997ddfa49fa97af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a2cb6dda88c44fcb77f894c49cdd601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cae46a9f65b405d85cf51b6f2bb0fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2daef0eb064b4623b5b2f8784b13c916",
            "placeholder": "​",
            "style": "IPY_MODEL_33f7fdaa3c474825b5a459e522dba22e",
            "value": " 24.2M/24.2M [00:00&lt;00:00, 35.0MB/s]"
          }
        },
        "2daef0eb064b4623b5b2f8784b13c916": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e501534210c4b91af028ff2351db078": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90653e03f504430ab4d9d5b63412622b",
            "placeholder": "​",
            "style": "IPY_MODEL_c3daf453930246cb871d62a99ad7e451",
            "value": " 52002/52002 [00:00&lt;00:00, 407585.07 examples/s]"
          }
        },
        "33f7fdaa3c474825b5a459e522dba22e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f397b76398c40759480351c131584ff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d16f3bd3424013a8fb40530707d4f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46fdff999d0a4bd9bfbb17cc0a25affe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdc2d511613e4c6fac54d5aa4329b90d",
              "IPY_MODEL_a65b9a8311614159831421c9f621ee28",
              "IPY_MODEL_2e501534210c4b91af028ff2351db078"
            ],
            "layout": "IPY_MODEL_da9131d9bc9f4434bf7864ddc673ea10"
          }
        },
        "473d2b6786f24930881551061f42cae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48bdbebd897040679d2907e29758bc07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57429077ad3c4ae4b4127ffd3639b0d6",
            "placeholder": "​",
            "style": "IPY_MODEL_2a2cb6dda88c44fcb77f894c49cdd601",
            "value": "data/train-00000-of-00001-a09b74b3ef9c3b(…): 100%"
          }
        },
        "4b38c521ee034f94b23ca0203b6c8f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5465b044f9bc49c2a51f0f022956d210": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "57429077ad3c4ae4b4127ffd3639b0d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "582a2fb4ba6d4746923311e98af9512e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2bf78ac1b644558f10c014df2edaab",
            "placeholder": "​",
            "style": "IPY_MODEL_18103075e01949c9a997ddfa49fa97af",
            "value": " 21/21 [00:04&lt;00:00,  1.91it/s]"
          }
        },
        "59fc8e96e5c849b290d38ca0842082b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d16f3bd3424013a8fb40530707d4f8",
            "max": 24246638,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdf94d6ff6484c7ba33a627c49a90d12",
            "value": 24246638
          }
        },
        "6b1bd572947346a2a754626ea26a5128": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d797882f766a4e73b79c11fbde000041",
            "placeholder": "​",
            "style": "IPY_MODEL_9d707c2839f34818a5a44704a4df1cc5",
            "value": "README.md: "
          }
        },
        "6f15b879fc804ee797b361e98104b91f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e60ea264914dad8c4f648a600e0484": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f0da75d431d4403bf2242f1884759d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90653e03f504430ab4d9d5b63412622b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "926822267aa942fbac672960c64443c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e149c85863d940149aaabd974f0df7bb",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b68d8b29e5e34885977d64de83fc0b60",
            "value": 21
          }
        },
        "93ecef5610e54665b8238d7f8f10185d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948f9f3e9e484bffb9d6d5dbbf3dc951": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b391a6b31fe43d3bf12deb483d032f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d707c2839f34818a5a44704a4df1cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a16659663d684d8ca9fe2fba5f966655": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3c4dccba56d45408efb8678a7df534d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65b9a8311614159831421c9f621ee28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3c4dccba56d45408efb8678a7df534d",
            "max": 52002,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a16659663d684d8ca9fe2fba5f966655",
            "value": 52002
          }
        },
        "b15821b3803344be8c3d01860b888282": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68d8b29e5e34885977d64de83fc0b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb2bf78ac1b644558f10c014df2edaab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc1aa1ae2984a8f912597d09a877716": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b15821b3803344be8c3d01860b888282",
            "placeholder": "​",
            "style": "IPY_MODEL_80e60ea264914dad8c4f648a600e0484",
            "value": " 7.47k/? [00:00&lt;00:00, 860kB/s]"
          }
        },
        "bdc2d511613e4c6fac54d5aa4329b90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f0da75d431d4403bf2242f1884759d1",
            "placeholder": "​",
            "style": "IPY_MODEL_9b391a6b31fe43d3bf12deb483d032f3",
            "value": "Generating train split: 100%"
          }
        },
        "c3daf453930246cb871d62a99ad7e451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cdf94d6ff6484c7ba33a627c49a90d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d797882f766a4e73b79c11fbde000041": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9131d9bc9f4434bf7864ddc673ea10": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbdd6729098f439180735c6ed5cb6ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5465b044f9bc49c2a51f0f022956d210",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b38c521ee034f94b23ca0203b6c8f07",
            "value": 1
          }
        },
        "dd05ead8ecb9448185ea26fbcd53bd1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f15b879fc804ee797b361e98104b91f",
            "placeholder": "​",
            "style": "IPY_MODEL_473d2b6786f24930881551061f42cae9",
            "value": "Downloading 21 files: 100%"
          }
        },
        "e149c85863d940149aaabd974f0df7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
