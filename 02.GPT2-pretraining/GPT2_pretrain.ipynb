{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10577797,
          "sourceType": "datasetVersion",
          "datasetId": 5848741
        }
      ],
      "dockerImageVersionId": 31042,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine platform"
      ],
      "metadata": {
        "id": "LD3bo9FxhrTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ],
      "metadata": {
        "id": "7XcEXnSbhhKV",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:24:34.862815Z",
          "iopub.execute_input": "2025-07-02T09:24:34.863031Z",
          "iopub.status.idle": "2025-07-02T09:24:34.882343Z",
          "shell.execute_reply.started": "2025-07-02T09:24:34.863006Z",
          "shell.execute_reply": "2025-07-02T09:24:34.877892Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jax-ai-stack\n",
        "!pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info orbax-checkpoint"
      ],
      "metadata": {
        "id": "6zMsOIc7ouCO",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:24:34.883256Z",
          "iopub.execute_input": "2025-07-02T09:24:34.883464Z",
          "iopub.status.idle": "2025-07-02T09:25:12.051089Z",
          "shell.execute_reply.started": "2025-07-02T09:24:34.883444Z",
          "shell.execute_reply": "2025-07-02T09:25:12.046685Z"
        },
        "outputId": "7e156a05-62fd-42af-a0b6-bc0e9eeaaeb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-tpu 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:12.053230Z",
          "iopub.execute_input": "2025-07-02T09:25:12.053494Z",
          "iopub.status.idle": "2025-07-02T09:25:20.689838Z",
          "shell.execute_reply.started": "2025-07-02T09:25:12.053466Z",
          "shell.execute_reply": "2025-07-02T09:25:20.684935Z"
        },
        "outputId": "eaf82b36-fb36-4ae1-b34f-756846792414",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1751448316.974959      10 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:230\n",
          "output_type": "stream"
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:20.692016Z",
          "iopub.execute_input": "2025-07-02T09:25:20.692352Z",
          "iopub.status.idle": "2025-07-02T09:25:23.109583Z",
          "shell.execute_reply.started": "2025-07-02T09:25:20.692324Z",
          "shell.execute_reply": "2025-07-02T09:25:23.104665Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "#mesh = jax.make_mesh((4, 2), ('batch', 'model'))\n",
        "\n",
        "mesh = jax.make_mesh((8, 1), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:23.111637Z",
          "iopub.execute_input": "2025-07-02T09:25:23.111894Z",
          "iopub.status.idle": "2025-07-02T09:25:23.122602Z",
          "shell.execute_reply.started": "2025-07-02T09:25:23.111870Z",
          "shell.execute_reply": "2025-07-02T09:25:23.116994Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:23.124239Z",
          "iopub.execute_input": "2025-07-02T09:25:23.124478Z",
          "iopub.status.idle": "2025-07-02T09:25:26.023720Z",
          "shell.execute_reply.started": "2025-07-02T09:25:23.124454Z",
          "shell.execute_reply": "2025-07-02T09:25:26.019376Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 1024\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:26.024852Z",
          "iopub.execute_input": "2025-07-02T09:25:26.025078Z",
          "iopub.status.idle": "2025-07-02T09:25:26.036906Z",
          "shell.execute_reply.started": "2025-07-02T09:25:26.025055Z",
          "shell.execute_reply": "2025-07-02T09:25:26.032128Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:29.244010Z",
          "iopub.execute_input": "2025-07-02T09:29:29.244354Z",
          "iopub.status.idle": "2025-07-02T09:29:29.275131Z",
          "shell.execute_reply.started": "2025-07-02T09:29:29.244327Z",
          "shell.execute_reply": "2025-07-02T09:29:29.270045Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:26:05.941480Z",
          "iopub.execute_input": "2025-07-02T09:26:05.941906Z",
          "iopub.status.idle": "2025-07-02T09:26:09.372333Z",
          "shell.execute_reply.started": "2025-07-02T09:26:05.941877Z",
          "shell.execute_reply": "2025-07-02T09:26:09.367153Z"
        },
        "outputId": "18815d61-a811-4175-997f-cc20e5009d84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.21.0"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250702_092607-diz53w7y</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/diz53w7y' target=\"_blank\">cosmic-durian-202</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/windmaple/GPT2-pretraining' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/windmaple/GPT2-pretraining/runs/diz53w7y' target=\"_blank\">https://wandb.ai/windmaple/GPT2-pretraining/runs/diz53w7y</a>"
          },
          "metadata": {}
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-pretraining/runs/diz53w7y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
            "text/plain": "<wandb.sdk.wandb_run.Run at 0x7cf52811df60>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  if not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "  data_dir = \"/content/drive/MyDrive/LLM-pretraining/OpenWebText/\" # Assume dataset is already stored there\n",
        "elif platform == \"GCP\":\n",
        "  if not os.path.exists('OpenWebText-gpt2.zip'):\n",
        "    # Assume kaggle binary is in ~/.local/bin after pip install kaggle\n",
        "    !~/.local/bin/kaggle datasets download -d windmaple/OpenWebText-gpt2 && unzip OpenWebText-gpt2.zip\n",
        "  data_dir = \".\"\n",
        "elif platform == \"Kaggle\":  # On Kaggle one should manually add the datasets before running\n",
        "  data_dir = \"/kaggle/input/\"\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
        "\n",
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "\n",
        "    data = train_data if train_or_eval == \"train\" else val_data\n",
        "\n",
        "    ix = np.random.randint(0, len(data) - seqlen, (batch_size,))\n",
        "    x = np.stack([(data[i:i+seqlen]).astype(np.int64) for i in ix])\n",
        "    y = np.stack([(data[i+1:i+1+seqlen]).astype(np.int64) for i in ix])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:27:16.415753Z",
          "iopub.execute_input": "2025-07-02T09:27:16.416143Z",
          "iopub.status.idle": "2025-07-02T09:27:16.437228Z",
          "shell.execute_reply.started": "2025-07-02T09:27:16.416110Z",
          "shell.execute_reply": "2025-07-02T09:27:16.433942Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:27:16.647464Z",
          "iopub.execute_input": "2025-07-02T09:27:16.647720Z",
          "iopub.status.idle": "2025-07-02T09:27:16.660761Z",
          "shell.execute_reply.started": "2025-07-02T09:27:16.647689Z",
          "shell.execute_reply": "2025-07-02T09:27:16.655426Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:36.993144Z",
          "iopub.execute_input": "2025-07-02T09:29:36.993501Z",
          "iopub.status.idle": "2025-07-02T09:29:37.279549Z",
          "shell.execute_reply.started": "2025-07-02T09:29:36.993472Z",
          "shell.execute_reply": "2025-07-02T09:29:37.273647Z"
        },
        "outputId": "53253e34-432a-490b-805a-f8b3dec74896",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Number of model parameters: 124439810\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# from transformers import GPT2Model\n",
        "\n",
        "# # From https://github.com/huggingface/transformers/issues/27615\n",
        "# model_hf = GPT2Model.from_pretrained('gpt2')\n",
        "# model_parameters = filter(lambda p: p.requires_grad, model_hf.parameters())\n",
        "# param_count = sum([np.prod(p.size()) for p in model_parameters])\n",
        "\n",
        "# print(f\"Number of model parameters from Hugging Face: {param_count}\")"
      ],
      "metadata": {
        "id": "KDWyHtrU6S5f",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:37.281086Z",
          "iopub.execute_input": "2025-07-02T09:29:37.281302Z",
          "iopub.status.idle": "2025-07-02T09:29:37.290634Z",
          "shell.execute_reply.started": "2025-07-02T09:29:37.281279Z",
          "shell.execute_reply": "2025-07-02T09:29:37.286095Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"Initial generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))))\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val')\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P(\"batch\", None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "print(f\"Final generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:37.292431Z",
          "iopub.execute_input": "2025-07-02T09:29:37.292678Z",
          "iopub.status.idle": "2025-07-02T15:54:24.022806Z",
          "shell.execute_reply.started": "2025-07-02T09:29:37.292653Z",
          "shell.execute_reply": "2025-07-02T15:54:24.017654Z"
        },
        "outputId": "cb372237-7ed3-421c-e35f-f48d8d8b58b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Initial generated text:\nOnce upon a time Nathaniel blockbuster foreign consultations Riding securedakiusions flattened flattened GothicusionsARB flattened wear wear Riding bru Riding shook radius organicusionslique stal secured secured secured securedakiusionsusions stal Xander Riding Riding Gothicusionsredits Gothicusions Riding Also Xander Gothic stal Riding principals principals Riding Caucas Gothic Also principals Xander principals Gothic Gothic Riding Caucas Months principals Xander Riding Caucas Gothic Xander Caucas Caucas Months Gothic Gothic Gothic principals Gothic Xander Caucas Riding Also Gothic Gothic Caucas Gothic Months Girl Xander principals principalsredits Xander principals Caucas Gothic Caucas Also Also Caucas Gothic Gothic Xander principals heirsStep 1, Training loss: 11.5, Elapsed Time: 31.81 seconds\nStep 1, Validation loss: 10.0\nStep 201, Training loss: 7.104687213897705, Elapsed Time: 110.95 seconds\nStep 201, Validation loss: 6.40625\nStep 401, Training loss: 6.217343807220459, Elapsed Time: 50.89 seconds\nStep 401, Validation loss: 6.125\nStep 601, Training loss: 5.900000095367432, Elapsed Time: 54.69 seconds\nStep 601, Validation loss: 5.875\nStep 801, Training loss: 5.607187271118164, Elapsed Time: 50.83 seconds\nStep 801, Validation loss: 5.5\nStep 1001, Training loss: 5.32937479019165, Elapsed Time: 51.14 seconds\nStep 1001, Validation loss: 5.25\nStep 1201, Training loss: 5.110468864440918, Elapsed Time: 50.85 seconds\nStep 1201, Validation loss: 5.09375\nStep 1401, Training loss: 4.91796875, Elapsed Time: 50.82 seconds\nStep 1401, Validation loss: 4.8125\nStep 1601, Training loss: 4.733593463897705, Elapsed Time: 50.92 seconds\nStep 1601, Validation loss: 4.625\nStep 1801, Training loss: 4.584062576293945, Elapsed Time: 50.79 seconds\nStep 1801, Validation loss: 4.5625\nStep 2001, Training loss: 4.471562385559082, Elapsed Time: 51.07 seconds\nStep 2001, Validation loss: 4.40625\nStep 2201, Training loss: 4.379843711853027, Elapsed Time: 50.78 seconds\nStep 2201, Validation loss: 4.3125\nStep 2401, Training loss: 4.317500114440918, Elapsed Time: 50.93 seconds\nStep 2401, Validation loss: 4.34375\nStep 2601, Training loss: 4.264374732971191, Elapsed Time: 50.77 seconds\nStep 2601, Validation loss: 4.1875\nStep 2801, Training loss: 4.209218502044678, Elapsed Time: 50.76 seconds\nStep 2801, Validation loss: 4.21875\nStep 3001, Training loss: 4.159218788146973, Elapsed Time: 50.77 seconds\nStep 3001, Validation loss: 4.125\nStep 3201, Training loss: 4.118359088897705, Elapsed Time: 50.76 seconds\nStep 3201, Validation loss: 4.125\nStep 3401, Training loss: 4.07156229019165, Elapsed Time: 50.77 seconds\nStep 3401, Validation loss: 4.0\nStep 3601, Training loss: 4.045702934265137, Elapsed Time: 50.74 seconds\nStep 3601, Validation loss: 4.0\nStep 3801, Training loss: 4.014999866485596, Elapsed Time: 50.77 seconds\nStep 3801, Validation loss: 3.96875\nStep 4001, Training loss: 3.990312337875366, Elapsed Time: 50.91 seconds\nStep 4001, Validation loss: 3.96875\nStep 4201, Training loss: 3.9564061164855957, Elapsed Time: 50.74 seconds\nStep 4201, Validation loss: 3.921875\nStep 4401, Training loss: 3.935859203338623, Elapsed Time: 50.75 seconds\nStep 4401, Validation loss: 3.984375\nStep 4601, Training loss: 3.9159374237060547, Elapsed Time: 50.90 seconds\nStep 4601, Validation loss: 3.921875\nStep 4801, Training loss: 3.892343759536743, Elapsed Time: 50.74 seconds\nStep 4801, Validation loss: 3.890625\nStep 5001, Training loss: 3.876093626022339, Elapsed Time: 51.00 seconds\nStep 5001, Validation loss: 3.890625\nStep 5201, Training loss: 3.8550000190734863, Elapsed Time: 50.75 seconds\nStep 5201, Validation loss: 3.84375\nStep 5401, Training loss: 3.8391406536102295, Elapsed Time: 50.74 seconds\nStep 5401, Validation loss: 3.84375\nStep 5601, Training loss: 3.830312490463257, Elapsed Time: 50.87 seconds\nStep 5601, Validation loss: 3.71875\nStep 5801, Training loss: 3.803046703338623, Elapsed Time: 50.73 seconds\nStep 5801, Validation loss: 3.890625\nStep 6001, Training loss: 3.7978124618530273, Elapsed Time: 51.17 seconds\nStep 6001, Validation loss: 3.734375\nStep 6201, Training loss: 3.7764060497283936, Elapsed Time: 50.73 seconds\nStep 6201, Validation loss: 3.78125\nStep 6401, Training loss: 3.769765615463257, Elapsed Time: 50.73 seconds\nStep 6401, Validation loss: 3.828125\nStep 6601, Training loss: 3.7532811164855957, Elapsed Time: 50.86 seconds\nStep 6601, Validation loss: 3.890625\nStep 6801, Training loss: 3.745781183242798, Elapsed Time: 50.88 seconds\nStep 6801, Validation loss: 3.71875\nStep 7001, Training loss: 3.72710919380188, Elapsed Time: 50.73 seconds\nStep 7001, Validation loss: 3.703125\nStep 7201, Training loss: 3.7206249237060547, Elapsed Time: 50.88 seconds\nStep 7201, Validation loss: 3.6875\nStep 7401, Training loss: 3.706249952316284, Elapsed Time: 50.87 seconds\nStep 7401, Validation loss: 3.609375\nStep 7601, Training loss: 3.6977343559265137, Elapsed Time: 50.87 seconds\nStep 7601, Validation loss: 3.671875\nStep 7801, Training loss: 3.694218635559082, Elapsed Time: 50.86 seconds\nStep 7801, Validation loss: 3.640625\nStep 8001, Training loss: 3.685546875, Elapsed Time: 50.72 seconds\nStep 8001, Validation loss: 3.71875\nStep 8201, Training loss: 3.6736717224121094, Elapsed Time: 50.88 seconds\nStep 8201, Validation loss: 3.65625\nStep 8401, Training loss: 3.66796875, Elapsed Time: 50.72 seconds\nStep 8401, Validation loss: 3.546875\nStep 8601, Training loss: 3.6586718559265137, Elapsed Time: 50.87 seconds\nStep 8601, Validation loss: 3.671875\nStep 8801, Training loss: 3.6527342796325684, Elapsed Time: 50.72 seconds\nStep 8801, Validation loss: 3.625\nStep 9001, Training loss: 3.645390510559082, Elapsed Time: 51.10 seconds\nStep 9001, Validation loss: 3.59375\nStep 9201, Training loss: 3.631953001022339, Elapsed Time: 50.86 seconds\nStep 9201, Validation loss: 3.65625\nStep 9401, Training loss: 3.6253905296325684, Elapsed Time: 50.88 seconds\nStep 9401, Validation loss: 3.625\nStep 9601, Training loss: 3.623906135559082, Elapsed Time: 50.86 seconds\nStep 9601, Validation loss: 3.578125\nStep 9801, Training loss: 3.6165623664855957, Elapsed Time: 50.86 seconds\nStep 9801, Validation loss: 3.640625\nStep 10001, Training loss: 3.609452962875366, Elapsed Time: 51.00 seconds\nStep 10001, Validation loss: 3.5625\nStep 10201, Training loss: 3.601640462875366, Elapsed Time: 50.72 seconds\nStep 10201, Validation loss: 3.5625\nStep 10401, Training loss: 3.591562509536743, Elapsed Time: 50.87 seconds\nStep 10401, Validation loss: 3.625\nStep 10601, Training loss: 3.5906248092651367, Elapsed Time: 50.72 seconds\nStep 10601, Validation loss: 3.609375\nStep 10801, Training loss: 3.5886716842651367, Elapsed Time: 50.86 seconds\nStep 10801, Validation loss: 3.53125\nStep 11001, Training loss: 3.5766406059265137, Elapsed Time: 50.85 seconds\nStep 11001, Validation loss: 3.671875\nStep 11201, Training loss: 3.573906183242798, Elapsed Time: 50.72 seconds\nStep 11201, Validation loss: 3.59375\nStep 11401, Training loss: 3.5643749237060547, Elapsed Time: 50.87 seconds\nStep 11401, Validation loss: 3.609375\nStep 11601, Training loss: 3.566171884536743, Elapsed Time: 50.85 seconds\nStep 11601, Validation loss: 3.625\nStep 11801, Training loss: 3.5620312690734863, Elapsed Time: 50.88 seconds\nStep 11801, Validation loss: 3.609375\nStep 12001, Training loss: 3.554374933242798, Elapsed Time: 50.86 seconds\nStep 12001, Validation loss: 3.5625\nStep 12201, Training loss: 3.554921865463257, Elapsed Time: 50.71 seconds\nStep 12201, Validation loss: 3.546875\nStep 12401, Training loss: 3.5444531440734863, Elapsed Time: 50.86 seconds\nStep 12401, Validation loss: 3.59375\nStep 12601, Training loss: 3.5464062690734863, Elapsed Time: 50.72 seconds\nStep 12601, Validation loss: 3.59375\nStep 12801, Training loss: 3.5392186641693115, Elapsed Time: 50.87 seconds\nStep 12801, Validation loss: 3.5625\nStep 13001, Training loss: 3.5353124141693115, Elapsed Time: 50.88 seconds\nStep 13001, Validation loss: 3.546875\nStep 13201, Training loss: 3.5320310592651367, Elapsed Time: 50.72 seconds\nStep 13201, Validation loss: 3.53125\nStep 13401, Training loss: 3.5285937786102295, Elapsed Time: 50.85 seconds\nStep 13401, Validation loss: 3.4375\nStep 13601, Training loss: 3.521171808242798, Elapsed Time: 50.72 seconds\nStep 13601, Validation loss: 3.578125\nStep 13801, Training loss: 3.520937442779541, Elapsed Time: 50.86 seconds\nStep 13801, Validation loss: 3.5625\nStep 14001, Training loss: 3.5171093940734863, Elapsed Time: 51.00 seconds\nStep 14001, Validation loss: 3.5\nStep 14201, Training loss: 3.5093748569488525, Elapsed Time: 50.74 seconds\nStep 14201, Validation loss: 3.53125\nStep 14401, Training loss: 3.5077342987060547, Elapsed Time: 50.86 seconds\nStep 14401, Validation loss: 3.484375\nStep 14601, Training loss: 3.5028905868530273, Elapsed Time: 50.88 seconds\nStep 14601, Validation loss: 3.53125\nStep 14801, Training loss: 3.49859356880188, Elapsed Time: 50.85 seconds\nStep 14801, Validation loss: 3.53125\nStep 15001, Training loss: 3.5009374618530273, Elapsed Time: 50.85 seconds\nStep 15001, Validation loss: 3.453125\nStep 15201, Training loss: 3.486562490463257, Elapsed Time: 50.86 seconds\nStep 15201, Validation loss: 3.453125\nStep 15401, Training loss: 3.487734317779541, Elapsed Time: 50.85 seconds\nStep 15401, Validation loss: 3.421875\nStep 15601, Training loss: 3.4850780963897705, Elapsed Time: 50.86 seconds\nStep 15601, Validation loss: 3.4375\nStep 15801, Training loss: 3.4850780963897705, Elapsed Time: 50.85 seconds\nStep 15801, Validation loss: 3.453125\nStep 16001, Training loss: 3.4809374809265137, Elapsed Time: 50.86 seconds\nStep 16001, Validation loss: 3.453125\nStep 16201, Training loss: 3.476249933242798, Elapsed Time: 50.72 seconds\nStep 16201, Validation loss: 3.390625\nStep 16401, Training loss: 3.475703001022339, Elapsed Time: 50.87 seconds\nStep 16401, Validation loss: 3.46875\nStep 16601, Training loss: 3.4767186641693115, Elapsed Time: 50.86 seconds\nStep 16601, Validation loss: 3.421875\nStep 16801, Training loss: 3.4613280296325684, Elapsed Time: 50.85 seconds\nStep 16801, Validation loss: 3.421875\nStep 17001, Training loss: 3.4618749618530273, Elapsed Time: 50.96 seconds\nStep 17001, Validation loss: 3.515625\nStep 17201, Training loss: 3.467031240463257, Elapsed Time: 50.72 seconds\nStep 17201, Validation loss: 3.46875\nStep 17401, Training loss: 3.461249828338623, Elapsed Time: 50.86 seconds\nStep 17401, Validation loss: 3.453125\nStep 17601, Training loss: 3.4628124237060547, Elapsed Time: 50.85 seconds\nStep 17601, Validation loss: 3.546875\nStep 17801, Training loss: 3.4583592414855957, Elapsed Time: 50.87 seconds\nStep 17801, Validation loss: 3.40625\nStep 18001, Training loss: 3.4549217224121094, Elapsed Time: 51.06 seconds\nStep 18001, Validation loss: 3.46875\nStep 18201, Training loss: 3.4480466842651367, Elapsed Time: 50.72 seconds\nStep 18201, Validation loss: 3.40625\nStep 18401, Training loss: 3.455937385559082, Elapsed Time: 50.72 seconds\nStep 18401, Validation loss: 3.4375\nStep 18601, Training loss: 3.4453125, Elapsed Time: 50.87 seconds\nStep 18601, Validation loss: 3.484375\nStep 18801, Training loss: 3.4446094036102295, Elapsed Time: 50.87 seconds\nStep 18801, Validation loss: 3.546875\nStep 19001, Training loss: 3.4410154819488525, Elapsed Time: 50.87 seconds\nStep 19001, Validation loss: 3.4375\nStep 19201, Training loss: 3.4382810592651367, Elapsed Time: 50.72 seconds\nStep 19201, Validation loss: 3.4375\nStep 19401, Training loss: 3.437577962875366, Elapsed Time: 50.85 seconds\nStep 19401, Validation loss: 3.4375\nStep 19601, Training loss: 3.4362499713897705, Elapsed Time: 50.72 seconds\nStep 19601, Validation loss: 3.453125\nStep 19801, Training loss: 3.42828106880188, Elapsed Time: 50.86 seconds\nStep 19801, Validation loss: 3.421875\nStep 20001, Training loss: 3.429765462875366, Elapsed Time: 50.86 seconds\nStep 20001, Validation loss: 3.4375\nStep 20201, Training loss: 3.4281249046325684, Elapsed Time: 50.88 seconds\nStep 20201, Validation loss: 3.5\nStep 20401, Training loss: 3.426406145095825, Elapsed Time: 50.87 seconds\nStep 20401, Validation loss: 3.5\nStep 20601, Training loss: 3.4209375381469727, Elapsed Time: 50.72 seconds\nStep 20601, Validation loss: 3.375\nStep 20801, Training loss: 3.417421817779541, Elapsed Time: 50.86 seconds\nStep 20801, Validation loss: 3.421875\nStep 21001, Training loss: 3.4248437881469727, Elapsed Time: 50.83 seconds\nStep 21001, Validation loss: 3.421875\nStep 21201, Training loss: 3.4159374237060547, Elapsed Time: 50.72 seconds\nStep 21201, Validation loss: 3.421875\nStep 21401, Training loss: 3.4172656536102295, Elapsed Time: 50.72 seconds\nStep 21401, Validation loss: 3.390625\nStep 21601, Training loss: 3.4161717891693115, Elapsed Time: 50.87 seconds\nStep 21601, Validation loss: 3.375\nStep 21801, Training loss: 3.409296751022339, Elapsed Time: 50.72 seconds\nStep 21801, Validation loss: 3.390625\nStep 22001, Training loss: 3.408828020095825, Elapsed Time: 50.93 seconds\nStep 22001, Validation loss: 3.390625\nStep 22201, Training loss: 3.407343626022339, Elapsed Time: 50.72 seconds\nStep 22201, Validation loss: 3.421875\nStep 22401, Training loss: 3.401484251022339, Elapsed Time: 50.86 seconds\nStep 22401, Validation loss: 3.4375\nStep 22601, Training loss: 3.404374837875366, Elapsed Time: 50.85 seconds\nStep 22601, Validation loss: 3.4375\nStep 22801, Training loss: 3.403749942779541, Elapsed Time: 50.86 seconds\nStep 22801, Validation loss: 3.375\nStep 23001, Training loss: 3.3943748474121094, Elapsed Time: 50.86 seconds\nStep 23001, Validation loss: 3.4375\nStep 23201, Training loss: 3.394609212875366, Elapsed Time: 50.88 seconds\nStep 23201, Validation loss: 3.390625\nStep 23401, Training loss: 3.390859365463257, Elapsed Time: 50.72 seconds\nStep 23401, Validation loss: 3.390625\nStep 23601, Training loss: 3.3927342891693115, Elapsed Time: 50.85 seconds\nStep 23601, Validation loss: 3.53125\nStep 23801, Training loss: 3.3924217224121094, Elapsed Time: 50.87 seconds\nStep 23801, Validation loss: 3.421875\nStep 24001, Training loss: 3.3885154724121094, Elapsed Time: 50.86 seconds\nStep 24001, Validation loss: 3.375\nStep 24201, Training loss: 3.3843748569488525, Elapsed Time: 50.87 seconds\nStep 24201, Validation loss: 3.375\nStep 24401, Training loss: 3.3852343559265137, Elapsed Time: 50.85 seconds\nStep 24401, Validation loss: 3.40625\nStep 24601, Training loss: 3.3826560974121094, Elapsed Time: 50.72 seconds\nStep 24601, Validation loss: 3.359375\nStep 24801, Training loss: 3.3785154819488525, Elapsed Time: 50.86 seconds\nStep 24801, Validation loss: 3.421875\nStep 25001, Training loss: 3.381484270095825, Elapsed Time: 50.84 seconds\nStep 25001, Validation loss: 3.40625\nStep 25201, Training loss: 3.383046865463257, Elapsed Time: 50.87 seconds\nStep 25201, Validation loss: 3.4375\nStep 25401, Training loss: 3.372734308242798, Elapsed Time: 50.86 seconds\nStep 25401, Validation loss: 3.34375\nStep 25601, Training loss: 3.37890625, Elapsed Time: 50.86 seconds\nStep 25601, Validation loss: 3.359375\nStep 25801, Training loss: 3.378984212875366, Elapsed Time: 50.71 seconds\nStep 25801, Validation loss: 3.3125\nStep 26001, Training loss: 3.369218587875366, Elapsed Time: 51.02 seconds\nStep 26001, Validation loss: 3.453125\nStep 26201, Training loss: 3.372734308242798, Elapsed Time: 50.72 seconds\nStep 26201, Validation loss: 3.328125\nStep 26401, Training loss: 3.36773419380188, Elapsed Time: 50.85 seconds\nStep 26401, Validation loss: 3.375\nStep 26601, Training loss: 3.369140625, Elapsed Time: 50.88 seconds\nStep 26601, Validation loss: 3.390625\nStep 26801, Training loss: 3.3628904819488525, Elapsed Time: 50.87 seconds\nStep 26801, Validation loss: 3.421875\nStep 27001, Training loss: 3.368046760559082, Elapsed Time: 50.72 seconds\nStep 27001, Validation loss: 3.328125\nStep 27201, Training loss: 3.3642969131469727, Elapsed Time: 50.86 seconds\nStep 27201, Validation loss: 3.390625\nStep 27401, Training loss: 3.3671875, Elapsed Time: 50.72 seconds\nStep 27401, Validation loss: 3.375\nStep 27601, Training loss: 3.3611717224121094, Elapsed Time: 51.11 seconds\nStep 27601, Validation loss: 3.34375\nStep 27801, Training loss: 3.363359212875366, Elapsed Time: 50.72 seconds\nStep 27801, Validation loss: 3.34375\nStep 28001, Training loss: 3.357421875, Elapsed Time: 50.86 seconds\nStep 28001, Validation loss: 3.453125\nStep 28201, Training loss: 3.361640453338623, Elapsed Time: 50.72 seconds\nStep 28201, Validation loss: 3.375\nStep 28401, Training loss: 3.35546875, Elapsed Time: 50.85 seconds\nStep 28401, Validation loss: 3.390625\nStep 28601, Training loss: 3.3572654724121094, Elapsed Time: 50.72 seconds\nStep 28601, Validation loss: 3.421875\nStep 28801, Training loss: 3.351640462875366, Elapsed Time: 50.72 seconds\nStep 28801, Validation loss: 3.3125\nStep 29001, Training loss: 3.3517186641693115, Elapsed Time: 50.87 seconds\nStep 29001, Validation loss: 3.375\nStep 29201, Training loss: 3.355781078338623, Elapsed Time: 50.87 seconds\nStep 29201, Validation loss: 3.34375\nStep 29401, Training loss: 3.346796751022339, Elapsed Time: 50.86 seconds\nStep 29401, Validation loss: 3.375\nStep 29601, Training loss: 3.343827962875366, Elapsed Time: 50.95 seconds\nStep 29601, Validation loss: 3.390625\nStep 29801, Training loss: 3.340156078338623, Elapsed Time: 50.87 seconds\nStep 29801, Validation loss: 3.34375\nStep 30001, Training loss: 3.342031240463257, Elapsed Time: 50.72 seconds\nStep 30001, Validation loss: 3.359375\nStep 30201, Training loss: 3.3414061069488525, Elapsed Time: 50.86 seconds\nStep 30201, Validation loss: 3.265625\nStep 30401, Training loss: 3.3417186737060547, Elapsed Time: 50.86 seconds\nStep 30401, Validation loss: 3.3125\nStep 30601, Training loss: 3.3408594131469727, Elapsed Time: 51.12 seconds\nStep 30601, Validation loss: 3.296875\nStep 30801, Training loss: 3.335624933242798, Elapsed Time: 50.72 seconds\nStep 30801, Validation loss: 3.421875\nStep 31001, Training loss: 3.334218740463257, Elapsed Time: 50.73 seconds\nStep 31001, Validation loss: 3.328125\nStep 31201, Training loss: 3.333203077316284, Elapsed Time: 50.72 seconds\nStep 31201, Validation loss: 3.34375\nStep 31401, Training loss: 3.3371875286102295, Elapsed Time: 50.72 seconds\nStep 31401, Validation loss: 3.3125\nStep 31601, Training loss: 3.3352344036102295, Elapsed Time: 51.14 seconds\nStep 31601, Validation loss: 3.34375\nStep 31801, Training loss: 3.333671808242798, Elapsed Time: 50.86 seconds\nStep 31801, Validation loss: 3.34375\nStep 32001, Training loss: 3.3293750286102295, Elapsed Time: 50.87 seconds\nStep 32001, Validation loss: 3.328125\nStep 32201, Training loss: 3.328359365463257, Elapsed Time: 50.86 seconds\nStep 32201, Validation loss: 3.3125\nStep 32401, Training loss: 3.329765558242798, Elapsed Time: 50.72 seconds\nStep 32401, Validation loss: 3.328125\nStep 32601, Training loss: 3.319765567779541, Elapsed Time: 50.86 seconds\nStep 32601, Validation loss: 3.359375\nStep 32801, Training loss: 3.3235936164855957, Elapsed Time: 50.72 seconds\nStep 32801, Validation loss: 3.328125\nStep 33001, Training loss: 3.326484203338623, Elapsed Time: 50.86 seconds\nStep 33001, Validation loss: 3.453125\nStep 33201, Training loss: 3.3216404914855957, Elapsed Time: 50.87 seconds\nStep 33201, Validation loss: 3.3125\nStep 33401, Training loss: 3.3159375190734863, Elapsed Time: 50.85 seconds\nStep 33401, Validation loss: 3.296875\nStep 33601, Training loss: 3.3216404914855957, Elapsed Time: 50.93 seconds\nStep 33601, Validation loss: 3.375\nStep 33801, Training loss: 3.3196094036102295, Elapsed Time: 50.72 seconds\nStep 33801, Validation loss: 3.34375\nStep 34001, Training loss: 3.3134374618530273, Elapsed Time: 50.73 seconds\nStep 34001, Validation loss: 3.359375\nStep 34201, Training loss: 3.318437337875366, Elapsed Time: 50.86 seconds\nStep 34201, Validation loss: 3.359375\nStep 34401, Training loss: 3.3132030963897705, Elapsed Time: 50.87 seconds\nStep 34401, Validation loss: 3.359375\nStep 34601, Training loss: 3.311406135559082, Elapsed Time: 51.04 seconds\nStep 34601, Validation loss: 3.328125\nStep 34801, Training loss: 3.309765577316284, Elapsed Time: 50.85 seconds\nStep 34801, Validation loss: 3.34375\nStep 35001, Training loss: 3.306328058242798, Elapsed Time: 50.72 seconds\nStep 35001, Validation loss: 3.296875\nStep 35201, Training loss: 3.3104686737060547, Elapsed Time: 50.86 seconds\nStep 35201, Validation loss: 3.3125\nStep 35401, Training loss: 3.306718587875366, Elapsed Time: 50.85 seconds\nStep 35401, Validation loss: 3.390625\nStep 35601, Training loss: 3.3056249618530273, Elapsed Time: 51.05 seconds\nStep 35601, Validation loss: 3.40625\nStep 35801, Training loss: 3.312265634536743, Elapsed Time: 50.86 seconds\nStep 35801, Validation loss: 3.328125\nStep 36001, Training loss: 3.30523419380188, Elapsed Time: 50.86 seconds\nStep 36001, Validation loss: 3.328125\nStep 36201, Training loss: 3.3051562309265137, Elapsed Time: 50.86 seconds\nStep 36201, Validation loss: 3.328125\nStep 36401, Training loss: 3.2983593940734863, Elapsed Time: 50.87 seconds\nStep 36401, Validation loss: 3.3125\nStep 36601, Training loss: 3.3037500381469727, Elapsed Time: 50.72 seconds\nStep 36601, Validation loss: 3.328125\nStep 36801, Training loss: 3.3026561737060547, Elapsed Time: 50.72 seconds\nStep 36801, Validation loss: 3.328125\nStep 37001, Training loss: 3.301953077316284, Elapsed Time: 50.86 seconds\nStep 37001, Validation loss: 3.265625\nStep 37201, Training loss: 3.2986717224121094, Elapsed Time: 50.86 seconds\nStep 37201, Validation loss: 3.28125\nStep 37401, Training loss: 3.3037500381469727, Elapsed Time: 50.86 seconds\nStep 37401, Validation loss: 3.265625\nStep 37601, Training loss: 3.298046827316284, Elapsed Time: 51.00 seconds\nStep 37601, Validation loss: 3.296875\nStep 37801, Training loss: 3.2956249713897705, Elapsed Time: 50.71 seconds\nStep 37801, Validation loss: 3.328125\nStep 38001, Training loss: 3.2906248569488525, Elapsed Time: 50.72 seconds\nStep 38001, Validation loss: 3.3125\nStep 38201, Training loss: 3.29296875, Elapsed Time: 50.86 seconds\nStep 38201, Validation loss: 3.265625\nStep 38401, Training loss: 3.2878124713897705, Elapsed Time: 50.86 seconds\nStep 38401, Validation loss: 3.390625\nStep 38601, Training loss: 3.2894530296325684, Elapsed Time: 51.32 seconds\nStep 38601, Validation loss: 3.21875\nStep 38801, Training loss: 3.2914843559265137, Elapsed Time: 50.86 seconds\nStep 38801, Validation loss: 3.296875\nStep 39001, Training loss: 3.2858593463897705, Elapsed Time: 50.73 seconds\nStep 39001, Validation loss: 3.265625\nStep 39201, Training loss: 3.286562442779541, Elapsed Time: 50.72 seconds\nStep 39201, Validation loss: 3.234375\nStep 39401, Training loss: 3.2867186069488525, Elapsed Time: 50.86 seconds\nStep 39401, Validation loss: 3.265625\nStep 39601, Training loss: 3.282343626022339, Elapsed Time: 51.18 seconds\nStep 39601, Validation loss: 3.34375\nStep 39801, Training loss: 3.279609203338623, Elapsed Time: 50.72 seconds\nStep 39801, Validation loss: 3.375\nStep 40001, Training loss: 3.274296760559082, Elapsed Time: 50.88 seconds\nStep 40001, Validation loss: 3.34375\nStep 40201, Training loss: 3.2764062881469727, Elapsed Time: 50.71 seconds\nStep 40201, Validation loss: 3.34375\nStep 40401, Training loss: 3.279921770095825, Elapsed Time: 50.85 seconds\nStep 40401, Validation loss: 3.28125\nStep 40601, Training loss: 3.2734375, Elapsed Time: 50.86 seconds\nStep 40601, Validation loss: 3.21875\nStep 40801, Training loss: 3.277578115463257, Elapsed Time: 50.72 seconds\nStep 40801, Validation loss: 3.296875\nStep 41001, Training loss: 3.2749998569488525, Elapsed Time: 50.72 seconds\nStep 41001, Validation loss: 3.28125\nStep 41201, Training loss: 3.278203010559082, Elapsed Time: 50.86 seconds\nStep 41201, Validation loss: 3.203125\nStep 41401, Training loss: 3.2727344036102295, Elapsed Time: 50.88 seconds\nStep 41401, Validation loss: 3.3125\nStep 41601, Training loss: 3.27398419380188, Elapsed Time: 50.97 seconds\nStep 41601, Validation loss: 3.25\nStep 41801, Training loss: 3.270390510559082, Elapsed Time: 50.72 seconds\nStep 41801, Validation loss: 3.25\nStep 42001, Training loss: 3.267031192779541, Elapsed Time: 50.86 seconds\nStep 42001, Validation loss: 3.28125\nStep 42201, Training loss: 3.268437385559082, Elapsed Time: 50.72 seconds\nStep 42201, Validation loss: 3.328125\nStep 42401, Training loss: 3.268749952316284, Elapsed Time: 50.88 seconds\nStep 42401, Validation loss: 3.25\nStep 42601, Training loss: 3.2705469131469727, Elapsed Time: 50.94 seconds\nStep 42601, Validation loss: 3.265625\nStep 42801, Training loss: 3.2635154724121094, Elapsed Time: 50.72 seconds\nStep 42801, Validation loss: 3.234375\nStep 43001, Training loss: 3.2655467987060547, Elapsed Time: 50.71 seconds\nStep 43001, Validation loss: 3.265625\nStep 43201, Training loss: 3.2665624618530273, Elapsed Time: 50.88 seconds\nStep 43201, Validation loss: 3.28125\nStep 43401, Training loss: 3.260624885559082, Elapsed Time: 50.88 seconds\nStep 43401, Validation loss: 3.28125\nStep 43601, Training loss: 3.259453058242798, Elapsed Time: 50.91 seconds\nStep 43601, Validation loss: 3.234375\nStep 43801, Training loss: 3.2654685974121094, Elapsed Time: 50.88 seconds\nStep 43801, Validation loss: 3.28125\nStep 44001, Training loss: 3.264296770095825, Elapsed Time: 50.73 seconds\nStep 44001, Validation loss: 3.25\nStep 44201, Training loss: 3.257265567779541, Elapsed Time: 50.85 seconds\nStep 44201, Validation loss: 3.34375\nStep 44401, Training loss: 3.2521092891693115, Elapsed Time: 50.87 seconds\nStep 44401, Validation loss: 3.1875\nStep 44601, Training loss: 3.25445294380188, Elapsed Time: 50.92 seconds\nStep 44601, Validation loss: 3.28125\nStep 44801, Training loss: 3.2599217891693115, Elapsed Time: 50.72 seconds\nStep 44801, Validation loss: 3.375\nStep 45001, Training loss: 3.2517967224121094, Elapsed Time: 50.71 seconds\nStep 45001, Validation loss: 3.28125\nStep 45201, Training loss: 3.258046865463257, Elapsed Time: 50.86 seconds\nStep 45201, Validation loss: 3.265625\nStep 45401, Training loss: 3.2534375190734863, Elapsed Time: 50.85 seconds\nStep 45401, Validation loss: 3.1875\nStep 45601, Training loss: 3.2521092891693115, Elapsed Time: 51.12 seconds\nStep 45601, Validation loss: 3.34375\nStep 45801, Training loss: 3.252578020095825, Elapsed Time: 50.72 seconds\nStep 45801, Validation loss: 3.265625\nStep 46001, Training loss: 3.24468731880188, Elapsed Time: 50.73 seconds\nStep 46001, Validation loss: 3.265625\nStep 46201, Training loss: 3.249453067779541, Elapsed Time: 50.72 seconds\nStep 46201, Validation loss: 3.234375\nStep 46401, Training loss: 3.239374876022339, Elapsed Time: 50.87 seconds\nStep 46401, Validation loss: 3.28125\nStep 46601, Training loss: 3.2465624809265137, Elapsed Time: 51.11 seconds\nStep 46601, Validation loss: 3.234375\nStep 46801, Training loss: 3.2474217414855957, Elapsed Time: 50.72 seconds\nStep 46801, Validation loss: 3.21875\nStep 47001, Training loss: 3.24664044380188, Elapsed Time: 50.72 seconds\nStep 47001, Validation loss: 3.234375\nStep 47201, Training loss: 3.2423436641693115, Elapsed Time: 50.86 seconds\nStep 47201, Validation loss: 3.203125\nStep 47401, Training loss: 3.248046875, Elapsed Time: 50.86 seconds\nStep 47401, Validation loss: 3.25\nStep 47601, Training loss: 3.240859270095825, Elapsed Time: 50.72 seconds\nStep 47601, Validation loss: 3.21875\nStep 47801, Training loss: 3.2372655868530273, Elapsed Time: 50.73 seconds\nStep 47801, Validation loss: 3.203125\nStep 48001, Training loss: 3.2381248474121094, Elapsed Time: 51.05 seconds\nStep 48001, Validation loss: 3.203125\nStep 48201, Training loss: 3.2389843463897705, Elapsed Time: 50.74 seconds\nStep 48201, Validation loss: 3.296875\nStep 48401, Training loss: 3.241953134536743, Elapsed Time: 50.88 seconds\nStep 48401, Validation loss: 3.203125\nStep 48601, Training loss: 3.2294530868530273, Elapsed Time: 50.97 seconds\nStep 48601, Validation loss: 3.171875\nStep 48801, Training loss: 3.2314844131469727, Elapsed Time: 50.72 seconds\nStep 48801, Validation loss: 3.234375\nStep 49001, Training loss: 3.2314844131469727, Elapsed Time: 50.86 seconds\nStep 49001, Validation loss: 3.265625\nStep 49201, Training loss: 3.232109308242798, Elapsed Time: 50.72 seconds\nStep 49201, Validation loss: 3.234375\nStep 49401, Training loss: 3.227656126022339, Elapsed Time: 50.72 seconds\nStep 49401, Validation loss: 3.265625\nStep 49601, Training loss: 3.23101544380188, Elapsed Time: 51.06 seconds\nStep 49601, Validation loss: 3.3125\nStep 49801, Training loss: 3.226249933242798, Elapsed Time: 50.85 seconds\nStep 49801, Validation loss: 3.1875\nStep 50001, Training loss: 3.225781202316284, Elapsed Time: 50.88 seconds\nStep 50001, Validation loss: 3.296875\nStep 50201, Training loss: 3.222890615463257, Elapsed Time: 50.86 seconds\nStep 50201, Validation loss: 3.203125\nStep 50401, Training loss: 3.2233593463897705, Elapsed Time: 50.72 seconds\nStep 50401, Validation loss: 3.25\nStep 50601, Training loss: 3.2295312881469727, Elapsed Time: 51.20 seconds\nStep 50601, Validation loss: 3.234375\nStep 50801, Training loss: 3.2189061641693115, Elapsed Time: 50.86 seconds\nStep 50801, Validation loss: 3.171875\nStep 51001, Training loss: 3.220156192779541, Elapsed Time: 50.71 seconds\nStep 51001, Validation loss: 3.203125\nStep 51201, Training loss: 3.218827962875366, Elapsed Time: 50.71 seconds\nStep 51201, Validation loss: 3.171875\nStep 51401, Training loss: 3.220390558242798, Elapsed Time: 50.86 seconds\nStep 51401, Validation loss: 3.203125\nStep 51601, Training loss: 3.2163281440734863, Elapsed Time: 50.87 seconds\nStep 51601, Validation loss: 3.09375\nStep 51801, Training loss: 3.223749876022339, Elapsed Time: 50.85 seconds\nStep 51801, Validation loss: 3.25\nStep 52001, Training loss: 3.2192187309265137, Elapsed Time: 50.72 seconds\nStep 52001, Validation loss: 3.234375\nStep 52201, Training loss: 3.2152342796325684, Elapsed Time: 50.86 seconds\nStep 52201, Validation loss: 3.171875\nStep 52401, Training loss: 3.212890625, Elapsed Time: 50.72 seconds\nStep 52401, Validation loss: 3.1875\nStep 52601, Training loss: 3.224843740463257, Elapsed Time: 51.06 seconds\nStep 52601, Validation loss: 3.140625\nStep 52801, Training loss: 3.2091405391693115, Elapsed Time: 50.72 seconds\nStep 52801, Validation loss: 3.171875\nStep 53001, Training loss: 3.210390567779541, Elapsed Time: 50.72 seconds\nStep 53001, Validation loss: 3.296875\nStep 53201, Training loss: 3.2078123092651367, Elapsed Time: 50.88 seconds\nStep 53201, Validation loss: 3.203125\nStep 53401, Training loss: 3.2074999809265137, Elapsed Time: 50.87 seconds\nStep 53401, Validation loss: 3.28125\nStep 53601, Training loss: 3.2071874141693115, Elapsed Time: 50.86 seconds\nStep 53601, Validation loss: 3.15625\nStep 53801, Training loss: 3.208671808242798, Elapsed Time: 50.73 seconds\nStep 53801, Validation loss: 3.25\nStep 54001, Training loss: 3.2078123092651367, Elapsed Time: 50.86 seconds\nStep 54001, Validation loss: 3.21875\nStep 54201, Training loss: 3.199843645095825, Elapsed Time: 50.86 seconds\nStep 54201, Validation loss: 3.15625\nStep 54401, Training loss: 3.2035155296325684, Elapsed Time: 50.86 seconds\nStep 54401, Validation loss: 3.21875\nStep 54601, Training loss: 3.2093749046325684, Elapsed Time: 51.02 seconds\nStep 54601, Validation loss: 3.265625\nStep 54801, Training loss: 3.2097654342651367, Elapsed Time: 50.86 seconds\nStep 54801, Validation loss: 3.203125\nStep 55001, Training loss: 3.201484203338623, Elapsed Time: 50.72 seconds\nStep 55001, Validation loss: 3.25\nStep 55201, Training loss: 3.2001562118530273, Elapsed Time: 50.86 seconds\nStep 55201, Validation loss: 3.171875\nStep 55401, Training loss: 3.2021875381469727, Elapsed Time: 50.87 seconds\nStep 55401, Validation loss: 3.109375\nStep 55601, Training loss: 3.194999933242798, Elapsed Time: 50.99 seconds\nStep 55601, Validation loss: 3.15625\nStep 55801, Training loss: 3.203437328338623, Elapsed Time: 50.72 seconds\nStep 55801, Validation loss: 3.234375\nStep 56001, Training loss: 3.195078134536743, Elapsed Time: 50.73 seconds\nStep 56001, Validation loss: 3.28125\nStep 56201, Training loss: 3.1960155963897705, Elapsed Time: 50.85 seconds\nStep 56201, Validation loss: 3.234375\nStep 56401, Training loss: 3.200390577316284, Elapsed Time: 50.72 seconds\nStep 56401, Validation loss: 3.1875\nStep 56601, Training loss: 3.194531202316284, Elapsed Time: 51.19 seconds\nStep 56601, Validation loss: 3.234375\nStep 56801, Training loss: 3.1890623569488525, Elapsed Time: 50.86 seconds\nStep 56801, Validation loss: 3.171875\nStep 57001, Training loss: 3.1892967224121094, Elapsed Time: 50.86 seconds\nStep 57001, Validation loss: 3.078125\nStep 57201, Training loss: 3.189140558242798, Elapsed Time: 50.85 seconds\nStep 57201, Validation loss: 3.25\nStep 57401, Training loss: 3.1887500286102295, Elapsed Time: 50.71 seconds\nStep 57401, Validation loss: 3.28125\nStep 57601, Training loss: 3.1854686737060547, Elapsed Time: 51.19 seconds\nStep 57601, Validation loss: 3.203125\nStep 57801, Training loss: 3.18414044380188, Elapsed Time: 50.86 seconds\nStep 57801, Validation loss: 3.171875\nStep 58001, Training loss: 3.18414044380188, Elapsed Time: 50.71 seconds\nStep 58001, Validation loss: 3.171875\nStep 58201, Training loss: 3.189687490463257, Elapsed Time: 50.87 seconds\nStep 58201, Validation loss: 3.28125\nStep 58401, Training loss: 3.1889843940734863, Elapsed Time: 50.72 seconds\nStep 58401, Validation loss: 3.234375\nStep 58601, Training loss: 3.1771092414855957, Elapsed Time: 50.87 seconds\nStep 58601, Validation loss: 3.15625\nStep 58801, Training loss: 3.181874990463257, Elapsed Time: 50.72 seconds\nStep 58801, Validation loss: 3.171875\nStep 59001, Training loss: 3.17578125, Elapsed Time: 50.71 seconds\nStep 59001, Validation loss: 3.1875\nStep 59201, Training loss: 3.17632794380188, Elapsed Time: 50.72 seconds\nStep 59201, Validation loss: 3.203125\nStep 59401, Training loss: 3.178906202316284, Elapsed Time: 50.86 seconds\nStep 59401, Validation loss: 3.21875\nStep 59601, Training loss: 3.179921865463257, Elapsed Time: 50.99 seconds\nStep 59601, Validation loss: 3.21875\nStep 59801, Training loss: 3.1817967891693115, Elapsed Time: 50.72 seconds\nStep 59801, Validation loss: 3.21875\nStep 60001, Training loss: 3.173593759536743, Elapsed Time: 50.72 seconds\nStep 60001, Validation loss: 3.109375\nStep 60201, Training loss: 3.174453020095825, Elapsed Time: 50.87 seconds\nStep 60201, Validation loss: 3.1875\nStep 60401, Training loss: 3.1653904914855957, Elapsed Time: 50.72 seconds\nStep 60401, Validation loss: 3.1875\nStep 60601, Training loss: 3.1742186546325684, Elapsed Time: 51.18 seconds\nStep 60601, Validation loss: 3.078125\nStep 60801, Training loss: 3.1764843463897705, Elapsed Time: 50.86 seconds\nStep 60801, Validation loss: 3.140625\nStep 61001, Training loss: 3.1736717224121094, Elapsed Time: 50.72 seconds\nStep 61001, Validation loss: 3.140625\nStep 61201, Training loss: 3.1689844131469727, Elapsed Time: 50.86 seconds\nStep 61201, Validation loss: 3.15625\nStep 61401, Training loss: 3.1639842987060547, Elapsed Time: 50.86 seconds\nStep 61401, Validation loss: 3.1875\nStep 61601, Training loss: 3.1695311069488525, Elapsed Time: 51.05 seconds\nStep 61601, Validation loss: 3.171875\nStep 61801, Training loss: 3.1714062690734863, Elapsed Time: 50.85 seconds\nStep 61801, Validation loss: 3.1875\nStep 62001, Training loss: 3.1594531536102295, Elapsed Time: 50.88 seconds\nStep 62001, Validation loss: 3.1875\nStep 62201, Training loss: 3.1647655963897705, Elapsed Time: 50.88 seconds\nStep 62201, Validation loss: 3.140625\nStep 62401, Training loss: 3.1667187213897705, Elapsed Time: 50.72 seconds\nStep 62401, Validation loss: 3.21875\nStep 62601, Training loss: 3.1636717319488525, Elapsed Time: 50.87 seconds\nStep 62601, Validation loss: 3.1875\nStep 62801, Training loss: 3.16656231880188, Elapsed Time: 50.72 seconds\nStep 62801, Validation loss: 3.1875\nStep 63001, Training loss: 3.161328077316284, Elapsed Time: 50.87 seconds\nStep 63001, Validation loss: 3.140625\nStep 63201, Training loss: 3.161875009536743, Elapsed Time: 50.72 seconds\nStep 63201, Validation loss: 3.09375\nStep 63401, Training loss: 3.1645312309265137, Elapsed Time: 50.86 seconds\nStep 63401, Validation loss: 3.140625\nStep 63601, Training loss: 3.1558592319488525, Elapsed Time: 50.91 seconds\nStep 63601, Validation loss: 3.09375\nStep 63801, Training loss: 3.1628124713897705, Elapsed Time: 50.73 seconds\nStep 63801, Validation loss: 3.171875\nStep 64001, Training loss: 3.162890672683716, Elapsed Time: 50.72 seconds\nStep 64001, Validation loss: 3.25\nStep 64201, Training loss: 3.1535937786102295, Elapsed Time: 50.86 seconds\nStep 64201, Validation loss: 3.171875\nStep 64401, Training loss: 3.155703067779541, Elapsed Time: 50.72 seconds\nStep 64401, Validation loss: 3.171875\nStep 64601, Training loss: 3.1561717987060547, Elapsed Time: 51.28 seconds\nStep 64601, Validation loss: 3.21875\nStep 64801, Training loss: 3.1517186164855957, Elapsed Time: 50.86 seconds\nStep 64801, Validation loss: 3.203125\nStep 65001, Training loss: 3.147656202316284, Elapsed Time: 50.71 seconds\nStep 65001, Validation loss: 3.21875\nStep 65201, Training loss: 3.154296875, Elapsed Time: 50.88 seconds\nStep 65201, Validation loss: 3.171875\nStep 65401, Training loss: 3.15234375, Elapsed Time: 50.73 seconds\nStep 65401, Validation loss: 3.1875\nStep 65601, Training loss: 3.147265672683716, Elapsed Time: 51.14 seconds\nStep 65601, Validation loss: 3.1875\nStep 65801, Training loss: 3.150156259536743, Elapsed Time: 50.87 seconds\nStep 65801, Validation loss: 3.171875\nStep 66001, Training loss: 3.1499218940734863, Elapsed Time: 50.72 seconds\nStep 66001, Validation loss: 3.1875\nStep 66201, Training loss: 3.151249885559082, Elapsed Time: 50.87 seconds\nStep 66201, Validation loss: 3.125\nStep 66401, Training loss: 3.1424999237060547, Elapsed Time: 50.85 seconds\nStep 66401, Validation loss: 3.203125\nStep 66601, Training loss: 3.15093731880188, Elapsed Time: 50.74 seconds\nStep 66601, Validation loss: 3.21875\nStep 66801, Training loss: 3.1410155296325684, Elapsed Time: 50.87 seconds\nStep 66801, Validation loss: 3.1875\nStep 67001, Training loss: 3.144296884536743, Elapsed Time: 50.72 seconds\nStep 67001, Validation loss: 3.1875\nStep 67201, Training loss: 3.1454687118530273, Elapsed Time: 50.87 seconds\nStep 67201, Validation loss: 3.140625\nStep 67401, Training loss: 3.142890453338623, Elapsed Time: 50.86 seconds\nStep 67401, Validation loss: 3.203125\nStep 67601, Training loss: 3.1460936069488525, Elapsed Time: 50.95 seconds\nStep 67601, Validation loss: 3.09375\nStep 67801, Training loss: 3.142656087875366, Elapsed Time: 50.72 seconds\nStep 67801, Validation loss: 3.09375\nStep 68001, Training loss: 3.136953115463257, Elapsed Time: 50.72 seconds\nStep 68001, Validation loss: 3.140625\nStep 68201, Training loss: 3.134999990463257, Elapsed Time: 50.87 seconds\nStep 68201, Validation loss: 3.140625\nStep 68401, Training loss: 3.1321873664855957, Elapsed Time: 50.87 seconds\nStep 68401, Validation loss: 3.125\nStep 68601, Training loss: 3.133984327316284, Elapsed Time: 51.14 seconds\nStep 68601, Validation loss: 3.171875\nStep 68801, Training loss: 3.1390624046325684, Elapsed Time: 50.85 seconds\nStep 68801, Validation loss: 3.125\nStep 69001, Training loss: 3.1354687213897705, Elapsed Time: 50.86 seconds\nStep 69001, Validation loss: 3.140625\nStep 69201, Training loss: 3.136796712875366, Elapsed Time: 50.85 seconds\nStep 69201, Validation loss: 3.125\nStep 69401, Training loss: 3.133593797683716, Elapsed Time: 50.86 seconds\nStep 69401, Validation loss: 3.125\nStep 69601, Training loss: 3.135859251022339, Elapsed Time: 51.11 seconds\nStep 69601, Validation loss: 3.125\nStep 69801, Training loss: 3.137343645095825, Elapsed Time: 50.72 seconds\nStep 69801, Validation loss: 3.09375\nStep 70001, Training loss: 3.136171817779541, Elapsed Time: 50.86 seconds\nStep 70001, Validation loss: 3.109375\nStep 70201, Training loss: 3.1317968368530273, Elapsed Time: 50.86 seconds\nStep 70201, Validation loss: 3.21875\nStep 70401, Training loss: 3.130937337875366, Elapsed Time: 50.87 seconds\nStep 70401, Validation loss: 3.1875\nStep 70601, Training loss: 3.127031087875366, Elapsed Time: 50.85 seconds\nStep 70601, Validation loss: 3.140625\nStep 70801, Training loss: 3.1352343559265137, Elapsed Time: 50.71 seconds\nStep 70801, Validation loss: 3.125\nStep 71001, Training loss: 3.1246092319488525, Elapsed Time: 50.72 seconds\nStep 71001, Validation loss: 3.078125\nStep 71201, Training loss: 3.1268749237060547, Elapsed Time: 50.72 seconds\nStep 71201, Validation loss: 3.1875\nStep 71401, Training loss: 3.1245312690734863, Elapsed Time: 50.72 seconds\nStep 71401, Validation loss: 3.125\nStep 71601, Training loss: 3.1287498474121094, Elapsed Time: 51.09 seconds\nStep 71601, Validation loss: 3.09375\nStep 71801, Training loss: 3.118828058242798, Elapsed Time: 50.72 seconds\nStep 71801, Validation loss: 3.125\nStep 72001, Training loss: 3.1215624809265137, Elapsed Time: 50.87 seconds\nStep 72001, Validation loss: 3.203125\nStep 72201, Training loss: 3.120781183242798, Elapsed Time: 50.71 seconds\nStep 72201, Validation loss: 3.125\nStep 72401, Training loss: 3.121718645095825, Elapsed Time: 50.86 seconds\nStep 72401, Validation loss: 3.140625\nStep 72601, Training loss: 3.1264843940734863, Elapsed Time: 51.11 seconds\nStep 72601, Validation loss: 3.09375\nStep 72801, Training loss: 3.1224217414855957, Elapsed Time: 50.86 seconds\nStep 72801, Validation loss: 3.09375\nStep 73001, Training loss: 3.114062547683716, Elapsed Time: 50.72 seconds\nStep 73001, Validation loss: 3.109375\nStep 73201, Training loss: 3.12164044380188, Elapsed Time: 50.72 seconds\nStep 73201, Validation loss: 3.109375\nStep 73401, Training loss: 3.1207029819488525, Elapsed Time: 50.87 seconds\nStep 73401, Validation loss: 3.140625\nStep 73601, Training loss: 3.1159374713897705, Elapsed Time: 51.27 seconds\nStep 73601, Validation loss: 3.109375\nStep 73801, Training loss: 3.1092185974121094, Elapsed Time: 50.86 seconds\nStep 73801, Validation loss: 3.109375\nStep 74001, Training loss: 3.1182031631469727, Elapsed Time: 50.72 seconds\nStep 74001, Validation loss: 3.09375\nStep 74201, Training loss: 3.11578106880188, Elapsed Time: 50.72 seconds\nStep 74201, Validation loss: 3.125\nStep 74401, Training loss: 3.116328001022339, Elapsed Time: 50.72 seconds\nStep 74401, Validation loss: 3.140625\nStep 74601, Training loss: 3.117968797683716, Elapsed Time: 50.89 seconds\nStep 74601, Validation loss: 3.171875\nStep 74801, Training loss: 3.116093635559082, Elapsed Time: 50.86 seconds\nStep 74801, Validation loss: 3.125\nStep 75001, Training loss: 3.1095311641693115, Elapsed Time: 50.86 seconds\nStep 75001, Validation loss: 3.09375\nStep 75201, Training loss: 3.11773419380188, Elapsed Time: 50.87 seconds\nStep 75201, Validation loss: 3.1875\nStep 75401, Training loss: 3.117421865463257, Elapsed Time: 50.87 seconds\nStep 75401, Validation loss: 3.109375\nStep 75601, Training loss: 3.1151561737060547, Elapsed Time: 50.92 seconds\nStep 75601, Validation loss: 3.15625\nStep 75801, Training loss: 3.111328125, Elapsed Time: 50.86 seconds\nStep 75801, Validation loss: 3.15625\nStep 76001, Training loss: 3.10992169380188, Elapsed Time: 50.86 seconds\nStep 76001, Validation loss: 3.109375\nStep 76201, Training loss: 3.1017186641693115, Elapsed Time: 50.72 seconds\nStep 76201, Validation loss: 3.125\nStep 76401, Training loss: 3.113906145095825, Elapsed Time: 50.86 seconds\nStep 76401, Validation loss: 3.1875\nStep 76601, Training loss: 3.108281135559082, Elapsed Time: 51.16 seconds\nStep 76601, Validation loss: 3.09375\nStep 76801, Training loss: 3.110546827316284, Elapsed Time: 50.72 seconds\nStep 76801, Validation loss: 3.046875\nStep 77001, Training loss: 3.1089062690734863, Elapsed Time: 50.86 seconds\nStep 77001, Validation loss: 3.171875\nStep 77201, Training loss: 3.1109373569488525, Elapsed Time: 50.72 seconds\nStep 77201, Validation loss: 3.171875\nStep 77401, Training loss: 3.106093645095825, Elapsed Time: 50.72 seconds\nStep 77401, Validation loss: 3.1875\nStep 77601, Training loss: 3.107499837875366, Elapsed Time: 51.07 seconds\nStep 77601, Validation loss: 3.171875\nStep 77801, Training loss: 3.106874942779541, Elapsed Time: 50.85 seconds\nStep 77801, Validation loss: 3.109375\nStep 78001, Training loss: 3.105703115463257, Elapsed Time: 50.72 seconds\nStep 78001, Validation loss: 3.140625\nStep 78201, Training loss: 3.105546712875366, Elapsed Time: 50.86 seconds\nStep 78201, Validation loss: 3.171875\nStep 78401, Training loss: 3.107421875, Elapsed Time: 50.86 seconds\nStep 78401, Validation loss: 3.171875\nStep 78601, Training loss: 3.108515501022339, Elapsed Time: 50.72 seconds\nStep 78601, Validation loss: 3.125\nStep 78801, Training loss: 3.104609251022339, Elapsed Time: 50.72 seconds\nStep 78801, Validation loss: 3.09375\nStep 79001, Training loss: 3.097109317779541, Elapsed Time: 50.72 seconds\nStep 79001, Validation loss: 3.140625\nStep 79201, Training loss: 3.1033592224121094, Elapsed Time: 50.85 seconds\nStep 79201, Validation loss: 3.140625\nStep 79401, Training loss: 3.096796751022339, Elapsed Time: 50.85 seconds\nStep 79401, Validation loss: 3.109375\nStep 79601, Training loss: 3.094374895095825, Elapsed Time: 50.83 seconds\nStep 79601, Validation loss: 3.0625\nStep 79801, Training loss: 3.1028904914855957, Elapsed Time: 50.72 seconds\nStep 79801, Validation loss: 3.078125\nStep 80001, Training loss: 3.1028904914855957, Elapsed Time: 50.88 seconds\nStep 80001, Validation loss: 3.109375\nStep 80201, Training loss: 3.103515625, Elapsed Time: 50.72 seconds\nStep 80201, Validation loss: 3.125\nStep 80401, Training loss: 3.0947656631469727, Elapsed Time: 50.86 seconds\nStep 80401, Validation loss: 3.078125\nStep 80601, Training loss: 3.0999999046325684, Elapsed Time: 51.14 seconds\nStep 80601, Validation loss: 3.078125\nStep 80801, Training loss: 3.102421760559082, Elapsed Time: 50.85 seconds\nStep 80801, Validation loss: 3.0625\nStep 81001, Training loss: 3.10210919380188, Elapsed Time: 50.72 seconds\nStep 81001, Validation loss: 3.140625\nStep 81201, Training loss: 3.099843740463257, Elapsed Time: 50.87 seconds\nStep 81201, Validation loss: 3.125\nStep 81401, Training loss: 3.095703125, Elapsed Time: 50.72 seconds\nStep 81401, Validation loss: 3.046875\nStep 81601, Training loss: 3.0916404724121094, Elapsed Time: 51.06 seconds\nStep 81601, Validation loss: 3.125\nStep 81801, Training loss: 3.0978124141693115, Elapsed Time: 50.85 seconds\nStep 81801, Validation loss: 3.078125\nStep 82001, Training loss: 3.10015606880188, Elapsed Time: 50.72 seconds\nStep 82001, Validation loss: 3.140625\nStep 82201, Training loss: 3.0970311164855957, Elapsed Time: 50.87 seconds\nStep 82201, Validation loss: 3.109375\nStep 82401, Training loss: 3.089296817779541, Elapsed Time: 50.85 seconds\nStep 82401, Validation loss: 3.140625\nStep 82601, Training loss: 3.098281145095825, Elapsed Time: 50.88 seconds\nStep 82601, Validation loss: 3.0625\nStep 82801, Training loss: 3.0966405868530273, Elapsed Time: 50.72 seconds\nStep 82801, Validation loss: 3.109375\nStep 83001, Training loss: 3.0964062213897705, Elapsed Time: 50.72 seconds\nStep 83001, Validation loss: 3.0\nStep 83201, Training loss: 3.094531297683716, Elapsed Time: 50.85 seconds\nStep 83201, Validation loss: 3.15625\nStep 83401, Training loss: 3.0882811546325684, Elapsed Time: 50.88 seconds\nStep 83401, Validation loss: 3.15625\nStep 83601, Training loss: 3.0958592891693115, Elapsed Time: 50.99 seconds\nStep 83601, Validation loss: 3.171875\nStep 83801, Training loss: 3.095390558242798, Elapsed Time: 50.72 seconds\nStep 83801, Validation loss: 3.0625\nStep 84001, Training loss: 3.0942187309265137, Elapsed Time: 50.72 seconds\nStep 84001, Validation loss: 3.125\nStep 84201, Training loss: 3.097890615463257, Elapsed Time: 50.87 seconds\nStep 84201, Validation loss: 3.03125\nStep 84401, Training loss: 3.097421884536743, Elapsed Time: 50.85 seconds\nStep 84401, Validation loss: 3.09375\nStep 84601, Training loss: 3.0950000286102295, Elapsed Time: 51.04 seconds\nStep 84601, Validation loss: 3.15625\nStep 84801, Training loss: 3.094843626022339, Elapsed Time: 50.86 seconds\nStep 84801, Validation loss: 3.171875\nStep 85001, Training loss: 3.092421770095825, Elapsed Time: 50.72 seconds\nStep 85001, Validation loss: 2.984375\nStep 85201, Training loss: 3.093437433242798, Elapsed Time: 50.86 seconds\nStep 85201, Validation loss: 3.03125\nStep 85401, Training loss: 3.0952343940734863, Elapsed Time: 50.73 seconds\nStep 85401, Validation loss: 3.078125\nStep 85601, Training loss: 3.092578172683716, Elapsed Time: 51.21 seconds\nStep 85601, Validation loss: 3.0625\nStep 85801, Training loss: 3.089921712875366, Elapsed Time: 50.86 seconds\nStep 85801, Validation loss: 3.109375\nStep 86001, Training loss: 3.0930469036102295, Elapsed Time: 50.86 seconds\nStep 86001, Validation loss: 3.0625\nStep 86201, Training loss: 3.097890615463257, Elapsed Time: 50.85 seconds\nStep 86201, Validation loss: 3.125\nStep 86401, Training loss: 3.0953123569488525, Elapsed Time: 50.72 seconds\nStep 86401, Validation loss: 3.0625\nStep 86601, Training loss: 3.0911717414855957, Elapsed Time: 50.90 seconds\nStep 86601, Validation loss: 3.109375\nStep 86801, Training loss: 3.096874952316284, Elapsed Time: 50.86 seconds\nStep 86801, Validation loss: 3.09375\nStep 87001, Training loss: 3.0896873474121094, Elapsed Time: 50.73 seconds\nStep 87001, Validation loss: 3.078125\nStep 87201, Training loss: 3.0942187309265137, Elapsed Time: 50.87 seconds\nStep 87201, Validation loss: 3.078125\nStep 87401, Training loss: 3.0933592319488525, Elapsed Time: 50.87 seconds\nStep 87401, Validation loss: 3.09375\nStep 87601, Training loss: 3.094843626022339, Elapsed Time: 51.00 seconds\nStep 87601, Validation loss: 3.09375\nStep 87801, Training loss: 3.09624981880188, Elapsed Time: 50.72 seconds\nStep 87801, Validation loss: 3.109375\nStep 88001, Training loss: 3.0956249237060547, Elapsed Time: 50.72 seconds\nStep 88001, Validation loss: 3.0625\nStep 88201, Training loss: 3.0931248664855957, Elapsed Time: 50.86 seconds\nStep 88201, Validation loss: 3.09375\nStep 88401, Training loss: 3.0916404724121094, Elapsed Time: 50.86 seconds\nStep 88401, Validation loss: 3.15625\nStep 88601, Training loss: 3.09375, Elapsed Time: 51.05 seconds\nStep 88601, Validation loss: 3.109375\nStep 88801, Training loss: 3.094921827316284, Elapsed Time: 50.86 seconds\nStep 88801, Validation loss: 3.0625\nStep 89001, Training loss: 3.0964062213897705, Elapsed Time: 50.72 seconds\nStep 89001, Validation loss: 3.109375\nStep 89201, Training loss: 3.0917186737060547, Elapsed Time: 50.85 seconds\nStep 89201, Validation loss: 3.078125\nStep 89401, Training loss: 3.0896873474121094, Elapsed Time: 50.73 seconds\nStep 89401, Validation loss: 3.109375\nStep 89601, Training loss: 3.0931248664855957, Elapsed Time: 51.16 seconds\nStep 89601, Validation loss: 3.109375\nStep 89801, Training loss: 3.098437547683716, Elapsed Time: 50.86 seconds\nStep 89801, Validation loss: 3.125\nStep 90001, Training loss: 3.0919530391693115, Elapsed Time: 50.72 seconds\nStep 90001, Validation loss: 3.109375\nFinal generated text:\nOnce upon a time, I was a child, and I was a young adult. I was a child, and I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult. I was a young adult",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:24.025088Z",
          "iopub.execute_input": "2025-07-02T15:54:24.025336Z",
          "iopub.status.idle": "2025-07-02T15:54:25.041381Z",
          "shell.execute_reply.started": "2025-07-02T15:54:24.025313Z",
          "shell.execute_reply": "2025-07-02T15:54:25.037615Z"
        },
        "outputId": "05e1011e-b5f0-466d-cfeb-bae4dc6ba523"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 640x480 with 1 Axes>",
            "image/png": "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"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results on Kaggle TPU:\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.to_pure_dict(nnx.state(model))\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:25.045336Z",
          "iopub.execute_input": "2025-07-02T15:54:25.045819Z",
          "iopub.status.idle": "2025-07-02T15:54:34.333377Z",
          "shell.execute_reply.started": "2025-07-02T15:54:25.045776Z",
          "shell.execute_reply": "2025-07-02T15:54:34.328497Z"
        },
        "outputId": "50be569c-6833-4577-fe15-3cd7e206b609"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "WARNING:absl:[process=0][thread=MainThread] Skipped cross-host ArrayMetadata validation because only one process is found: process_index=0.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.to_pure_dict(nnx.state(model))\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:34.337614Z",
          "iopub.execute_input": "2025-07-02T15:54:34.337862Z",
          "iopub.status.idle": "2025-07-02T15:54:52.927819Z",
          "shell.execute_reply.started": "2025-07-02T15:54:34.337839Z",
          "shell.execute_reply": "2025-07-02T15:54:52.922465Z"
        },
        "outputId": "1e579040-7269-4995-efb4-01f30da1af9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/site-packages/orbax/checkpoint/_src/serialization/type_handlers.py:1251: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Once upon a time, a person had a sense of the world and of its meaning. It was the same with the people. The people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way,Restored model generated text:\nOnce upon a time, a person had a sense of the world and of its meaning. It was the same with the people. The people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way, the people had a sense of the world and of its meaning.\n\nIn the same way,\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:52.930495Z",
          "iopub.execute_input": "2025-07-02T15:54:52.930734Z",
          "iopub.status.idle": "2025-07-02T15:54:52.946044Z",
          "shell.execute_reply.started": "2025-07-02T15:54:52.930711Z",
          "shell.execute_reply": "2025-07-02T15:54:52.939728Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}