This repository host LLM tutorials based on JAX.
* [01.miniGPT](./01.miniGPT/): build a miniGPT model from scratch and pretrain it on the TinyStories dataset
* [02.GPT2 pretraning](./02.GPT2-pretraining/): pretrain 124M and 354M GPT2 on the OpenWebText dataset (inspired by [nanoGPT](https://github.com/karpathy/nanoGPT))
* [03.GPT2 instruction tuning](./03.GPT2-instruct-tuning/): instruction tune the 124M pretrained GPT2 from above and from Hugging Face
* [04.GPT2_LoRA](./04.GPT2-LoRA): use LoRA to instruct tune the 124M pretrained GPT2
* [05.GPT2 DPO](./05.GPT2-DPO/GPT2-DPO.ipynb): DPO GPT2
* [06.Loading the Llama 3.2 1B Instruct model from Hugging Face](./04.Loading-model-from-HF/): load an existing model from HF and run inference
