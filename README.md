This repository host LLM tutorials based on JAX.
* [01.miniGPT](./01.miniGPT/): build a miniGPT model from scratch and pretrain it on the TinyStories dataset
* [02.GPT2 pretraning](./02.GPT2-pretraining/): pretrain 124M and 354M GPT2 on the OpenWebText dataset (inspired by [nanoGPT](https://github.com/karpathy/nanoGPT))
* [03.GPT2 instruction tuning](./03.GPT2-instruct-tuning/): instruction tune the 124M pretrained GPT2 from above
* [04.Loading the Llama 3.2 1B Instruct model from Hugging Face](./04.Loading-model-from-HF/): load an existing model from HF and run inference
* DPO
* RLVR
