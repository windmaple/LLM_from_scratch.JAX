This repository host LLM tutorials based on JAX.
* [miniGPT](./miniGPT): build a miniGPT model from scratch and pretrain it on the TinyStories dataset
* [GPT2 pretraning](./GPT2-pretraining): pretrain 124M and 354M GPT2 on the OpenWebText dataset (inspired by nanoGPT)
* [GPT2 instruction tuning](./GPT2-instruct-tuning/): instruction tune the 124M pretrained GPT2 from above
* [Loading the Llama 3.2 1B Instruct model from Hugging Face](./Loading-model-from-HF): load an existing model from HF and run inference
* LLama 3.2 1B DPO
* LLama 3.2 1B RLVR
