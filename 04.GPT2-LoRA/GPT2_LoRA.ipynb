{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "# GPT2 instruction tuning\n",
        "\n",
        "This notebook demonstrates how to finetune a pretrained GPT2(124M) model to follow user instructions. We are going to load the pretrained GPT2 model weights from Hugging Face and then instruct finetune the model on TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD3bo9FxhrTE"
      },
      "source": [
        "## Determine platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.403273Z",
          "iopub.status.busy": "2025-02-21T04:30:18.403068Z",
          "iopub.status.idle": "2025-02-21T04:30:18.413321Z",
          "shell.execute_reply": "2025-02-21T04:30:18.412689Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.403251Z"
        },
        "id": "7XcEXnSbhhKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "ddde99d3-2aee-4872-b981-abf11914bbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m450.6/456.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.0/456.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/473.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.3/473.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m152.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/319.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.2/135.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2025.4.9 requires jax==0.5.3, but you have jax 0.6.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.7/356.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jax-ai-stack[grain]\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cWxBvz6bZDd"
      },
      "source": [
        "Confirm we have TPUs set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z"
        },
        "id": "uZUaKdi5bSEN",
        "outputId": "8ef83482-0830-4209-dc12-ea544b70d7aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z"
        },
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb\n",
        "from huggingface_hub import snapshot_download\n",
        "from safetensors import safe_open\n",
        "from pathlib import Path\n",
        "from flax.nnx.nn.lora import LoRAParam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z"
        },
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z"
        },
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z"
        },
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # Only supports GPT2\n",
        "num_transformer_blocks = 12\n",
        "seqlen = 1024\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "feed_forward_dim = 4 * embed_dim\n",
        "if platform == \"Colab\":\n",
        "    batch_size = 24 # TPU v2\n",
        "else:\n",
        "    batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "lora_rank = 8\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "sampling_temp = 2\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZuT5uOUHqk2"
      },
      "source": [
        "We are going to load the weights from Hugging Face, which has a different tensor layout from what we used to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "M_rOqui1HuQI"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "\n",
        "class CustomMHA(nnx.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate, layer_idx, weights, rngs):\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // self.num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        kernel_init = nnx.with_partitioning(\n",
        "            nnx.initializers.xavier_uniform(), (P(None, \"model\"),)\n",
        "        )\n",
        "\n",
        "        self.query = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.key = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.value = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.out = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "\n",
        "        qkv_kernel = weights[f\"h.{layer_idx}.attn.c_attn.weight\"]\n",
        "        q_kernel, k_kernel, v_kernel = jnp.split(qkv_kernel, 3, axis=-1)\n",
        "        self.query.kernel.value = q_kernel\n",
        "        self.key.kernel.value = k_kernel\n",
        "        self.value.kernel.value = v_kernel\n",
        "\n",
        "        qkv_bias = weights[f\"h.{layer_idx}.attn.c_attn.bias\"]\n",
        "        q_b, k_b, v_b = jnp.split(qkv_bias, 3, axis=-1)\n",
        "\n",
        "        self.q_bias = nnx.Param(q_b, sharding=P(\"model\"))\n",
        "        self.k_bias = nnx.Param(k_b, sharding=P(\"model\"))\n",
        "        self.v_bias = nnx.Param(v_b, sharding=P(\"model\"))\n",
        "\n",
        "        self.out.kernel.value = weights[f\"h.{layer_idx}.attn.c_proj.weight\"]\n",
        "        self.out_bias = nnx.Param(\n",
        "            weights[f\"h.{layer_idx}.attn.c_proj.bias\"], sharding=P(\"model\")\n",
        "        )\n",
        "\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(self, x, mask, padding_mask=None, training: bool = False):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.query(x) + self.q_bias\n",
        "        k = self.key(x) + self.k_bias\n",
        "        v = self.value(x) + self.v_bias\n",
        "\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "\n",
        "        attn_weights = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(\n",
        "            self.head_dim\n",
        "        )\n",
        "\n",
        "        combined_mask = mask\n",
        "        if padding_mask is not None:\n",
        "            combined_mask = jnp.logical_and(mask, padding_mask)\n",
        "\n",
        "        if combined_mask is not None:\n",
        "            attn_weights = jnp.where(combined_mask, attn_weights, -jnp.inf)\n",
        "\n",
        "        attn_weights = nnx.softmax(attn_weights, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, deterministic=not training)\n",
        "\n",
        "        attn_output = jnp.matmul(attn_weights, v)\n",
        "        attn_output = attn_output.transpose((0, 2, 1, 3)).reshape(\n",
        "            (batch_size, seq_len, self.embed_dim)\n",
        "        )\n",
        "\n",
        "        output = self.out(attn_output) + self.out_bias\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "Now define the model architecture, which is the same as in our previous pretraining notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z"
        },
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        dropout_rate: float,\n",
        "        rngs: nnx.Rngs,\n",
        "        layer_idx: int,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.layer_norm1 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm1.scale.value = weights[f\"h.{layer_idx}.ln_1.weight\"]\n",
        "        self.layer_norm1.bias.value = weights[f\"h.{layer_idx}.ln_1.bias\"]\n",
        "        self.mha = CustomMHA(\n",
        "            embed_dim, num_heads, dropout_rate, layer_idx, weights, rngs\n",
        "        )\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm2.scale.value = weights[f\"h.{layer_idx}.ln_2.weight\"]\n",
        "        self.layer_norm2.bias.value = weights[f\"h.{layer_idx}.ln_2.bias\"]\n",
        "        self.linear1 = nnx.LoRALinear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=ff_dim,\n",
        "            kernel_init=nnx.with_partitioning(\n",
        "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.linear1.kernel.value = weights[f\"h.{layer_idx}.mlp.c_fc.weight\"]\n",
        "        self.linear1.bias.value = weights[f\"h.{layer_idx}.mlp.c_fc.bias\"]\n",
        "        self.linear2 = nnx.LoRALinear(\n",
        "            in_features=ff_dim,\n",
        "            out_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(\n",
        "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.linear2.kernel.value = weights[f\"h.{layer_idx}.mlp.c_proj.weight\"]\n",
        "        self.linear2.bias.value = weights[f\"h.{layer_idx}.mlp.c_proj.bias\"]\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, padding_mask=None, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            padding_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seqlen: int,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        rngs: nnx.Rngs,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.token_emb = nnx.Embed(\n",
        "            num_embeddings=vocab_size,\n",
        "            features=embed_dim,\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.pos_emb = nnx.Embed(\n",
        "            num_embeddings=seqlen,\n",
        "            features=embed_dim,\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.token_emb.embedding.value = weights[\"wte.weight\"]\n",
        "        self.pos_emb.embedding.value = weights[\"wpe.weight\"]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seqlen: int,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        rate: float,\n",
        "        feed_forward_dim: int,\n",
        "        num_transformer_blocks: int,\n",
        "        rngs: nnx.Rngs,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "            seqlen, vocab_size, embed_dim, rngs=rngs, weights=weights\n",
        "        )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(\n",
        "                embed_dim,\n",
        "                num_heads,\n",
        "                feed_forward_dim,\n",
        "                dropout_rate,\n",
        "                rngs=rngs,\n",
        "                layer_idx=i,\n",
        "                weights=weights,\n",
        "            )\n",
        "            for i in range(num_transformer_blocks)\n",
        "        ]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm.scale.value = weights[\"ln_f.weight\"]\n",
        "        self.layer_norm.bias.value = weights[\"ln_f.bias\"]\n",
        "\n",
        "    def __call__(self, inputs, padding_mask=None, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, padding_mask=padding_mask, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits, key):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits / sampling_temp)\n",
        "        return jax.random.choice(key, indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, params, static_def, padded_tokens, length, key):\n",
        "        padding_mask = jnp.arange(seqlen) < length\n",
        "        padding_mask = padding_mask.reshape(1, 1, 1, seqlen)\n",
        "\n",
        "        model = nnx.merge(params, static_def)\n",
        "        logits = model(padded_tokens, padding_mask=padding_mask, training=False)\n",
        "        last_token_logits = logits[:, length - 1, :]\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        next_token = self.sample_from(\n",
        "            jnp.squeeze(last_token_logits), subkey\n",
        "        )  # Pass subkey here\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        key = jax.random.PRNGKey(int(time.time()))\n",
        "\n",
        "        params, static_def = nnx.split(self)\n",
        "\n",
        "        tokens = jnp.array(start_tokens, dtype=jnp.int32)[None, :]\n",
        "        end_token = tokenizer.encode(\n",
        "            \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
        "        )[0]\n",
        "\n",
        "        current_len = tokens.shape[1]\n",
        "        padded_tokens = jnp.pad(tokens, ((0, 0), (0, seqlen - current_len)))\n",
        "\n",
        "        print(tokenizer.decode(tokens[0]), end=\"\", flush=True)\n",
        "\n",
        "        for i in range(max_tokens):\n",
        "            key, subkey = jax.random.split(key)\n",
        "\n",
        "            next_token = self.generate_step(\n",
        "                params, static_def, padded_tokens, current_len, subkey\n",
        "            )\n",
        "\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "\n",
        "            print(tokenizer.decode([next_token.item()]), end=\"\", flush=True)\n",
        "\n",
        "            padded_tokens = padded_tokens.at[:, current_len].set(next_token.item())\n",
        "            current_len += 1\n",
        "\n",
        "        final_tokens = padded_tokens[0, :current_len]\n",
        "        return tokenizer.decode(final_tokens.tolist())\n",
        "\n",
        "\n",
        "def create_model(rngs, weights):\n",
        "    return GPT2(\n",
        "        seqlen,\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout_rate,\n",
        "        feed_forward_dim,\n",
        "        num_transformer_blocks,\n",
        "        rngs=rngs,\n",
        "        weights=weights,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHW5lQfIO7y"
      },
      "source": [
        "Although we previously pretrained a pretty good GPT2 model from scratch, it is still less capable than the OpenAI official model (this is probably because the OpenWebText dataset is less comprehensive). So we are going to load the official weights from Hugging Face now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "027782d948b2491698a39bfc8c7c3dd5",
            "024010e2c6ae49e984b7859d793a4631",
            "6a11963b34bd417f960a909de74dbe31",
            "b000b5ba99a6400eab18fd1a29abdb9e",
            "f2f8398568d542919d87b53054e3f6ad",
            "04503c4244f2484aaf6c3dafce272403",
            "d965af6e46fe4b7baab1c072fe81bf86",
            "7c6261c7d64340ff9110e02b7989055e",
            "2b09ee635ef847a48e7751e6f817aa61",
            "d270db900f874ff48a5114899fb89816",
            "72d809185f194e71962867bb5f4b178f"
          ]
        },
        "id": "_NpkjhXrH7n3",
        "outputId": "a5790319-3fcd-4212-8f58-886402cfbc0c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "027782d948b2491698a39bfc8c7c3dd5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_id = \"openai-community/gpt2\"\n",
        "if os.path.exists(\"/kaggle\"):\n",
        "    weights_base_dir = \"/kaggle/tmp\"\n",
        "elif os.path.exists(\"/content\"):\n",
        "    # Colab\n",
        "    weights_base_dir = \"/content\"\n",
        "else:\n",
        "    # Local machine\n",
        "    weights_base_dir = \".\"\n",
        "\n",
        "path_to_model_weights = os.path.join(weights_base_dir, model_id)\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=model_id, local_dir=path_to_model_weights, allow_patterns=\"*.safetensors\"\n",
        ")\n",
        "\n",
        "\n",
        "def load_safetensors():\n",
        "    weights = {}\n",
        "    safetensors_files = Path(path_to_model_weights).glob(\"*.safetensors\")\n",
        "\n",
        "    for file in safetensors_files:\n",
        "        with safe_open(file, framework=\"jax\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                weights[key] = f.get_tensor(key)\n",
        "    return weights\n",
        "\n",
        "\n",
        "weights = load_safetensors()\n",
        "model = create_model(rngs=nnx.Rngs(0), weights=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHVR53fxIJhL",
        "outputId": "da615653-c593-4784-9157-fff0036e7acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Initial generated text:\n",
            "Once uppon a time (say 10), i.i.upport is the first step and upports a time to a time. So i.o.vaport is the time. The last part of our code, if i.p. is an empty array, and is not called a second step (for example in C++11 we would have called upport, which returns upport to uap. But if we use upp, it's an empty array and we need to re-arrand the array in order to make it work), then the next part will call upp to return it and so we would have uptime. This method will be called to call our own callback method, uppon. uptime() , but if uppon is called to return a pointer, we would need a way to call the method that was passed in, i.i.upport. The callback is called to return upp. upport\n",
            "\n",
            "For a list and for loops"
          ]
        }
      ],
      "source": [
        "start_prompt = \"Once uppon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"***Initial generated text:\")\n",
        "generated_text = model.generate_text(seqlen // 5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBfT1dp5hMUm"
      },
      "source": [
        "Use Weights and Biases to track training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z"
        },
        "id": "IbhEtsganEWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "232ff5e5-5347-4d2a-ccf9-5af9f7c4f559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250722_103952-2hraf94g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-LoRA/runs/2hraf94g' target=\"_blank\">olive-dust-2</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/windmaple/GPT2-LoRA' target=\"_blank\">https://wandb.ai/windmaple/GPT2-LoRA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/windmaple/GPT2-LoRA/runs/2hraf94g' target=\"_blank\">https://wandb.ai/windmaple/GPT2-LoRA/runs/2hraf94g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-LoRA/runs/2hraf94g?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e1e48990ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-LoRA',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Instruct tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5GU7yPKSdtj"
      },
      "source": [
        "We are going to use the [Alpaca dataset](https://huggingface.co/datasets/tatsu-lab/alpaca) from Stanford."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0VcKINdFKmzQ"
      },
      "outputs": [],
      "source": [
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "@dataclass\n",
        "class TextDataset:\n",
        "    data_df: list\n",
        "    seqlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(\n",
        "            self.data_df.iloc[idx], allowed_special={\"<|endoftext|>\"}\n",
        "        )[:self.seqlen        ]\n",
        "        return encoding + [50256] * (self.seqlen - len(encoding))\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(alpaca_data, batch_size, seqlen):\n",
        "    alpaca_data_df = pd.DataFrame(alpaca_data)\n",
        "    dataset = TextDataset(alpaca_data_df[\"text\"], seqlen)\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=1,\n",
        "    )\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "    return dl\n",
        "\n",
        "\n",
        "alpaca_data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
        "text_dl = load_and_preprocess_data(alpaca_data, batch_size, seqlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VarkoU2TIn2"
      },
      "source": [
        "Define the loss and training step function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FfHW-ESTTOyf"
      },
      "outputs": [],
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=batch[1]\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(\n",
        "    lora_params, opt_state, graphdef, static_params, metrics: nnx.MultiMetric, batch\n",
        "):\n",
        "    grad_fn = nnx.value_and_grad(\n",
        "        lambda lp: loss_fn(nnx.merge(graphdef, lp, static_params), batch), has_aux=True\n",
        "    )\n",
        "    (loss, logits), grads = grad_fn(lora_params)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    updates, opt_state = tx.update(grads, opt_state, lora_params)\n",
        "    lora_params = optax.apply_updates(lora_params, updates)\n",
        "    return lora_params, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc9ySGQgTwaJ"
      },
      "source": [
        "Define optimizer and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "om0mZQkqTzP6"
      },
      "outputs": [],
      "source": [
        "graphdef, lora_params, static_params = nnx.split(model, LoRAParam, nnx.Param)\n",
        "\n",
        "schedule = optax.cosine_decay_schedule(\n",
        "    init_value=init_learning_rate, decay_steps=max_steps\n",
        ")\n",
        "tx = optax.chain(optax.adamw(learning_rate=schedule, weight_decay=weight_decay))\n",
        "opt_state = tx.init(lora_params)\n",
        "\n",
        "\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYCeS5BOT65i"
      },
      "source": [
        "Do a test run on our pretrained model to see how it reponds to instruction. Note how we use a template to format the prompt, which needs to be consistent with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8_OTAAZUYhF",
        "outputId": "e55c9dbf-c711-466d-951a-62f4e5c39877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Initial generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "How does the future for man look like:\n",
            "\n",
            "\n",
            "How does the future look like:\n",
            "\n",
            " and why would I ever like it if man doesn't get a human?\n",
            "\n",
            "I want a human in this world, so this is not my choice.\n",
            "\n",
            "I am going to do what he wants, if I'm not careful.\n",
            "\n",
            "\n",
            "What does this mean about human?\n",
            "\n",
            "\n",
            "I want to do what he's going to do, and I have to do what's right in this case?\n",
            "\n",
            "\n",
            "What does human think about this, and if it makes them happy, then it has something to do with my human. And I am the guy that does the things to get human to do these things for them?\n",
            "\n",
            "\n",
            "And I don't care. That's why he will do anything to help man. I don't care if a human lives and I'm happy.\n",
            "\n",
            "\n",
            "What do I do? That's my decision now, to make sure this human has something in his life that"
          ]
        }
      ],
      "source": [
        "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n{output}\"\n",
        "\n",
        "start_prompt = template.format(\n",
        "    instruction=\"What is the future for human?\",\n",
        "    input=\"\",\n",
        "    output=\"\",\n",
        ")\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"***Initial generated text:\")\n",
        "generated_text = model.generate_text(seqlen//5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUZamJKUpyg"
      },
      "source": [
        "As you can see, the pretrained model generates a bunch of garbage; clearly it does not know how to follow the instruction to generate an appropriate answer, which is not surprising given that we have not trained it to do so.\n",
        "\n",
        "Now let's do the instruction tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWWc2Eb_n52d",
        "outputId": "6c7e1324-f331-45e7-e9dc-571c312136d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Step 20, Loss: 2.0909180641174316, Elapsed Time: 38.90 seconds\n",
            "\n",
            "\n",
            "Step 40, Loss: 0.3573242127895355, Elapsed Time: 18.07 seconds\n",
            "\n",
            "\n",
            "Step 60, Loss: 0.32646486163139343, Elapsed Time: 3.31 seconds\n",
            "\n",
            "\n",
            "Step 80, Loss: 0.30937501788139343, Elapsed Time: 3.31 seconds\n",
            "\n",
            "\n",
            "Step 100, Loss: 0.294921875, Elapsed Time: 3.54 seconds\n",
            "\n",
            "\n",
            "Step 120, Loss: 0.26103517413139343, Elapsed Time: 3.31 seconds\n",
            "\n",
            "\n",
            "Step 140, Loss: 0.22988282144069672, Elapsed Time: 3.44 seconds\n",
            "\n",
            "\n",
            "Step 160, Loss: 0.21030274033546448, Elapsed Time: 3.37 seconds\n",
            "\n",
            "\n",
            "Step 180, Loss: 0.20200195908546448, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 200, Loss: 0.19423829019069672, Elapsed Time: 3.46 seconds\n",
            "\n",
            "\n",
            "Step 220, Loss: 0.19350586831569672, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 240, Loss: 0.1982421875, Elapsed Time: 3.40 seconds\n",
            "\n",
            "\n",
            "Step 260, Loss: 0.18901367485523224, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 280, Loss: 0.19008789956569672, Elapsed Time: 3.55 seconds\n",
            "\n",
            "\n",
            "Step 300, Loss: 0.18852539360523224, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 320, Loss: 0.18686524033546448, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 340, Loss: 0.19584961235523224, Elapsed Time: 3.48 seconds\n",
            "\n",
            "\n",
            "Step 360, Loss: 0.19965820014476776, Elapsed Time: 3.56 seconds\n",
            "\n",
            "\n",
            "Step 380, Loss: 0.18310546875, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 400, Loss: 0.19570313394069672, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 420, Loss: 0.17807617783546448, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 440, Loss: 0.19174805283546448, Elapsed Time: 3.46 seconds\n",
            "\n",
            "\n",
            "Step 460, Loss: 0.18496094644069672, Elapsed Time: 3.52 seconds\n",
            "\n",
            "\n",
            "Step 480, Loss: 0.17856445908546448, Elapsed Time: 3.52 seconds\n",
            "\n",
            "\n",
            "Step 500, Loss: 0.17827148735523224, Elapsed Time: 3.45 seconds\n",
            "\n",
            "\n",
            "Step 520, Loss: 0.19609375298023224, Elapsed Time: 3.36 seconds\n",
            "\n",
            "\n",
            "Step 540, Loss: 0.172607421875, Elapsed Time: 3.46 seconds\n",
            "\n",
            "\n",
            "Step 560, Loss: 0.19223633408546448, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 580, Loss: 0.18740235269069672, Elapsed Time: 3.36 seconds\n",
            "\n",
            "\n",
            "Step 600, Loss: 0.18422852456569672, Elapsed Time: 3.45 seconds\n",
            "\n",
            "\n",
            "Step 620, Loss: 0.17885743081569672, Elapsed Time: 3.47 seconds\n",
            "\n",
            "\n",
            "Step 640, Loss: 0.18671874701976776, Elapsed Time: 3.46 seconds\n",
            "\n",
            "\n",
            "Step 660, Loss: 0.18623046576976776, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 680, Loss: 0.17670898139476776, Elapsed Time: 3.31 seconds\n",
            "\n",
            "\n",
            "Step 700, Loss: 0.182861328125, Elapsed Time: 3.40 seconds\n",
            "\n",
            "\n",
            "Step 720, Loss: 0.19174805283546448, Elapsed Time: 3.48 seconds\n",
            "\n",
            "\n",
            "Step 740, Loss: 0.185302734375, Elapsed Time: 3.29 seconds\n",
            "\n",
            "\n",
            "Step 760, Loss: 0.17055664956569672, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 780, Loss: 0.18339844048023224, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 800, Loss: 0.18349610269069672, Elapsed Time: 3.57 seconds\n",
            "\n",
            "\n",
            "Step 820, Loss: 0.18505859375, Elapsed Time: 3.49 seconds\n",
            "\n",
            "\n",
            "Step 840, Loss: 0.17685547471046448, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 860, Loss: 0.18017578125, Elapsed Time: 3.39 seconds\n",
            "\n",
            "\n",
            "Step 880, Loss: 0.18505859375, Elapsed Time: 3.63 seconds\n",
            "\n",
            "\n",
            "Step 900, Loss: 0.19013671576976776, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 920, Loss: 0.17880859971046448, Elapsed Time: 3.36 seconds\n",
            "\n",
            "\n",
            "Step 940, Loss: 0.18891601264476776, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 960, Loss: 0.18354491889476776, Elapsed Time: 3.44 seconds\n",
            "\n",
            "\n",
            "Step 980, Loss: 0.18295899033546448, Elapsed Time: 3.58 seconds\n",
            "\n",
            "\n",
            "Step 1000, Loss: 0.17578125, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 1020, Loss: 0.17524413764476776, Elapsed Time: 3.36 seconds\n",
            "\n",
            "\n",
            "Step 1040, Loss: 0.18022461235523224, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1060, Loss: 0.17148438096046448, Elapsed Time: 3.56 seconds\n",
            "\n",
            "\n",
            "Step 1080, Loss: 0.177734375, Elapsed Time: 3.49 seconds\n",
            "\n",
            "\n",
            "Step 1100, Loss: 0.18525390326976776, Elapsed Time: 3.49 seconds\n",
            "\n",
            "\n",
            "Step 1120, Loss: 0.182861328125, Elapsed Time: 3.55 seconds\n",
            "\n",
            "\n",
            "Step 1140, Loss: 0.17832031846046448, Elapsed Time: 3.47 seconds\n",
            "\n",
            "\n",
            "Step 1160, Loss: 0.16923828423023224, Elapsed Time: 3.48 seconds\n",
            "\n",
            "\n",
            "Step 1180, Loss: 0.17363281548023224, Elapsed Time: 3.51 seconds\n",
            "\n",
            "\n",
            "Step 1200, Loss: 0.17885743081569672, Elapsed Time: 3.37 seconds\n",
            "\n",
            "\n",
            "Step 1220, Loss: 0.177490234375, Elapsed Time: 3.47 seconds\n",
            "\n",
            "\n",
            "Step 1240, Loss: 0.18603515625, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1260, Loss: 0.1806640625, Elapsed Time: 3.68 seconds\n",
            "\n",
            "\n",
            "Step 1280, Loss: 0.16445313394069672, Elapsed Time: 3.59 seconds\n",
            "\n",
            "\n",
            "Step 1300, Loss: 0.17499999701976776, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1320, Loss: 0.16596679389476776, Elapsed Time: 3.53 seconds\n",
            "\n",
            "\n",
            "Step 1340, Loss: 0.16982422769069672, Elapsed Time: 3.85 seconds\n",
            "\n",
            "\n",
            "Step 1360, Loss: 0.166748046875, Elapsed Time: 3.42 seconds\n",
            "\n",
            "\n",
            "Step 1380, Loss: 0.18623046576976776, Elapsed Time: 3.59 seconds\n",
            "\n",
            "\n",
            "Step 1400, Loss: 0.1641845703125, Elapsed Time: 3.37 seconds\n",
            "\n",
            "\n",
            "Step 1420, Loss: 0.17973633110523224, Elapsed Time: 3.45 seconds\n",
            "\n",
            "\n",
            "Step 1440, Loss: 0.17148438096046448, Elapsed Time: 3.82 seconds\n",
            "\n",
            "\n",
            "Step 1460, Loss: 0.18930664658546448, Elapsed Time: 3.36 seconds\n",
            "\n",
            "\n",
            "Step 1480, Loss: 0.17634277045726776, Elapsed Time: 3.48 seconds\n",
            "\n",
            "\n",
            "Step 1500, Loss: 0.1873779296875, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 1520, Loss: 0.18754883110523224, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 1540, Loss: 0.18471680581569672, Elapsed Time: 3.64 seconds\n",
            "\n",
            "\n",
            "Step 1560, Loss: 0.17363281548023224, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 1580, Loss: 0.17119140923023224, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1600, Loss: 0.1767578125, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 1620, Loss: 0.18081055581569672, Elapsed Time: 3.43 seconds\n",
            "\n",
            "\n",
            "Step 1640, Loss: 0.17963866889476776, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 1660, Loss: 0.16928711533546448, Elapsed Time: 3.37 seconds\n",
            "\n",
            "\n",
            "Step 1680, Loss: 0.18134765326976776, Elapsed Time: 3.49 seconds\n",
            "\n",
            "\n",
            "Step 1700, Loss: 0.183837890625, Elapsed Time: 3.75 seconds\n",
            "\n",
            "\n",
            "Step 1720, Loss: 0.17387695610523224, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1740, Loss: 0.18291015923023224, Elapsed Time: 3.47 seconds\n",
            "\n",
            "\n",
            "Step 1760, Loss: 0.18415527045726776, Elapsed Time: 3.55 seconds\n",
            "\n",
            "\n",
            "Step 1780, Loss: 0.18515625596046448, Elapsed Time: 3.50 seconds\n",
            "\n",
            "\n",
            "Step 1800, Loss: 0.17685547471046448, Elapsed Time: 3.50 seconds\n",
            "\n",
            "\n",
            "Step 1820, Loss: 0.18210449814796448, Elapsed Time: 3.38 seconds\n",
            "\n",
            "\n",
            "Step 1840, Loss: 0.17543946206569672, Elapsed Time: 3.48 seconds\n",
            "\n",
            "\n",
            "Step 1860, Loss: 0.174560546875, Elapsed Time: 3.60 seconds\n",
            "\n",
            "\n",
            "Step 1880, Loss: 0.16787110269069672, Elapsed Time: 3.34 seconds\n",
            "\n",
            "\n",
            "Step 1900, Loss: 0.174560546875, Elapsed Time: 3.39 seconds\n",
            "\n",
            "\n",
            "Step 1920, Loss: 0.1722412109375, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 1940, Loss: 0.16872559487819672, Elapsed Time: 3.38 seconds\n",
            "\n",
            "\n",
            "Step 1960, Loss: 0.17070312798023224, Elapsed Time: 3.74 seconds\n",
            "\n",
            "\n",
            "Step 1980, Loss: 0.17304687201976776, Elapsed Time: 3.39 seconds\n",
            "\n",
            "\n",
            "Step 2000, Loss: 0.17412109673023224, Elapsed Time: 3.75 seconds\n",
            "\n",
            "\n",
            "Step 2020, Loss: 0.179931640625, Elapsed Time: 3.35 seconds\n",
            "\n",
            "\n",
            "Step 2040, Loss: 0.17451171576976776, Elapsed Time: 3.74 seconds\n",
            "\n",
            "\n",
            "Step 2060, Loss: 0.17709961533546448, Elapsed Time: 3.43 seconds\n",
            "\n",
            "\n",
            "Step 2080, Loss: 0.17861329019069672, Elapsed Time: 3.33 seconds\n",
            "\n",
            "\n",
            "Step 2100, Loss: 0.17070312798023224, Elapsed Time: 3.49 seconds\n",
            "\n",
            "\n",
            "Step 2120, Loss: 0.17578125, Elapsed Time: 3.62 seconds\n",
            "\n",
            "\n",
            "Step 2140, Loss: 0.18925781548023224, Elapsed Time: 3.32 seconds\n",
            "\n",
            "\n",
            "Step 2160, Loss: 0.16748046875, Elapsed Time: 3.36 seconds\n",
            "\n",
            "***Final generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Input:\n",
            "\n",
            "\n",
            "### Response:\n",
            "As the global economy expands and technology advances, human needs continue to require higher wages for all involved in the creation and maintenance of the human race. As human beings have been forced out from work due to technology advancements or lack of access for services and jobs, a greater reliance on technology for jobs is likely as society continues its rapid development. As technological advancements increase and new jobs and skills become needed in the workplace, people need to invest time, labor resources into their jobs, and resources to get them started. Additionally the human population of countries, regions & countries will be affected due to their increased economic mobility, increased population, population growth, population density and the increased demand for resources and services."
          ]
        }
      ],
      "source": [
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "for batch in text_dl:\n",
        "    if len(batch) % len(jax.devices()) != 0:\n",
        "        continue  # skip the remaining elements\n",
        "    input_batch = jnp.array(batch).T\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    lora_params, opt_state = train_step(\n",
        "        lora_params,\n",
        "        opt_state,\n",
        "        graphdef,\n",
        "        static_params,\n",
        "        metrics,\n",
        "        jax.device_put(\n",
        "            (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if (step + 1) % 20 == 0:\n",
        "        for metric, value in metrics.compute().items():\n",
        "            metrics_history[f\"train_{metric}\"].append(value)\n",
        "        metrics.reset()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "        )\n",
        "        # wandb.log(data={'train_loss': metrics_history['train_loss'][-1]}, step=step)\n",
        "        start_time = time.time()\n",
        "        # print(f\"\\n***Intermediate generated text:\")\n",
        "        intermediate_model = nnx.merge(graphdef, lora_params, static_params)\n",
        "        generated_text = intermediate_model.generate_text(seqlen // 5, start_tokens)\n",
        "    step += 1\n",
        "\n",
        "# Final text generation\n",
        "model = nnx.merge(graphdef, lora_params, static_params)\n",
        "print(f\"\\n***Final generated text:\")\n",
        "generated_text = model.generate_text(seqlen // 5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beji0Q63afpe"
      },
      "source": [
        "As you can see, at the end of the finetuning, the model is able to follow human instruction and generate a somewhat sensible answer. And the answer is actually better than what previously got from our own pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6uHcKfOvbcsx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "datasetId": 5848741,
          "sourceId": 10577797,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30920,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "027782d948b2491698a39bfc8c7c3dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_024010e2c6ae49e984b7859d793a4631",
              "IPY_MODEL_6a11963b34bd417f960a909de74dbe31",
              "IPY_MODEL_b000b5ba99a6400eab18fd1a29abdb9e"
            ],
            "layout": "IPY_MODEL_f2f8398568d542919d87b53054e3f6ad"
          }
        },
        "024010e2c6ae49e984b7859d793a4631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04503c4244f2484aaf6c3dafce272403",
            "placeholder": "​",
            "style": "IPY_MODEL_d965af6e46fe4b7baab1c072fe81bf86",
            "value": "Fetching 1 files: 100%"
          }
        },
        "6a11963b34bd417f960a909de74dbe31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6261c7d64340ff9110e02b7989055e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b09ee635ef847a48e7751e6f817aa61",
            "value": 1
          }
        },
        "b000b5ba99a6400eab18fd1a29abdb9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d270db900f874ff48a5114899fb89816",
            "placeholder": "​",
            "style": "IPY_MODEL_72d809185f194e71962867bb5f4b178f",
            "value": " 1/1 [00:00&lt;00:00, 114.61it/s]"
          }
        },
        "f2f8398568d542919d87b53054e3f6ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04503c4244f2484aaf6c3dafce272403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d965af6e46fe4b7baab1c072fe81bf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6261c7d64340ff9110e02b7989055e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b09ee635ef847a48e7751e6f817aa61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d270db900f874ff48a5114899fb89816": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d809185f194e71962867bb5f4b178f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}