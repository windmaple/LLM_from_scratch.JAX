{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvP1eNN_pExM"
      },
      "source": [
        "# GPT2 LoRA finetuning\n",
        "\n",
        "This notebook demonstrates how to LoRA finetune a pretrained GPT2(124M) model to follow user instructions. We are going to load the pretrained GPT2 model weights from Hugging Face and then instruct finetune the model on TPU with LoRA, using the [Databrick Dolly 15K dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k). If you are not familiar with LoRA, please go ahead and read the original [paper](https://arxiv.org/abs/2106.09685) first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD3bo9FxhrTE"
      },
      "source": [
        "## Determine platform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.403273Z",
          "iopub.status.busy": "2025-02-21T04:30:18.403068Z",
          "iopub.status.idle": "2025-02-21T04:30:18.413321Z",
          "shell.execute_reply": "2025-02-21T04:30:18.412689Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.403251Z"
        },
        "id": "7XcEXnSbhhKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if os.path.exists('/content/'):\n",
        "  platform = \"Colab\"\n",
        "elif os.path.exists('/kaggle/'):\n",
        "  platform = \"Kaggle\"\n",
        "else:\n",
        "  # Assume using Cloud TPU otherwise\n",
        "  platform = \"GCP\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:18.414249Z",
          "iopub.status.busy": "2025-02-21T04:30:18.414058Z",
          "iopub.status.idle": "2025-02-21T04:30:51.718249Z",
          "shell.execute_reply": "2025-02-21T04:30:51.716902Z",
          "shell.execute_reply.started": "2025-02-21T04:30:18.414229Z"
        },
        "id": "6zMsOIc7ouCO",
        "outputId": "444a483e-7cb7-4c10-c46b-3555d384949c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/456.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m450.6/456.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.0/456.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/473.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.3/473.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m164.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/319.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.2/319.2 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/406.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m406.3/406.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m927.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m869.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.5/136.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jax-ai-stack 2025.4.9 requires jax==0.5.3, but you have jax 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.7/356.7 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q jax-ai-stack[grain]\n",
        "if platform == \"Colab\": # temp workaround on Colab (https://github.com/jax-ml/jax-ai-stack/issues/149)\n",
        "  !pip install -Uq \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cWxBvz6bZDd"
      },
      "source": [
        "Confirm we have TPUs set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:51.719450Z",
          "iopub.status.busy": "2025-02-21T04:30:51.719166Z",
          "iopub.status.idle": "2025-02-21T04:30:59.572796Z",
          "shell.execute_reply": "2025-02-21T04:30:59.571540Z",
          "shell.execute_reply.started": "2025-02-21T04:30:51.719422Z"
        },
        "id": "uZUaKdi5bSEN",
        "outputId": "745240f7-70f7-42fb-9dec-0701fc1de5cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Take care of the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:30:59.573940Z",
          "iopub.status.busy": "2025-02-21T04:30:59.573640Z",
          "iopub.status.idle": "2025-02-21T04:31:01.600418Z",
          "shell.execute_reply": "2025-02-21T04:31:01.598633Z",
          "shell.execute_reply.started": "2025-02-21T04:30:59.573916Z"
        },
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb\n",
        "from huggingface_hub import snapshot_download\n",
        "from safetensors import safe_open\n",
        "from pathlib import Path\n",
        "from flax.nnx.nn.lora import LoRAParam\n",
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.601598Z",
          "iopub.status.busy": "2025-02-21T04:31:01.601360Z",
          "iopub.status.idle": "2025-02-21T04:31:01.605772Z",
          "shell.execute_reply": "2025-02-21T04:31:01.604615Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.601574Z"
        },
        "id": "xuMlCK3Q8WJD"
      },
      "outputs": [],
      "source": [
        "### Alternative data and model parallel\n",
        "# mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
        "\n",
        "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:31:01.606937Z",
          "iopub.status.busy": "2025-02-21T04:31:01.606708Z",
          "iopub.status.idle": "2025-02-21T04:31:04.402839Z",
          "shell.execute_reply": "2025-02-21T04:31:04.401628Z",
          "shell.execute_reply.started": "2025-02-21T04:31:01.606915Z"
        },
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.706850Z",
          "iopub.status.busy": "2025-02-21T04:32:00.706531Z",
          "iopub.status.idle": "2025-02-21T04:32:00.712524Z",
          "shell.execute_reply": "2025-02-21T04:32:00.711567Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.706823Z"
        },
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # Only supports GPT2\n",
        "num_transformer_blocks = 12\n",
        "seqlen = 1024\n",
        "embed_dim = 768\n",
        "num_heads = 12\n",
        "feed_forward_dim = 4 * embed_dim\n",
        "if platform == \"Colab\":\n",
        "    batch_size = 24 # TPU v2\n",
        "else:\n",
        "    batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "lora_rank = 8\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "sampling_temp = 2\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZuT5uOUHqk2"
      },
      "source": [
        "Let's define a custom multi-head attention class first. Since we are doing LoRA finetuning this time, we are going to replace all Linear layers with [nnx.LoRALinear layers](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/lora.html#flax.nnx.LoRALinear). Notice how the query, key, value and out projection layers are defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_rOqui1HuQI"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "\n",
        "class CustomMHA(nnx.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout_rate, layer_idx, weights, rngs):\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // self.num_heads\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        kernel_init = nnx.with_partitioning(\n",
        "            nnx.initializers.xavier_uniform(), (P(None, \"model\"),)\n",
        "        )\n",
        "\n",
        "        self.query = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.key = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.value = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.out = nnx.LoRALinear(\n",
        "            embed_dim,\n",
        "            embed_dim,\n",
        "            rngs=rngs,\n",
        "            use_bias=False,\n",
        "            kernel_init=kernel_init,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "\n",
        "        qkv_kernel = weights[f\"h.{layer_idx}.attn.c_attn.weight\"]\n",
        "        q_kernel, k_kernel, v_kernel = jnp.split(qkv_kernel, 3, axis=-1)\n",
        "        self.query.kernel.value = q_kernel\n",
        "        self.key.kernel.value = k_kernel\n",
        "        self.value.kernel.value = v_kernel\n",
        "\n",
        "        qkv_bias = weights[f\"h.{layer_idx}.attn.c_attn.bias\"]\n",
        "        q_b, k_b, v_b = jnp.split(qkv_bias, 3, axis=-1)\n",
        "\n",
        "        self.q_bias = nnx.Param(q_b, sharding=P(\"model\"))\n",
        "        self.k_bias = nnx.Param(k_b, sharding=P(\"model\"))\n",
        "        self.v_bias = nnx.Param(v_b, sharding=P(\"model\"))\n",
        "\n",
        "        self.out.kernel.value = weights[f\"h.{layer_idx}.attn.c_proj.weight\"]\n",
        "        self.out_bias = nnx.Param(\n",
        "            weights[f\"h.{layer_idx}.attn.c_proj.bias\"], sharding=P(\"model\")\n",
        "        )\n",
        "\n",
        "        self.dropout = nnx.Dropout(dropout_rate)\n",
        "\n",
        "    def __call__(self, x, mask, padding_mask=None, training: bool = False):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        q = self.query(x) + self.q_bias\n",
        "        k = self.key(x) + self.k_bias\n",
        "        v = self.value(x) + self.v_bias\n",
        "\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(\n",
        "            (0, 2, 1, 3)\n",
        "        )\n",
        "\n",
        "        attn_weights = jnp.matmul(q, k.transpose((0, 1, 3, 2))) / jnp.sqrt(\n",
        "            self.head_dim\n",
        "        )\n",
        "\n",
        "        combined_mask = mask\n",
        "        if padding_mask is not None:\n",
        "            combined_mask = jnp.logical_and(mask, padding_mask)\n",
        "\n",
        "        if combined_mask is not None:\n",
        "            attn_weights = jnp.where(combined_mask, attn_weights, -jnp.inf)\n",
        "\n",
        "        attn_weights = nnx.softmax(attn_weights, axis=-1)\n",
        "        attn_weights = self.dropout(attn_weights, deterministic=not training)\n",
        "\n",
        "        attn_output = jnp.matmul(attn_weights, v)\n",
        "        attn_output = attn_output.transpose((0, 2, 1, 3)).reshape(\n",
        "            (batch_size, seq_len, self.embed_dim)\n",
        "        )\n",
        "\n",
        "        output = self.out(attn_output) + self.out_bias\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "Now define the model architecture. Similarly, notice how the up and down projection layers are defined with the NNX LoRALinear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:00.771416Z",
          "iopub.status.busy": "2025-02-21T04:32:00.771150Z",
          "iopub.status.idle": "2025-02-21T04:32:00.792958Z",
          "shell.execute_reply": "2025-02-21T04:32:00.791974Z",
          "shell.execute_reply.started": "2025-02-21T04:32:00.771393Z"
        },
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        ff_dim: int,\n",
        "        dropout_rate: float,\n",
        "        rngs: nnx.Rngs,\n",
        "        layer_idx: int,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.layer_norm1 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm1.scale.value = weights[f\"h.{layer_idx}.ln_1.weight\"]\n",
        "        self.layer_norm1.bias.value = weights[f\"h.{layer_idx}.ln_1.bias\"]\n",
        "        self.mha = CustomMHA(\n",
        "            embed_dim, num_heads, dropout_rate, layer_idx, weights, rngs\n",
        "        )\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)\n",
        "        self.layer_norm2 = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm2.scale.value = weights[f\"h.{layer_idx}.ln_2.weight\"]\n",
        "        self.layer_norm2.bias.value = weights[f\"h.{layer_idx}.ln_2.bias\"]\n",
        "        self.linear1 = nnx.LoRALinear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=ff_dim,\n",
        "            kernel_init=nnx.with_partitioning(\n",
        "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.linear1.kernel.value = weights[f\"h.{layer_idx}.mlp.c_fc.weight\"]\n",
        "        self.linear1.bias.value = weights[f\"h.{layer_idx}.mlp.c_fc.bias\"]\n",
        "        self.linear2 = nnx.LoRALinear(\n",
        "            in_features=ff_dim,\n",
        "            out_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(\n",
        "                nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, \"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "            lora_rank=lora_rank,\n",
        "        )\n",
        "        self.linear2.kernel.value = weights[f\"h.{layer_idx}.mlp.c_proj.weight\"]\n",
        "        self.linear2.bias.value = weights[f\"h.{layer_idx}.mlp.c_proj.bias\"]\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, padding_mask=None, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            padding_mask=padding_mask,\n",
        "            training=training,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seqlen: int,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        rngs: nnx.Rngs,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.token_emb = nnx.Embed(\n",
        "            num_embeddings=vocab_size,\n",
        "            features=embed_dim,\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.pos_emb = nnx.Embed(\n",
        "            num_embeddings=seqlen,\n",
        "            features=embed_dim,\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.token_emb.embedding.value = weights[\"wte.weight\"]\n",
        "        self.pos_emb.embedding.value = weights[\"wpe.weight\"]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding + position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        seqlen: int,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        rate: float,\n",
        "        feed_forward_dim: int,\n",
        "        num_transformer_blocks: int,\n",
        "        rngs: nnx.Rngs,\n",
        "        weights: dict,\n",
        "    ):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "            seqlen, vocab_size, embed_dim, rngs=rngs, weights=weights\n",
        "        )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [\n",
        "            TransformerBlock(\n",
        "                embed_dim,\n",
        "                num_heads,\n",
        "                feed_forward_dim,\n",
        "                dropout_rate,\n",
        "                rngs=rngs,\n",
        "                layer_idx=i,\n",
        "                weights=weights,\n",
        "            )\n",
        "            for i in range(num_transformer_blocks)\n",
        "        ]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(\n",
        "            epsilon=1e-6,\n",
        "            num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(\n",
        "                nnx.initializers.ones_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            bias_init=nnx.with_partitioning(\n",
        "                nnx.initializers.zeros_init(), NamedSharding(mesh, P(\"model\"))\n",
        "            ),\n",
        "            dtype=dtype,\n",
        "            param_dtype=param_dtype,\n",
        "            rngs=rngs,\n",
        "        )\n",
        "        self.layer_norm.scale.value = weights[\"ln_f.weight\"]\n",
        "        self.layer_norm.bias.value = weights[\"ln_f.bias\"]\n",
        "\n",
        "    def __call__(self, inputs, padding_mask=None, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, padding_mask=padding_mask, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits, key):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits / sampling_temp)\n",
        "        return jax.random.choice(key, indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, params, static_def, padded_tokens, length, key):\n",
        "        padding_mask = jnp.arange(seqlen) < length\n",
        "        padding_mask = padding_mask.reshape(1, 1, 1, seqlen)\n",
        "\n",
        "        model = nnx.merge(params, static_def)\n",
        "        logits = model(padded_tokens, padding_mask=padding_mask, training=False)\n",
        "        last_token_logits = logits[:, length - 1, :]\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        next_token = self.sample_from(\n",
        "            jnp.squeeze(last_token_logits), subkey\n",
        "        )  # Pass subkey here\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        key = jax.random.PRNGKey(int(time.time()))\n",
        "\n",
        "        params, static_def = nnx.split(self)\n",
        "\n",
        "        tokens = jnp.array(start_tokens, dtype=jnp.int32)[None, :]\n",
        "        end_token = tokenizer.encode(\n",
        "            \"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}\n",
        "        )[0]\n",
        "\n",
        "        current_len = tokens.shape[1]\n",
        "        padded_tokens = jnp.pad(tokens, ((0, 0), (0, seqlen - current_len)))\n",
        "\n",
        "        print(tokenizer.decode(tokens[0]), end=\"\", flush=True)\n",
        "\n",
        "        for i in range(max_tokens):\n",
        "            key, subkey = jax.random.split(key)\n",
        "\n",
        "            next_token = self.generate_step(\n",
        "                params, static_def, padded_tokens, current_len, subkey\n",
        "            )\n",
        "\n",
        "            if next_token.item() == end_token:\n",
        "                break\n",
        "\n",
        "            print(tokenizer.decode([next_token.item()]), end=\"\", flush=True)\n",
        "\n",
        "            padded_tokens = padded_tokens.at[:, current_len].set(next_token.item())\n",
        "            current_len += 1\n",
        "\n",
        "        final_tokens = padded_tokens[0, :current_len]\n",
        "        return tokenizer.decode(final_tokens.tolist())\n",
        "\n",
        "\n",
        "def create_model(rngs, weights):\n",
        "    return GPT2(\n",
        "        seqlen,\n",
        "        vocab_size,\n",
        "        embed_dim,\n",
        "        num_heads,\n",
        "        dropout_rate,\n",
        "        feed_forward_dim,\n",
        "        num_transformer_blocks,\n",
        "        rngs=rngs,\n",
        "        weights=weights,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHW5lQfIO7y"
      },
      "source": [
        "Now we are going to load the GPT2 model weights from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "9be5905439f1405e88106dd8f351de69",
            "cc14458e603d4fe6a908d173062c6aba",
            "f3d76f5865e34649baf805b3b1156456",
            "facb9a530d6f4e448c37721839109fb3",
            "86c5de6bf5d842a4821c71ebf1648810",
            "f5392eafbeeb46b2a9940f84808b6c5f",
            "ced9f85f596f4a548453f4a00d949d2a",
            "2a20cc1d40384470a312e639c46a0f1a",
            "f8b330eac64f479f8b5b1e356f0389a9",
            "3179e985eae7421c9bdd830625fd4cf3",
            "eeabfc75b7184a5a81a00efe7fa25bf8",
            "d36c312b01d24e43bf982e55188814e2",
            "623b7c21bc9e40cba60577e9257f395d",
            "6d21d95fc7d14277a1c387b7a53a98f8",
            "d1be46140a394bdca503199b1642a4fd",
            "1f23bc30877d432b891c17e3a337eb98",
            "37931b41282d495599382c0a5092c17b",
            "7a3794c883bd47daa06c9269965a7d87",
            "ebd46827148a4fb18fe3b5299c3d3a24",
            "04e7ef5050314b49ad220210dcbe2989",
            "762c23240707431a86249da21070dc1f",
            "83d92c8f5bc9461c8ec36b2397c9a650"
          ]
        },
        "id": "_NpkjhXrH7n3",
        "outputId": "66c0445d-7dda-48ae-8770-36894acabf1b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9be5905439f1405e88106dd8f351de69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d36c312b01d24e43bf982e55188814e2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_id = \"openai-community/gpt2\"\n",
        "if os.path.exists(\"/kaggle\"):\n",
        "    weights_base_dir = \"/kaggle/tmp\"\n",
        "elif os.path.exists(\"/content\"):\n",
        "    # Colab\n",
        "    weights_base_dir = \"/content\"\n",
        "else:\n",
        "    # Local machine\n",
        "    weights_base_dir = \".\"\n",
        "\n",
        "path_to_model_weights = os.path.join(weights_base_dir, model_id)\n",
        "\n",
        "snapshot_download(\n",
        "    repo_id=model_id, local_dir=path_to_model_weights, allow_patterns=\"*.safetensors\"\n",
        ")\n",
        "\n",
        "\n",
        "def load_safetensors():\n",
        "    weights = {}\n",
        "    safetensors_files = Path(path_to_model_weights).glob(\"*.safetensors\")\n",
        "\n",
        "    for file in safetensors_files:\n",
        "        with safe_open(file, framework=\"jax\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                weights[key] = f.get_tensor(key)\n",
        "    return weights\n",
        "\n",
        "\n",
        "weights = load_safetensors()\n",
        "model = create_model(rngs=nnx.Rngs(0), weights=weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBfT1dp5hMUm"
      },
      "source": [
        "Use Weights and Biases to track training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-21T04:32:25.482477Z",
          "iopub.status.busy": "2025-02-21T04:32:25.482072Z",
          "iopub.status.idle": "2025-02-21T04:32:28.629414Z",
          "shell.execute_reply": "2025-02-21T04:32:28.628576Z",
          "shell.execute_reply.started": "2025-02-21T04:32:25.482449Z"
        },
        "id": "IbhEtsganEWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "ad77c749-a59b-462a-a934-c71649f1b27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwindmaple\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250723_055647-l2eif28y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/windmaple/GPT2-LoRA/runs/l2eif28y' target=\"_blank\">fancy-thunder-7</a></strong> to <a href='https://wandb.ai/windmaple/GPT2-LoRA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/windmaple/GPT2-LoRA' target=\"_blank\">https://wandb.ai/windmaple/GPT2-LoRA</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/windmaple/GPT2-LoRA/runs/l2eif28y' target=\"_blank\">https://wandb.ai/windmaple/GPT2-LoRA/runs/l2eif28y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/windmaple/GPT2-LoRA/runs/l2eif28y?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x786938110c50>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import userdata\n",
        "  os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "  os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "  os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "elif platform == \"Kaggle\":\n",
        "  from kaggle_secrets import UserSecretsClient\n",
        "  user_secrets = UserSecretsClient()\n",
        "  os.environ['WANDB_API_KEY'] = user_secrets.get_secret('WANDB_API_KEY')\n",
        "else:\n",
        "  print(\"Please set the WANDB_API_KEY env variable manually\") #input()\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-LoRA',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Instruct tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5GU7yPKSdtj"
      },
      "source": [
        "We are going to use the [Dolly dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) from Databricks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VcKINdFKmzQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "b23283016d6c4cc2a9b1b93a4c525795",
            "59745767db854f9c9bdfc302f0e17bd9",
            "65ebfc432c964843a6bbb14a30e61d43",
            "76646c7fb4aa4ef09dc87ef5e99e5309",
            "c7581514ea78409d8429adb16aac9573",
            "f1cd57f1d57e41e7a18c1d10363222b7",
            "aeed37b4e86144768a1242c069d41c18",
            "c2486e7abceb45afaa09792f6d0a3daf",
            "d54dca18c5154f53b9a49a5033bb831e",
            "e208bf200d1e4309bf8a43a109a56aa5",
            "09571cf42a444b8ab4d0da0c03cec3e7",
            "d11fa9ad554141b186e8b9f6d5df30c1",
            "a37f3e2cdff6400c98d3bac407b7558f",
            "c47d44b163c848d68639705335657dec",
            "266dad82f872401ab0f3c16950e07962",
            "2012be1c22104f328a21cbe6514456b1",
            "942e81a9fb234d0486b698bf23b8a5bd",
            "d3f3202493d34f70a0007f9034c3b290",
            "d8919d4bf4944223b4b1eaa33609838c",
            "a1a3e8c4bae4467c8d819ba00df9a203",
            "e49c2049e14949c7be3d4021f67ac8fc",
            "6fd5aaa025e8427785f748808651dadb",
            "0e9cb302bc704e7aa19ac52a4602b017",
            "e001f07886ab4866a7a858715deb0f2b",
            "d7b264151a744fef8d85f9e91e5b93dd",
            "312a6b05bf2249eb9e725648f10dce74",
            "dab29142dee347398395a9ad3f55b254",
            "fd865ba6cb32415b8a6419e26ee2a0d3",
            "d2bd6a25d9d74db8bd64302513e3a8dc",
            "b424c7a2ceff4cf79b677d4ed2d6e1de",
            "80c43c4669f84026b9777b4ddc855c0b",
            "6a608f048b6d4042befba9858e05bd79",
            "a82c3ab045764f0c868f1041bd54a6e2"
          ]
        },
        "outputId": "3ef67237-d419-4411-a905-b06cee2672c6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b23283016d6c4cc2a9b1b93a4c525795"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d11fa9ad554141b186e8b9f6d5df30c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e9cb302bc704e7aa19ac52a4602b017"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Context:\\n{context}\\n\\n### Response:\\n{response}\"\n",
        "\n",
        "@dataclass\n",
        "class TextDataset:\n",
        "    data_df: pd.DataFrame\n",
        "    seqlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.data_df.iloc[idx]\n",
        "        full_prompt = template.format(\n",
        "            instruction=row['instruction'],\n",
        "            context=row['context'],\n",
        "            response=row['response']\n",
        "        )\n",
        "        # Use Tiktoken for tokenization\n",
        "        encoding = tokenizer.encode(\n",
        "            full_prompt, allowed_special={\"<|endoftext|>\"}\n",
        "        )[: self.seqlen]\n",
        "        return encoding + [50256] * (self.seqlen - len(encoding))\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(dolly_data, batch_size, seqlen):\n",
        "    dolly_data_df = pd.DataFrame(dolly_data)\n",
        "    dataset = TextDataset(dolly_data_df, seqlen)\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=1,\n",
        "    )\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "    return dl\n",
        "\n",
        "\n",
        "dolly_data = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "text_dl = load_and_preprocess_data(dolly_data, batch_size, seqlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VarkoU2TIn2"
      },
      "source": [
        "Define the loss and training step function. In the train_step() function, when we compute the gradients, we only compute the ones for the LoRA parameters and leave the other parameters alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfHW-ESTTOyf"
      },
      "outputs": [],
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits, labels=batch[1]\n",
        "    ).mean()\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(\n",
        "    lora_params, opt_state, graphdef, static_params, metrics: nnx.MultiMetric, batch\n",
        "):\n",
        "    grad_fn = nnx.value_and_grad(\n",
        "        lambda lp: loss_fn(nnx.merge(graphdef, lp, static_params), batch), has_aux=True\n",
        "    )\n",
        "    (loss, logits), grads = grad_fn(lora_params)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    updates, opt_state = tx.update(grads, opt_state, lora_params)\n",
        "    lora_params = optax.apply_updates(lora_params, updates)\n",
        "    return lora_params, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save a copy of the inital model parameters so that we can later verify what is changed and what is not after LoRA finetuning."
      ],
      "metadata": {
        "id": "yNDcY8Egtd1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graphdef, lora_params, static_params = nnx.split(model, LoRAParam, nnx.Param)\n",
        "\n",
        "# Save a copy of the initial parameters for later verification.\n",
        "initial_lora_params = lora_params\n",
        "initial_static_params = static_params"
      ],
      "metadata": {
        "id": "6EYWFcmRthK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc9ySGQgTwaJ"
      },
      "source": [
        "Define optimizer and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om0mZQkqTzP6"
      },
      "outputs": [],
      "source": [
        "graphdef, lora_params, static_params = nnx.split(model, LoRAParam, nnx.Param)\n",
        "\n",
        "schedule = optax.cosine_decay_schedule(\n",
        "    init_value=init_learning_rate, decay_steps=max_steps\n",
        ")\n",
        "tx = optax.chain(optax.adamw(learning_rate=schedule, weight_decay=weight_decay))\n",
        "opt_state = tx.init(lora_params)\n",
        "\n",
        "\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYCeS5BOT65i"
      },
      "source": [
        "Do a test run on our pretrained model to see how it reponds to instruction. Note how we use a template to format the prompt, which needs to be consistent with the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8_OTAAZUYhF",
        "outputId": "d56fbd1c-920f-489f-cfbb-d5131daa7a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Initial generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "\"You were successful\" - the user replied, \"Thank you\""
          ]
        }
      ],
      "source": [
        "start_prompt = template.format(\n",
        "    instruction=\"What is the future for human?\",\n",
        "    context=\"\",\n",
        "    response=\"\",\n",
        ")\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"***Initial generated text:\")\n",
        "generated_text = model.generate_text(seqlen//5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKUZamJKUpyg"
      },
      "source": [
        "As you can see, the pretrained model generates a bunch of garbage; clearly it does not know how to follow the instruction to generate an appropriate answer, which is not surprising given that we have not trained it to do so.\n",
        "\n",
        "Now let's do the instruction tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWWc2Eb_n52d",
        "outputId": "3269d7e8-7eca-48e4-c6be-b166aaf5ecd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Step 20, Loss: 2.236328125, Elapsed Time: 37.12 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "What does it tell us about the person you know?\n",
            "\n",
            "#### Response-\n",
            "\n",
            "Step 40, Loss: 0.638476550579071, Elapsed Time: 40.17 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "What will happen to our human?\n",
            "\n",
            "\n",
            "### Request :\n",
            "\n",
            "Step 60, Loss: 0.6368164420127869, Elapsed Time: 5.81 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "### The next stage in our development team is to implement an automated system with an automated API. We need a way to get information about a system. We can then send and receive an automated, real, real data that the developer can see and use. This will allow our team's development team (as a team, or in other words a company) and their customers the best chance, in any case, in creating new applications. The problem with automated solutions is not that they are easy to manage. They can have many uses and the problem with automated solutions is more complicated than that: the problem that a developer can handle and the challenge that customers will create for them is very different than that in which we might have a simple, straightforward way, or one that would work for most applications at least once, which would make it much faster for the developers and less of an inconvenience. I will show you the first two stages with an analogy: the problem with automation will always be the solution. The other problem in\n",
            "\n",
            "Step 80, Loss: 0.623339831829071, Elapsed Time: 32.14 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The human will evolve and develop. This is the reason why it is that we do not understand a task in any of the four basic ways it was intended in order to make sense of something in a way that was not intended to work out.\n",
            "\n",
            "\n",
            "### The Future of Human:\n",
            "\n",
            "Step 100, Loss: 0.544726550579071, Elapsed Time: 11.73 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "In your life and in the world today, your actions are a part of the fabric of society, which in your life and the world will continue to do so for centuries to come! It would behoove the future of all of the world that we all strive to make a difference. And to accomplish this and for others to make a change in their own lives would require our continued existence, which in my experience would include the continued support of those of our own race who have a history as members of that community and to whom the support, assistance, and love and compassion are our basic right to live and be a free people, in which there should be equal opportunities for all, to be part of an equal and equal society, and for all to have their fair shake.\n",
            "\n",
            "Step 120, Loss: 0.5218750238418579, Elapsed Time: 25.22 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "What is the future, human?\n",
            "### Context Response\n",
            "\n",
            "Step 140, Loss: 0.5113281607627869, Elapsed Time: 5.45 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "### Context:\n",
            "\n",
            "\n",
            "What does it mean in terms of \"future\"?\n",
            "\n",
            "Step 160, Loss: 0.52197265625, Elapsed Time: 5.75 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future will be the human. Humans are intelligent, so it was only natural. They are very powerful.\n",
            "\n",
            "### Context:\n",
            "\n",
            "Step 180, Loss: 0.4867187440395355, Elapsed Time: 7.47 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "What does it mean to be human?\n",
            "\n",
            "\n",
            "### Response/Response Type:\n",
            "\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "\n",
            "Step 200, Loss: 0.516308605670929, Elapsed Time: 7.56 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "There are three basic ways of thinking about what the future is. The first is, we can imagine that humans have already reached the limit of the time we can live on and are in need of the most energy. Humans are living longer lives on less resources but are not in dire need of the highest amounts of them. It is therefore likely that there will never be a \"post-humans-in-their-20th_generation\" world and humans would still be on a finite and unstable course of life. The fourth is, there would be no time limits to what could have been or become. We have the ability, through technological means, to travel, build and sustainably increase the amount we are in need and to survive without being in a position that is beyond our capability. Humans are not in dire need of a higher resource level. The only thing to worry about at this moment is what our \"inhabitants\", the \"humanity\", will have in the future, which would mean we may\n",
            "\n",
            "Step 220, Loss: 0.45917969942092896, Elapsed Time: 31.27 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The current age of our planet is about 2.0 times longer than the human-caused climate of the past 3 billion year, and the global average is 1.3 times that of humans. We're at around 4.6, with 2.0 times higher mean and 3.5 times higher sea level, compared to our current levels. We have 2 degrees of warming in our oceans, 2.9 degrees Celsius warmer in land oceans, 5 degrees Fahrenheit cooler on the equator than the planet average and 1.9° Fahrenheit colder than it does right next around our poles. In our future we might also be a planet on which our oceans could rise 1 meter a decade in a few billion years, with an even warmer climate than that.\n",
            "\n",
            "### Next question:\n",
            "The Earth will warm by 1,000 billion years on this planet. The Earth, like the planet of our planet, would not have a climate that's the same on the other planet, like the planet we have on Earth today\n",
            "\n",
            "Step 240, Loss: 0.5142578482627869, Elapsed Time: 32.26 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The human species will be extinct within the next 100 years, and humanity will be the first to be extinct on an entire planet for the first time ever. It would seem that a future for humans on a planet like our current one might have a very good outcome for humanity if not for an incredibly harsh planet like the Earth itself, as there are currently not much that could have changed for a planet of such high population as Earth has today in this age of climate change.\n",
            "\n",
            "However that doesn't mean that human will live in this lifetime on any planet at all in which the current population of people on any planet are at present, so it's still possible for a future on a planet like ours, if there's still a need to survive on this planet, that would make a lot of things that happen for humans in the present, including the extinction of a certain species of plant species. However it wouldn't be very long before humans were able to colonize a particular country to take care of the population.\n",
            "\n",
            "Step 260, Loss: 0.46342775225639343, Elapsed Time: 32.35 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "This would be to build a future human population, or something similar to the Earth's present-day population, but not one where humans live. A population that lives for an entire life period would still require the same technology and resources as an earth's present-like inhabitants, and a similar population. If humans live to an Earth-like level, that population could be expanded to another population that lives on the Moon.\n",
            "\n",
            "\n",
            "### Response\n",
            "This would be the same technology for life as the Earth and a future Earth, which the human civilization would not experience in order for them to be human and live on Earth.\n",
            "\n",
            "\n",
            "### Response:\n",
            "This would be one of the largest natural disasters since the Industrial Revolution to have occurred in history since it was thought to be responsible. If humans lived and live, it would be the biggest environmental disaster in human history.\n",
            "\n",
            "Step 280, Loss: 0.501269519329071, Elapsed Time: 26.79 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future is a great place with so many beautiful and beautiful people, with a great history, great culture, amazing animals, amazing things, with amazing people. We have to make them all great people so people can see them, to have more and to make it easier for the people to have their own culture and have more people to live in with and with and with and with and so much.\n",
            "\n",
            "Step 300, Loss: 0.48564454913139343, Elapsed Time: 14.77 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Humans are the only animal on the human race that is truly sentient, sentient, intelligent, independent of other human creatures and is in the present. The other animals on our planet are creatures with complex personalities and behaviors. Humans do the most to understand their own emotions or to learn about what motivms motivates and helps them in various different aspects. The most interesting aspect are human personalities that help human beings understand each other, how they are affected, how they relate, and the many different forms human beings can take in order to express them. The human emotions in our lives may be emotional and psychological but there must still be empathy in human lives to make them a part of us. In this article we try not to think of a single emotion, just a group of the emotional responses that a individual might take to understand and feel their own feelings.\n",
            "\n",
            "\n",
            "### Response:\n",
            "Human beings may only live a few thousand to 1.000 years, so we do still have some time for our individual selves that may never\n",
            "\n",
            "Step 320, Loss: 0.4742187559604645, Elapsed Time: 31.36 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "It's the future of mankind; it's not just a technology, the future is already there. It's a fact: the future can change forever. It's already a thing.\n",
            "\n",
            "Step 340, Loss: 0.500292956829071, Elapsed Time: 9.29 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A great way of understanding the world is with a basic overview, a basic idea of life. But as humans, we tend to forget what our future looks to us. This knowledge, the best way to learn, helps us make our decisions and decide for ourselves. This knowledge will make a future that we feel like we live in or will never experience, or will always experience as an alternative to it.\n",
            "\n",
            "It's an exciting time to live and a great time to create. The future can feel like a big change for us in ways that are only for humans in our present lives.\n",
            "\n",
            "\n",
            "Step 360, Loss: 0.46855470538139343, Elapsed Time: 20.54 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Human has a lot of potential. The next major advancement we can achieve would be the creation of intelligent artificial life.\n",
            "\n",
            "Step 380, Loss: 0.4996093809604645, Elapsed Time: 7.01 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Human evolution is a process in which human evolution evolves and becomes an evolving and expanding world. The human species evolved in a particular place or area in the world that we now know and are able to understand, such as Antarctica, Antarctica is the most complex planet, and there is no way in any way of measuring its scale. In fact, our planet, with a large and deep ice, was not even considered for the first time in 200BC, because in those days it had a size of just one cubic kilometers (1.6 m��) but in the 21st century we know that, with all due respect to the current world of our world and with the new world we're living in, we could easily get around in that area. The fact, in this time, that humans are able to live in areas where there's no ice, that it's an oceanic oceanic oceanic region that's much larger (about 0,800 meters), is evidence that we are living in the same universe\n",
            "\n",
            "Step 400, Loss: 0.46406251192092896, Elapsed Time: 31.66 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Future human is the term used when a person believes in a specific event of future human.\n",
            "\n",
            "Step 420, Loss: 0.4810546934604645, Elapsed Time: 6.47 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "We are an evolutionary force that exists at some place in the universe in all directions that is at the moment we evolved and have no control over, but are constantly developing, that can be thought and defined as the ultimate, the ultimate force that will ultimately change humanity's course, and that can be used for any of the following:\n",
            "1. Natural selection is at work to create an ever-changing set of values in an infinite variety of environments;\n",
            "2. Evolution can not have all these values, it is not a single individual of the universe; and\n",
            "3: the future of humankind will ultimately be the same, it's destiny to create the best and the last one of humanity will determine our survival for generations after this event; we can choose to be human or not, or we can choose to continue with evolution, but not evolve.\n",
            "\n",
            "We are one. If we continue to evolve, there will be more to our existence. But if not, and the future of us, and those we love\n",
            "\n",
            "Step 440, Loss: 0.4576171934604645, Elapsed Time: 31.93 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The next stage in human evolution is an evolution with a major impact to the human species and future evolution in the species. It has a profound effects upon the environment and on the human population. This is a very significant event. As humans grow stronger as a result. As these changes take place in our lives we become more vulnerable and in the course of them, we may become more resistant than our evolutionary descendants are. As our species progresses it can evolve to adapt more slowly, with fewer genes and many genes and the environment can change for the better.\n",
            "\n",
            "Step 460, Loss: 0.45976564288139343, Elapsed Time: 18.70 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The next billion people are projected to live about the same amount as humans and we would be about 10% larger by 2100\n",
            "\n",
            "### Next century:\n",
            "\n",
            "Step 480, Loss: 0.45966798067092896, Elapsed Time: 7.84 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future for humanity is an open science, with a future of the first human to be living, with no limits.\n",
            "\n",
            "A future in which humanity has evolved beyond a limited human species, the next human in the world will live forever.\n",
            "\n",
            "A Future in which the next human will be living, a future which has already arrived\n",
            "\n",
            "Step 500, Loss: 0.42041015625, Elapsed Time: 13.70 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future is a complex and fascinating one that is still largely unexplored in science fiction fiction. In fact, it is far from certain at all but there are some hints as to how the world could go in this direction: a nuclear winter would have been a perfect opportunity for an interplanetary strike against the Earth in 2027/30, which is why the world had a lot of interest to learn about this new world order that has yet to have any of that.\n",
            "\n",
            "Step 520, Loss: 0.42841798067092896, Elapsed Time: 16.52 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "The future of human has a long tradition as well as a very strong social force which has shaped our world since the days of man: agriculture. In particular, humans, as individuals, were the most powerful and influential social and economic power, and the dominant social institutions and institutions in the world, and the largest population of them (as measured by GDP in 1980). For example, human society was dominated by two major social movements: agriculture and trade, with the largest and largest trade union (BPP) in the Western World, while the second biggest social organization (Gosnosti) emerged, which dominated trade and economic affairs of agriculture.\n",
            "\n",
            "\n",
            "In contrast with these two movements are the major and main institutions, the political and economic, of the world, the economic system. As the main force for social, economic development, social and cultural development, the social, economic, and the technological institutions of our world are all based on agriculture and trade (BPP). A large portion of the population is still\n",
            "\n",
            "Step 540, Loss: 0.4427734315395355, Elapsed Time: 31.96 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A great deal can and do change people, but it all depends a lot of things - whether it's changing society, changing people around you, improving the way you look, and improving the lives of people. It may take decades for everyone to find their way and get their chance at the next great opportunity; we're just now starting to take this opportunity of changing people around us.\n",
            "\n",
            "Step 560, Loss: 0.45820313692092896, Elapsed Time: 14.24 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Currently humans are on average around 5 percent of humanity. We are in the middle of the 20th century, but we have not experienced the great progress humans are making over time. There have only become the major civilizations on the planet, with humanity on the edge. We are also the largest planet on Earth with more people on this earth than all of Europe combined or any other major civilizations on a human continent. Our planet and planet's population are at the same time a major global population.\n",
            "\n",
            "Step 580, Loss: 0.4878906309604645, Elapsed Time: 16.66 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Currently humans have two main social and cultural environments. One is the social, but the social also refers to human societies.\n",
            "In social society, humans are divided from other species by family and social status, so their family members and social groups are the ones who have more than a certain level in common with humans. A person is considered to have a certain level and status when a specific family members or groups of friends have been added to his social group in any way.\n",
            "\n",
            "Step 600, Loss: 0.45698243379592896, Elapsed Time: 16.59 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "There has already been a huge growth for human activity in the world as well with technological advancement. We have been a major player with advancements like mobile phone and computer-powered computing, and the rise of AI is one reason we are now an important global player.\n",
            "\n",
            "Step 620, Loss: 0.4585937559604645, Elapsed Time: 11.08 seconds\n",
            "\n",
            "***Intermediate generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "We live in modern society. A future that will be better for everyone is a world where everyone lives on the same terms, where each is equally free and is equally capable of doing all he or she does. A society that encourages diversity of interests and values and promotes an equal number of men/men in every position will have an economic success. It will also make a difference for everyone on the basis of race and social status. It will not only help everyone to have equal opportunity but will create more wealth. This could also be seen by creating social safety net schemes and social infrastructure to help everyone, regardless their income (such as housing).\n",
            "***Final generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What is the future for human?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "Humans have become more human, but not to that degree as much as they used to be\n",
            "\n",
            "### Context\n",
            "Humanity will become an extinct species\n"
          ]
        }
      ],
      "source": [
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "for batch in text_dl:\n",
        "    if len(batch) % len(jax.devices()) != 0:\n",
        "        continue  # skip the remaining elements\n",
        "    input_batch = jnp.array(batch).T\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    lora_params, opt_state = train_step(\n",
        "        lora_params,\n",
        "        opt_state,\n",
        "        graphdef,\n",
        "        static_params,\n",
        "        metrics,\n",
        "        jax.device_put(\n",
        "            (input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    if (step + 1) % 20 == 0:\n",
        "        for metric, value in metrics.compute().items():\n",
        "            metrics_history[f\"train_{metric}\"].append(value)\n",
        "        metrics.reset()\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(\n",
        "            f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\"\n",
        "        )\n",
        "        wandb.log(data={'train_loss': metrics_history['train_loss'][-1]}, step=step)\n",
        "        start_time = time.time()\n",
        "        print(f\"\\n***Intermediate generated text:\")\n",
        "        intermediate_model = nnx.merge(graphdef, lora_params, static_params)\n",
        "        generated_text = intermediate_model.generate_text(seqlen // 5, start_tokens)\n",
        "    step += 1\n",
        "\n",
        "# Final text generation\n",
        "model = nnx.merge(graphdef, lora_params, static_params)\n",
        "print(f\"\\n***Final generated text:\")\n",
        "generated_text = model.generate_text(seqlen // 5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beji0Q63afpe"
      },
      "source": [
        "As you can see, at the end of the finetuning, the model is able to follow human instruction and generate a somewhat sensible answer. The final answer is still not ideal, but if you could find another more comprehensive instruction tuning dataset, it will definitely improve.\n",
        "\n",
        "And to prove that only LoRA parameters were trained, we compare the final parameters with the inital copies we made before finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uHcKfOvbcsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62367fdc-7417-40d3-f54d-50ef05d1bc21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***Final generated text:\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "How would you make a pancake?\n",
            "\n",
            "### Context:\n",
            "\n",
            "\n",
            "### Response:\n",
            "A pancake should have an inner crust of crispy and crunchy edges. This can be accomplished by cooking an egg (or any other cooked egg substitute such as butter) or a batter in large pan. In other examples, you might use your own egg in addition to a pancame mixture, or use a flour-based pancake batter to create a pancaked dough instead if desired."
          ]
        }
      ],
      "source": [
        "start_prompt = template.format(\n",
        "    instruction=\"How would you make a pancake?\",\n",
        "    context=\"\",\n",
        "    response=\"\",\n",
        ")\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"***Final generated text:\")\n",
        "generated_text = model.generate_text(seqlen//5, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One other thing you could do is to compare the HBM usage of this LoRA run with the previous full parameter finetune run (for example, by eyeballing the tpu-info output). There will be a pretty significant reduction in terms of memory usage."
      ],
      "metadata": {
        "id": "LLifdQSmv1Xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the main kernel weights (static params) have NOT changed.\n",
        "static_leaves_before = jax.tree_util.tree_leaves(initial_static_params)\n",
        "static_leaves_after = jax.tree_util.tree_leaves(static_params)\n",
        "\n",
        "static_params_are_unchanged = all(\n",
        "    jnp.allclose(b, a) for b, a in zip(static_leaves_before, static_leaves_after)\n",
        ")\n",
        "print(f\"Original model parameters (static) remained frozen: {static_params_are_unchanged}\")\n",
        "\n",
        "# Check that the LoRA parameters HAVE changed.\n",
        "lora_leaves_before = jax.tree_util.tree_leaves(initial_lora_params)\n",
        "lora_leaves_after = jax.tree_util.tree_leaves(lora_params)\n",
        "\n",
        "lora_params_have_changed = not all(\n",
        "    jnp.allclose(b, a) for b, a in zip(lora_leaves_before, lora_leaves_after)\n",
        ")\n",
        "print(f\"LoRA parameters were updated: {lora_params_have_changed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98K2CI3fhAy9",
        "outputId": "4aa06621-5bdb-4714-95ee-73f582c2373b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model parameters (static) remained frozen: True\n",
            "LoRA parameters were updated: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwWVsvfH5ohd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "datasetId": 5848741,
          "sourceId": 10577797,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30920,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9be5905439f1405e88106dd8f351de69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc14458e603d4fe6a908d173062c6aba",
              "IPY_MODEL_f3d76f5865e34649baf805b3b1156456",
              "IPY_MODEL_facb9a530d6f4e448c37721839109fb3"
            ],
            "layout": "IPY_MODEL_86c5de6bf5d842a4821c71ebf1648810"
          }
        },
        "cc14458e603d4fe6a908d173062c6aba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5392eafbeeb46b2a9940f84808b6c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_ced9f85f596f4a548453f4a00d949d2a",
            "value": "Fetching 1 files: 100%"
          }
        },
        "f3d76f5865e34649baf805b3b1156456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a20cc1d40384470a312e639c46a0f1a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8b330eac64f479f8b5b1e356f0389a9",
            "value": 1
          }
        },
        "facb9a530d6f4e448c37721839109fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3179e985eae7421c9bdd830625fd4cf3",
            "placeholder": "​",
            "style": "IPY_MODEL_eeabfc75b7184a5a81a00efe7fa25bf8",
            "value": " 1/1 [00:02&lt;00:00,  2.39s/it]"
          }
        },
        "86c5de6bf5d842a4821c71ebf1648810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5392eafbeeb46b2a9940f84808b6c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced9f85f596f4a548453f4a00d949d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a20cc1d40384470a312e639c46a0f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b330eac64f479f8b5b1e356f0389a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3179e985eae7421c9bdd830625fd4cf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeabfc75b7184a5a81a00efe7fa25bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d36c312b01d24e43bf982e55188814e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_623b7c21bc9e40cba60577e9257f395d",
              "IPY_MODEL_6d21d95fc7d14277a1c387b7a53a98f8",
              "IPY_MODEL_d1be46140a394bdca503199b1642a4fd"
            ],
            "layout": "IPY_MODEL_1f23bc30877d432b891c17e3a337eb98"
          }
        },
        "623b7c21bc9e40cba60577e9257f395d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37931b41282d495599382c0a5092c17b",
            "placeholder": "​",
            "style": "IPY_MODEL_7a3794c883bd47daa06c9269965a7d87",
            "value": "model.safetensors: 100%"
          }
        },
        "6d21d95fc7d14277a1c387b7a53a98f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebd46827148a4fb18fe3b5299c3d3a24",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04e7ef5050314b49ad220210dcbe2989",
            "value": 548105171
          }
        },
        "d1be46140a394bdca503199b1642a4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_762c23240707431a86249da21070dc1f",
            "placeholder": "​",
            "style": "IPY_MODEL_83d92c8f5bc9461c8ec36b2397c9a650",
            "value": " 548M/548M [00:01&lt;00:00, 489MB/s]"
          }
        },
        "1f23bc30877d432b891c17e3a337eb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37931b41282d495599382c0a5092c17b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3794c883bd47daa06c9269965a7d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebd46827148a4fb18fe3b5299c3d3a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e7ef5050314b49ad220210dcbe2989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "762c23240707431a86249da21070dc1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83d92c8f5bc9461c8ec36b2397c9a650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b23283016d6c4cc2a9b1b93a4c525795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59745767db854f9c9bdfc302f0e17bd9",
              "IPY_MODEL_65ebfc432c964843a6bbb14a30e61d43",
              "IPY_MODEL_76646c7fb4aa4ef09dc87ef5e99e5309"
            ],
            "layout": "IPY_MODEL_c7581514ea78409d8429adb16aac9573"
          }
        },
        "59745767db854f9c9bdfc302f0e17bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1cd57f1d57e41e7a18c1d10363222b7",
            "placeholder": "​",
            "style": "IPY_MODEL_aeed37b4e86144768a1242c069d41c18",
            "value": "README.md: "
          }
        },
        "65ebfc432c964843a6bbb14a30e61d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2486e7abceb45afaa09792f6d0a3daf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d54dca18c5154f53b9a49a5033bb831e",
            "value": 1
          }
        },
        "76646c7fb4aa4ef09dc87ef5e99e5309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e208bf200d1e4309bf8a43a109a56aa5",
            "placeholder": "​",
            "style": "IPY_MODEL_09571cf42a444b8ab4d0da0c03cec3e7",
            "value": " 8.20k/? [00:00&lt;00:00, 1.03MB/s]"
          }
        },
        "c7581514ea78409d8429adb16aac9573": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cd57f1d57e41e7a18c1d10363222b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeed37b4e86144768a1242c069d41c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2486e7abceb45afaa09792f6d0a3daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d54dca18c5154f53b9a49a5033bb831e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e208bf200d1e4309bf8a43a109a56aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09571cf42a444b8ab4d0da0c03cec3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11fa9ad554141b186e8b9f6d5df30c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a37f3e2cdff6400c98d3bac407b7558f",
              "IPY_MODEL_c47d44b163c848d68639705335657dec",
              "IPY_MODEL_266dad82f872401ab0f3c16950e07962"
            ],
            "layout": "IPY_MODEL_2012be1c22104f328a21cbe6514456b1"
          }
        },
        "a37f3e2cdff6400c98d3bac407b7558f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_942e81a9fb234d0486b698bf23b8a5bd",
            "placeholder": "​",
            "style": "IPY_MODEL_d3f3202493d34f70a0007f9034c3b290",
            "value": "databricks-dolly-15k.jsonl: 100%"
          }
        },
        "c47d44b163c848d68639705335657dec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8919d4bf4944223b4b1eaa33609838c",
            "max": 13085339,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1a3e8c4bae4467c8d819ba00df9a203",
            "value": 13085339
          }
        },
        "266dad82f872401ab0f3c16950e07962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e49c2049e14949c7be3d4021f67ac8fc",
            "placeholder": "​",
            "style": "IPY_MODEL_6fd5aaa025e8427785f748808651dadb",
            "value": " 13.1M/13.1M [00:00&lt;00:00, 94.9MB/s]"
          }
        },
        "2012be1c22104f328a21cbe6514456b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "942e81a9fb234d0486b698bf23b8a5bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3f3202493d34f70a0007f9034c3b290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8919d4bf4944223b4b1eaa33609838c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1a3e8c4bae4467c8d819ba00df9a203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e49c2049e14949c7be3d4021f67ac8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd5aaa025e8427785f748808651dadb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e9cb302bc704e7aa19ac52a4602b017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e001f07886ab4866a7a858715deb0f2b",
              "IPY_MODEL_d7b264151a744fef8d85f9e91e5b93dd",
              "IPY_MODEL_312a6b05bf2249eb9e725648f10dce74"
            ],
            "layout": "IPY_MODEL_dab29142dee347398395a9ad3f55b254"
          }
        },
        "e001f07886ab4866a7a858715deb0f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd865ba6cb32415b8a6419e26ee2a0d3",
            "placeholder": "​",
            "style": "IPY_MODEL_d2bd6a25d9d74db8bd64302513e3a8dc",
            "value": "Generating train split: 100%"
          }
        },
        "d7b264151a744fef8d85f9e91e5b93dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b424c7a2ceff4cf79b677d4ed2d6e1de",
            "max": 15011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80c43c4669f84026b9777b4ddc855c0b",
            "value": 15011
          }
        },
        "312a6b05bf2249eb9e725648f10dce74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a608f048b6d4042befba9858e05bd79",
            "placeholder": "​",
            "style": "IPY_MODEL_a82c3ab045764f0c868f1041bd54a6e2",
            "value": " 15011/15011 [00:00&lt;00:00, 256272.20 examples/s]"
          }
        },
        "dab29142dee347398395a9ad3f55b254": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd865ba6cb32415b8a6419e26ee2a0d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bd6a25d9d74db8bd64302513e3a8dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b424c7a2ceff4cf79b677d4ed2d6e1de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c43c4669f84026b9777b4ddc855c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a608f048b6d4042befba9858e05bd79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a82c3ab045764f0c868f1041bd54a6e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}